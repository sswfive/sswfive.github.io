[{"title":"不止Rust，uv如此速度的背后真实原因","path":"/2026/01/06/tech/python/260106-why-is-uv-so-fast/","content":"在众多Python包管理工具中，uv的热度近两年持续攀升，并受到越来越多的开发者的认可与采用。一个关键原因是，uv被公认为目前社区中依赖管理速度最快的工具。尽管许多人将其出色的性能归功于其采用Rust语言开发，但实际上，uv的性能表现更主要源于其开发团队的设计思路和工程决策。 设计选择比编程语言更能计策工具的性能上限。 一、Python打包标准的演进在早期，pip的缓慢并非实现问题，而是被历史包袱所累。主要有以下几个原因： setup.py的致命缺陷 过去，要安装一个包，必须运行其setup.py才能知道依赖，但运行setup.py,又需要先安装它的构建依赖——形成了典型的“鸡生蛋蛋生鸡”的问题，这迫使pip不得不： 下载源码包 执行不受信任的任意代码 失败后重试，反复生成子进程 整个过程像curl | bash ，既慢又危险 PEP的出现解决了上述问题： PEP 518（2016）：引入pyproject.toml ,允许声明式指定构建依赖 PEP 517（2017）：分离构建前后端，pip不再需要理解setuptools内部 PEP 621（2020）：标准化[project]表，依赖信息可直接从TOML解决 PEP 658（2022， PyPI 2023年上线）：在Simple API 中直接提供wheel元数据，无需下载整个文件即可获取依赖。 从上述的时间线可以看出，uv之所以快，是因为生态系统终于有了支持它的基础设施。 Tips: PEP 658 在 2023 年 5 月上线 PyPI，而 uv 在 2024 年 2 月发布 二、设计取舍：uv主动“放弃”的功能uv 的速度很大程度上源于 有意识地剔除兼容性负担。它明确不支持以下 pip 支持的功能： 功能 uv 的处理 性能收益 .egg 格式 完全不支持（已淘汰十余年） 避免解析旧格式的开销 pip.conf 配置 忽略所有 pip 配置文件 省去配置解析、继承、环境变量查找 字节码编译（.pyc） 默认跳过（可选开启） 减少每次安装的 IO 和 CPU 开销 系统 Python 安装 强制使用虚拟环境（除非显式指定） 省去权限检查、安全防护逻辑 宽松包规范容忍 严格拒绝不符合标准的包 避免 fallback 逻辑和异常处理 requires-python 上界 忽略 python4.0 这类上界 极大减少依赖解析器回溯 多索引源行为 “第一个索引命中即停” 避免跨多个仓库查询 核心理念：每一条被移除的代码路径，都是用户等待时间的节省。 三、可移植优化：与语言无关的提速实现 HTTP Range 请求获取元数据 wheel是zip文件，目录在末尾。uv优先使用PEP 658元数据 ，否则用Range请求只下载zip目录， 最后才全量下载，这覆盖了绝大部分的场景。 并行下载 pip 串行下载；uv 并发下载多个包。 全局缓存 + 硬链接 pip每次复制文件到venv; uv 全局存一份，通过硬链接共享。10个venv安装同一个包约等于1份磁盘空间。 无Python的依赖解析 uv直接解析TOML和wheel元数据。仅在遇到纯setup.py包时才调用Python子进程。 PubGrub依赖解析算法 来自Dart的pub,采用冲突驱动子句学习（CDCL）。比pip的回溯更智能；失败时记录原因，避免重复探索。更快解决复杂依赖，且错误提示更清晰。pip完全可以集成PubGrub，无需重写。 四、Rust发挥作用的地方 零拷贝反序列化（rkyv）：缓存数据直接映射为内存结构，无需解析、拷贝 真正的线程级并行：Python受GIL限制，并行需多进程 + IPC；Rust可高效共享内存并行 无解释器启动开销：uv是静态二进制，启动即运行；pip每次子进程都需要加载Python解释器 紧凑的版本表示：90%+ 的版本号可压缩为当个u64，加速比较与哈希（微优化，但在百万次操作中显著） 启发： 如果你的生态系统需要运行任意代码才能发现包的依赖关系，那它就已经输了 性能提升往往不是靠换语言，而是靠重新思考“什么才是真正必要的”","tags":["包管理工具"],"categories":["Python"]},{"title":"Python开发者必知的性能数据与实用技巧","path":"/2026/01/04/tech/python/260104-py-performace-analysis-and-practice/","content":"在日常开发中，我们常常凭借直觉选择数据结构、编写函数、或决定是否使用异步。然而在Python中，那些”看起来差不多“的操作，实际开销可能相差几个数量级。 最近读到一篇文章《Python Numbers Every Programmer Should Know》，作者通过实测给出了常见操作的时间与内存开销，这些数字不仅令人惊叹，更能指导我们写出更高效、更省资源的代码。 本文将结合原文内容与实践经验，分享一些关键的性能认知与开发技巧。 一、内存开销：看不见的资源消耗Python的便利性背后，是显著的内存开销。理解这些开销，是处理大数据集或高并发应用的前提。 1. 基础对象的“体重” 一个空的字符串 重达 41字节。 一个小整数（如 1）占用 28字节（得益于-5到256的整数池缓存，相同小整数是共享的）。 一个空列表 [] 需要 56字节，一个空字典 需要 64字节。 2. 集合的容量增长当存储1000个元素时，不同数据结构的内存效率差异显著： 列表（1000个整数）：约36 KB 字典（1000个键值对）：约91 KB (是列表的2.5倍) 集合（1000个成员）：约60 KB 3. 类的内存激增面向对象编程的便利伴随着成本。一个具有5个属性的普通类实例占用约 694字节。如果程序中需要创建成千上万个实例，内存压力会急剧上升。 二、常见操作的性能概览：从纳秒到毫秒让我们看一下”性能金字塔”，从最快的操作到较慢的操作： 属性读取 (obj.x) 14 ns字典查询 22 ns函数调用（空函数） 22 ns列表追加 29 nsf-string 格式化 65 ns异常捕获 140 nsorjson 序列化复杂对象 310 nsjson 反序列化简单对象 714 ns遍历 1000 个元素的列表 7.9 μs打开并关闭文件 9.1 μsSQLite 主键查询 3.6 μs写入 1KB 文件 35 μsMongoDB find_one 121 μsSQLite 插入（带提交） 192 μs导入 json 模块 2.9 ms导入 asyncio 模块 18 ms导入 fastapi 模块 104 ms 数据说话： 基本操作（属性访问、字典查询）在纳秒级 文件操作和数据库操作在微秒到毫秒级 模块导入可能比你想象的慢的多 三、数据结构选择：性能差距可达200倍 列表 vs 字典 vs 集合的成员检查，在 1000 个元素中查找一个元素： 数据结构 耗时 每秒操作数 集合item in set 19 ns 52.7M opssec 字典item in dict 22 ns 45.7M opssec 列表item in list 3,850 ns 259.6k opssec 数据说话： 结论：列表比集合字典慢 200 倍！本质原因：集合和字典使用哈希表，时间复杂度 O(1)，列表需要线性扫描，时间复杂度 O(n) 实践建议 # 不推荐：频繁的成员检查allowed_users = [alice, bob, charlie, ...] # 列表if user in allowed_users: # 慢！ process()# 推荐：使用集合allowed_users = alice, bob, charlie, ... # 集合if user in allowed_users: # 快 200 倍！ process() 四、列表推导式真的更快 对比生成 1000 个元素： 方法 耗时 性能提升 列表推导式 9.45 μs +26% for 循环 + append 11.9 μs 基准 数据说话： 原因：列表推导式在 C 层面优化，减少了 Python 层面的函数调用开销。 五、内存优化：强大的__slots__单个对象的内存占用 类型 5个属性的实例 内存节省 普通类 694 bytes - __slots__类 212 bytes 69% dataclass 694 bytes - dataclass(slotsTrue) 212 bytes 69% 1000 个实例的集合 类型 总内存 内存节省 1000 个普通实例 301.8 KB - 1000 个 slots 实例 215.7 KB 28% 使用 __slots__ 的示例： # 普通类class User: def __init__(self, name, email, age, city, score): self.name = name self.email = email self.age = age self.city = city self.score = score# 使用 __slots__（节省 69% 内存）class User: __slots__ = [name, email, age, city, score] def __init__(self, name, email, age, city, score): self.name = name self.email = email self.age = age self.city = city self.score = score 访问速度几乎相同： 普通类读取：14.1 ns slots 类读取：14.1 ns 何时使用 __slots__： 需要创建大量实例（数千个以上） 属性固定且已知 内存受限的环境（如游戏实体、缓存项、日志记录） 不适用场景： 需要动态添加属性 需要使用 __dict__ 或 __weakref__ tips: Python便利性背后，是显著的内存开销。对于需要海量创建的小型数据对象，优先考虑使用namedtuple、dataclass或手动定义__slots__，可以带来巨大的内存节约 六、JSON 序列化：选对库提速 8 倍复杂对象序列化性能对比 JSON 库 序列化耗时 反序列化耗时 相对标准库 orjson 310 ns 839 ns 8.5x 更快 msgspec 445 ns 850 ns 6x 更快 ujson 1.64 μs 1.46 μs 1.6x 更快 json (标准库) 2.65 μs 2.22 μs 基准 数据说话： Python 的标准库稳定可靠，但在性能敏感场景，第三方库往往更胜一筹。如果你的 API 需要高频返回 JSON，换一个序列化库可能就解决了瓶颈。 七、Web 框架性能对比返回简单 JSON 响应的基准测试： 框架 平均响应时间 每秒请求数 相对性能 Starlette 8.01 μs 124.8k 最快 Litestar 8.19 μs 122.1k 1.02x FastAPI 8.63 μs 115.9k 1.08x Flask 16.5 μs 60.7k 2.06x Django 18.1 μs 55.4k 2.26x 数据说话： 现代异步框架（Starlette、FastAPI、Litestar）性能相近 FastAPI 比 Django 快约 2 倍 传统同步框架（Flask、Django）相对较慢 框架选择建议： 高性能 API：Starlette、Litestar、FastAPI 快速原型：Flask 全功能应用：Django 性能不是唯一考量：还要考虑生态系统、团队熟悉度、项目需求 八、数据库操作性能SQLite、diskcache、MongoDB 对比 操作 SQLite diskcache MongoDB 写入 192 μs 23.9 μs 119 μs 主键读取 3.57 μs 4.25 μs 121 μs 字段查询 4.27 μs NA 124 μs 更新 5.22 μs 23.9 μs 115 μs 数据说话： SQLite 读取极快（微秒级），适合读多写少 diskcache 写入最快，适合缓存场景 MongoDB 受网络延迟影响，进程内数据库（SQLite）更快 九、字符串格式化：f-string 的优势 格式化方法 耗时 相对性能 字符串拼接 + 39.1 ns 最快（简单场景） f-string 64.9 ns 推荐 %格式化 89.8 ns 1.4x 慢 .format() 103 ns 1.6x 慢 数据说话： f-string 在性能和可读性之间达到了最佳平衡，应作为首选。 \u0004十、异步（Async）不是银弹，它有成本同步 vs 异步函数调用 操作 耗时 倍数差异 同步函数调用 20.3 ns 基准 异步函数调用 28.2 μs 1,400x 慢 异步操作的开销 操作 耗时 创建协程对象 47 ns run_until_complete空协程 27.6 μs asyncio.sleep(0) 39.4 μs gather10 个协程 55 μs 关键认知：很多人以为“用了 async 就更快”，但事实并非如此。 启动一次空的 asyncio 事件循环：～28 微秒 调用一个普通函数：～20 纳秒 async 的启动开销比一次磁盘 IO（～10 微秒）还要大！ 这意味着： 如果你的任务是 CPU 密集型（如图像处理、数值计算），async 不仅没用，反而拖慢速度。 如果你只是调用一次数据库或 API，同步代码更简单、更快。 只有当你需要同时处理成百上千个 IO 请求时（如聊天服务器、爬虫），async 才真正发挥价值。 十一、异常处理的成本 操作 耗时 倍数差异 tryexcept（无异常） 21.5 ns 基准 tryexcept（有异常） 139 ns 6.5x 慢 最佳实践： # 不要用异常控制正常流程def get_value(d, key): try: return d[key] except KeyError: return None # 异常路径慢 6.5 倍# 使用条件判断def get_value(d, key): if key in d: return d[key] return None# 或使用 dict.get()def get_value(d, key): return d.get(key) 十二、必须警惕的“性能陷阱” 异常滥用：try/except 在无异常发生时开销很小（21.5 ns），但一旦触发并捕获异常，成本激增至约140 ns，是正常流程的6-7倍。切勿将异常用于常规控制流。 昂贵的导入：如前述，大型模块的首次导入成本极高。在设计需要快速启动的CLI工具或Lambda函数时，应考虑延迟导入或减少依赖。 列表的成员检查：在1000个元素的列表中检查成员，需要 3.85 µs，而等量集合的成员检查仅需 19 ns，快200倍以上。频繁的成员检查，务必使用集合（set）。 写文件 vs 读文件：写入1KB数据（35 µs）比读取（10 µs）慢3.5倍，比单纯打开关闭文件（9 µs）慢近4倍。对写操作密集的程序，缓冲和批量写入策略尤为重要。 写在后面理解这些性能数字不是为了过早优化，而是为了在架构设计和关键路径上做出明智的决策。 “过早优化是万恶之源。” —— Donald Knuth 但同时，了解基本的性能特性可以帮助我们避免明显的性能陷阱. 性能优化的黄金原则： 先让代码正确运行 用 cProfile、py-spy 等工具定位瓶颈 针对瓶颈做有针对性的优化 用基准测试验证效果 真正的性能艺术，是在开发效率、可读性与执行效率之间找到平衡。不必对每个纳秒斤斤计较，但要对数量级（倍数的差异）保持敏感**。 希望这些”应该知道的数字”，能帮助我们写出不仅正确，而且优雅高效的 Python 代码。 参考资源： 原文：Python Numbers Every Programmer Should Know 测试代码：GitHub Repository","tags":["python工程实践"],"categories":["Python"]},{"title":"从前端视角理解浏览器和HTTP的缓存机制","path":"/2025/11/06/tech/others/251116-browser-cache/","content":"什么是浏览器缓存我们都知道，一个典型的Web应用请求-响应流程通常如下所示： 客户端 - 发起HTTP请求 - 服务器端接收请求 -查询数据库 - 执行业务逻辑处理 - 构造HTTP响应 - 返回客户端 在这一链路中，性能瓶颈往往集中在两个关键阶段：网络请求阶段和服务端计算阶段。 网络请求阶段：主要包括客户端发起的HTTP请求以及服务端对数据库的查询请求； 服务端计算阶段：主要包括业务逻辑处理和数据库查询结果的加工处理。 为了提高Web应用的响应速度和用户体验，引入缓存技术成为了优化Web应用性能的关键手段之一。通过在不同层级引入缓存，可以显著减少重复计算和网络开销，从而提高页面响应效率。常见的缓存策略包括： 数据库缓存 CDN（内容分发网络）缓存 代理服务器缓存（如反向代理） 浏览器缓存 应用层缓存（如Redis、Memcached) 本文将从前端视角出发，重点探讨浏览器缓存的原理、类型及其实践。 浏览器缓存的基本原理浏览器缓存工作流程： 浏览器发起第一次HTTP请求：用户在浏览器中访问某个资源（如页面、图片、脚本等），浏览器向缓存系统发送首次请求。 缓存未命中（无缓存结果）：缓存系统检查是否存在该资源的缓存副本，由于是首次请求，缓存中没有对应数据，因此返回“未命中”状态。 浏览器向服务器发起请求：由于缓存未命中，浏览器继续向后端服务器发送原始HTTP请求以获取资源。 服务器返回请求资源：服务器接收到请求后，处理并返回所需要的资源内容（如HTML、CSS、JS文件等）。 资源存入缓存中：浏览器在接收到服务器返回的资源后，将其存储到本地缓存中，以便后续请求时快速读取，避免重复向服务器发起请求。 整个浏览器缓存的过程基于两个核心原则： 浏览器每次发起请求时，都会先在浏览器缓存中查找该请求的结果和缓存标识 浏览器每次拿到返回的请求结果都会将该结果和缓存标识存入浏览器缓存中 接下来将从两个维度来介绍浏览器缓存： 缓存的存储位置 缓存的类型 按照缓存位置分类浏览器缓存存存储位置上分为四种，按照优先级依次查找： Service Worker Memory Cache Disk Cache Push Cache Service WorkerService Worker（简称：SW）是运行在浏览器背后的独立线程，一般可以用来实现缓存功能。值得注意的是，使用SW，传输协议必须是HTTPS，因为SW中涉及到请求拦截，所以必须使用HTTPS协议来保障安全。 SW的的缓存与浏览器的其他内建的缓存机制不同，它可以让我们自由控制缓存哪些文件、如何匹配缓存、如何读取缓存、并且缓存是持续性的，常用于PWA离线访问场景。 SW的工作流程： 注册Service Worker 监听install事件并缓存文件 拦截请求查询缓存、存在则直接返回，否则继续网络请求 通常，当SW没有命中缓存时，需要去调用fetch函数获取数据，也就是说，如果没有在SW命中缓存时，会根据缓存优先级查找数据。但不管我们是从Memory Cache中还是请求网络中获取数据，浏览器都会显示我们是从SW中获取的内容。 Memory CacheMemory Cache（简称：MC）是内存中的缓存，主要包含的是当前页面中已经抓取的资源，例如页面上已经下载的样式、脚本、图片等。 读取内存的数据肯定比磁盘高效，但毕竟内存空间有限，所以缓存有效期很短，一般会随着进程的结束而释放，也就是说当我们关闭Tab页面，内存中的缓存也就被释放了。 当我们访问过页面后，再次刷新页面，可以发现很多数据都来自内存缓存。如下图所示： MC机制保证了一个页面如果有两个相同的请求，都只会被请求最多一次，避免请求浪费，如： 两个src相同的*img* 两个href相同的*link* Disk CacheDisk Cache（简称：DC）是存储在硬盘中的缓存，读取速度慢点，但什么都能存储到磁盘中，比 MC胜在容量和对存储时效性上。 在所有浏览器缓存中，DC覆盖面是最大的，它会根据HTTP Header中的字段（如：Cache-Control、ETag、Last-Modified ）判断哪些资源需要被缓存，哪些资源可以不请求直接使用，哪些资源已经过期需要重新请求。 通常，在跨站点时，只要相同地址的资源一旦被硬盘缓存下来，就不会再次去请求，因此绝大部分时候数据都来源于DC。但所有的持久化存储都会面临容量增长的问题，DC也不例外，在浏览器自动清理时，会有特殊的算法去把“最老的”或“最可能过时的”资源删除，因此是一个一个删除的，不过每个浏览器是缓存清除算法不尽相同，这点也可以看作是各个浏览器差异性的体现。 Push CachePush Cache（简称：PC）译作“推送缓存”，是属于HTTP2中新增的内容，当前面三种缓存都没有命中时，它才会被使用。它只会Session中存在，一旦Session结束就会被释放，并且缓存时间也是短暂的。通常在Chrome浏览器中只有5分钟的有效期，同时它也并非严格执行HTTP2头的缓存指令。 如果想进进一步深入了解PC， 推荐阅读文章：HTTP2 push is tougher than I thought 总体缓存流程的总结如果一个请求在上述所有阶段都没有找到缓存，那么浏览器会正式发送网络请求去获取资源，之后为了提升请求的缓存命中率，会把请求的资源添加到缓存中去，具体来说： 根据SW中的handler决定是否存入Cache Storage（额外的缓存位置）。SW是由开发者编写的额外的脚本，且缓存位置独立，出现也比较晚，使用还不算广泛。 MC保存一份资源的引用，以备下次使用。MC是浏览器为了加快读取缓存速度而进行的自身优化行为，不受开发者控制，也不受HTTP协议头的约束，算是一个黑盒。 根据HTTP头部的相关字段（Cache-Control 、Pragma等）决定是否存入DS。DS也是我们平时最熟悉的一种缓存机制，也叫HTTP Cache（因为不像MC，它遵守HTTP协议头中的字段)。值得一提的是，我们常说的强制缓存，协商缓存，以及Cache-Control等都归于此类。 按照缓存类型分类按照缓存类型进行分类，可以分为强制缓存和协商缓存。但需要注意的是，这两种缓存类型都是属于Disk Cache或者叫做HTTP Cache里的一种。 强制缓存强制缓存是指当客户端请求时，会先访问本地缓存看缓存是否存在且未过期，若缓存中资源有效直接返回，否则请求后端服务器获取最新资源，并在响应后存储到本地缓存中。通过减少不必要的网络请求，强制缓存能够显著提升网页的加载速度及用户体验，是性能优化中最有效的缓存策略之一。因此，考虑到性能优化时，强化缓存通常是首先得优化手段。 实现强制缓存的主要依靠HTTP响应头中的相关字段，具体包括： Expires：指定了一个具体的日期时间作为缓存的过期时间，它是一个绝对的时间 (通常设置为当前时间+缓存时长)。尽管它是一个早期引入的头字段，但在与Cache-Control共存的情况下，后者优先级更高。 Cache-Control: 用于指定资源缓存的最大有效时间，在该时间范围内，客户端可以直接使用缓存而无需向服务器发送请求。 Expires字段存在两个主要缺点： 由于它使用绝对时间，用户可能会将客户端本地的时间进行修改，而导致浏览器判断缓存失效，从而重新请求该资源。此外，即使不考虑人为修改，时差或者误差等因素也可能造成客户端与服务端的时间不一致，进而影响缓存准确性。 写法太复杂了。表示时间的字符串多个空格，少个字母，都会导致变为非法属性从而设置失效。 为了解决Expires的不足，在HTTP1.1中引入了 Cache-Control字段，该字段常用值如下(完整的列表可以查看 MDN)： max-age：设置缓存的最大有效时间，单位为秒，是一个相对时间。 must-revalidate：如果超过了 max-age 的时间，浏览器必须向服务器发送请求，验证资源是否还有效。 no-cache：虽然字面意思是“不要缓存”，但实际上还是要求客户端缓存内容的，只是是否使用这个内容由后续的协商缓存来决定。 no-store：真正意义上的“不要缓存”。所有内容都不走缓存，包括强制缓存和协商缓存。 public：所有的内容都可以被缓存（包括客户端和代理服务器， 如 CDN ） private：所有的内容只有客户端才可以缓存，代理服务器不能缓存。默认值。 这些值可以混合使用，例如Cache-control:public, max-age=2592000.在混合使用时，它们的优先级如下图所示： 协商缓存当强制缓存失效时，就需要使用协商缓存，由服务器决定缓存内容是否失效。具体流程为：浏览器先请求本地缓存数据库获取缓存标识，，随后浏览器拿着这个标识和服务器通讯，若资源未发生变化，则服务器返回HTTP状态码304，通知浏览器继续使用缓存，否则返回200状态码及最新资源。 协商缓存在请求数上和未使用缓存时相同，但如果是304状态码，返回的仅仅是一个状态码而已，并没有实际的文件内容，从而显著减小了网络传输体积。它的优化点主要体现在“响应”上面通过减少响应体体积，来缩短网络传输时间。因此和强制缓存相比提升幅度较小，但总比没有缓存好。 协商缓存是可以和强制缓存一起用的，作为强制缓存失效后的一种后备方案，在实际项目中也确实一同出现。对比缓存有 2 组字段（不是两个）： Last-Modified If-Modified-Since Etag If-None-Match Last-Modified If-Modified-Since 服务器通过 Last-Modified 字段告知客户端，资源最后一次被修改的时间。 浏览器将这个值和资源一起记录在缓存数据库中。 再次请求相同资源时时，浏览器从自己的缓存中找出“不确定是否过期的”缓存。因此在请求头中将上次的 Last-Modified 的值写入到请求头的 If-Modified-Since 字段 服务器会将 If-Modified-Since 的值与 Last-Modified 字段进行对比。如果相等，则表示未修改，响应 304；反之，则表示修改了，响应 200 状态码，并返回数据。 然而，该机制存在一定局限性： 如果资源更新的速度是秒以下单位，那么该缓存是不能被使用的，因为它的时间单位最低是秒。 如果文件是通过服务器动态生成的，那么该方法的更新时间永远是生成的时间，尽管文件可能没有变化，所以起不到缓存的作用。 因此在 HTTP1.1 出现了 ETag和 If-None-Match Etag If-None-Match为了解决上述问题，出现了一组新的字段 Etag 和 If-None-Match。 Etag 存储的是文件的特殊标识（一般都是一个 Hash 值），服务器存储着文件的 Etag 字段。后续流程和 Last-Modified 一致，只是 Last-Modified 字段和它所表示的更新时间改变成了 Etag 字段和它所表示的文件 hash，把 If-Modified-Since 变成了 If-None-Match。浏览器在下一次加载资源向服务器发送请求时，会将上一次返回的 Etag 值放到请求头里的 If-None-Match 里，服务器只需要比较客户端传来的 If-None-Match跟自己服务器上该资源的 ETag 是否一致，就能很好地判断资源相对客户端而言是否被修改过了。如果服务器发现 ETag 匹配不上，那么直接以常规 GET 200回包形式将新的资源（当然也包括了新的 ETag）发给客户端；如果 ETag是一致的，则直接返回 304 告诉客户端直接使用本地缓存即可。 以下是对协商缓存中两组字段的简要对比： 精确度 Etag 更精确：Last-Modified 的时间单位为秒，若文件在1秒内多次更新，Last-Modified 无法准确反映变化；而 Etag 基于文件内容生成哈希值，任何修改都会改变该值，能准确识别内容变化。 性能 Last-Modified 更高效：Last-Modified 仅需记录时间戳，服务端开销较小；而 Etag 需通过算法计算哈希值，对服务器性能有一定消耗。 优先级 Etag 优先级更高：当两者同时存在时，服务器会优先校验 Etag，仅在 Etag 不一致或缺失时，才使用 Last-Modified 进行判断。 浏览器的缓存读取规则当浏览器要请求资源时： 从 Service Worker 中获取内容（ 如果设置了 Service Worker ） 查看 Memory Cache 查看 Disk Cache。这里又细分： 如果有强制缓存且未失效，则使用强制缓存，不请求服务器。这时的状态码全部是 200 如果有强制缓存但已失效，使用协商缓存，比较后确定 304 还是 200 发送网络请求，等待网络响应 把响应内容存入 Disk Cache (如果 HTTP 响应头信息有相应配置的话) 把响应内容的引用存入 Memory Cache_(无视 HTTP 头信息的配置) 把响应内容存入 Service Worker的 Cache Storage（ 如果设置了 Service Worker） 其中针对第 3 步，具体的流程图如下： 用户行为对浏览器缓存影响在理解了缓存的基本原理后，还需要关注用户的不同操作会出发浏览器采用不同的缓存策略。主要可分为以下三种情况： 正常打开网页（地址栏输入URL并回车） 浏览器会优先查找Disk Cache。如果存在有效缓存，则直接使用；否则向服务器发送网络请求。 普通刷新（F5 或者工具栏刷新按钮） 由于页面标签页并未关闭，Memory Cache此时是可用的，因此浏览器会优先使用它（如果命中），其次才会检查Disk Cache。 强制刷新（Ctrl + F5 或 Ctrl + Shift + R） 浏览器不使用缓存。因此发送请求头部均带有 Cache-Control: no-cache 和 Pragma: no-cache（为了兼容 HTTP1.0），服务器收到后则会返回 200 状态码及最新的内容。 默认请求与F5与Ctrl + F5刷新 操作 行为 缓存影响 普通加载（直接输入 URL 点击链接 从书签打开） 正常加载流程 优先使用强缓存（Memory → Disk） 刷新（F5） “软刷新”，保留部分缓存 跳过 Memory Cache，但会使用 Disk Cache；仍可能触发协商缓存（304） 强制刷新（Ctrl + F5 Cmd + Shift + R） “硬刷新”，重新拉取 跳过所有缓存（Memory + Disk），直接向服务器请求新资源（200 from network） HTTP缓存协议来自服务器的缓存指令当客户端发出一个get请求到服务器，服务器可能有以下的内心活动：「你请求的这个资源，我很少会改动它，干脆你把它缓存起来吧，以后就不要来烦我了」 为了表达这个美好的愿望，服务器在响应头中加入了以下内容： Cache-Control: max-age=3600ETag: W/121-171ca289ebfDate: Thu, 30 Apr 2020 12:39:56 GMTLast-Modified: Thu, 30 Apr 2020 08:16:31 GMT 这个响应头表达了下面的信息： Cache-Control: max-age=3600，我希望你把这个资源缓存起来，缓存时间是3600秒（1小时） ETag: W/121-171ca289ebf，这个资源的编号是W/121-171ca289ebf Date: Thu, 30 Apr 2020 12:39:56 GMT，我给你响应这个资源的服务器时间是格林威治时间2020-04-30 12:39:56 Last-Modified: Thu, 30 Apr 2020 08:16:31 GMT，这个资源的上一次修改时间是格林威治时间2020-04-30 08:16:31 这个美好的缓存愿望，就这样通过响应头传递给客户端了，如果客户端是其他应用程序，可能并不会理会服务器的愿望，也就是说，可能根本不会缓存任何东西。但是凑巧客户端是一个浏览器，它和服务器一直以来都是相亲相爱的小伙伴，当它看到服务器的这个响应头表达的美好愿望后，立即忙起来： 浏览器把这次请求得到的响应体缓存到本地文件中 浏览器标记这次请求的请求方法和请求路径 浏览器标记这次缓存的时间是3600秒 浏览器记录服务器的响应时间是格林威治时间2020-04-30 12:39:56 浏览器记录服务器给予的资源编号W/121-171ca289ebf 浏览器记录资源的上一次修改时间是格林威治时间2020-04-30 08:16:31 来自客户端的缓存指令当客户端收拾好行李，准备再次请求GET /index.js时，它突然想起了一件事：我需要的东西在不在缓存里呢？此时，客户端会到缓存中去寻找是否有缓存的资源，寻找的过程如下： 缓存中是否有匹配的请求方法和路径？ 如果有，该缓存资源是否还有效呢？ 以上两个验证会导致浏览器产生不同的行为，要验证是否有匹配的缓存非常简单，只需要验证当前的请求方法GET和当前的请求路径/index.js是否有对应的缓存存在即可，如果没有，就直接请求服务器，就和第一次请求服务器时一样，无需赘述。 关键在于如何验证缓存是否有效？非常简单，就是把max-age + Date，得到一个过期时间，看看这个过期时间是否大于当前时间，如果是，则表示缓存还没有过期，仍然有效，如果不是，则表示缓存失效。 完整流程图","tags":["技术原理"],"categories":["技术实践"]},{"title":"阅后记：刘润2025年度演讲《进化的力量》","path":"/2025/10/26/thinking/251026-liurun-speech-2025/","content":"这个周六的晚上，朋友在群里分享了一篇微信公众号文章《刘润年度演讲2025：进化的力量》，文章很长，我花了整整75分钟才读完——印象中这是第一次为一篇文章投入了这么长时间，而且是一口气读完。内容非常打动我，于是趁着今天周日，特意找了这场演讲的视频（全长4小时55分钟）下载下来并上传到网盘（附视频链接），又花了一整个下午认真看完。下面是我个人觉得演讲中一些值得记录的精彩观点。 整场演讲围绕“大迁徙”这一核心主题，系统阐述了当前商业环境下企业生存与增长的六大进化路径。核心命题是：在“生态位干旱化”（如餐饮客流减少、母婴市场萎缩等）的背景下，企业如何活下去并实现持续增长？刘润老师指出，问题的根源在于需求端发生结构性变化，而解决方案是：开启“大迁徙”，主动离开内卷红海，寻找新的需求草原。 演讲共分为六个主题，具体内容如下： 开篇：大迁徙用角马迁徙的三个关建选择来比喻企业的出路： 停止内卷：角马面临的选择：留在干旱的塞伦盖蒂与同类争夺有限资源（内卷），或冒险向北迁徙寻找新草原。对应企业：在红海市场中继续价格战，还是寻找新需求。 穿越生死线：角马必须渡过充满鳄鱼的马拉河，对应企业转型过程中的阵痛与风险。 抵达新草原：成功迁徙的角马获得新生，对应企业找到新增长点的美好前景。 大迁徙的本质是：告别内卷、穿越生死、奔向新生。 主题一：品类大迁徙，打破品类僵化 对旧需求，用力过猛；对新需求，反应迟钝。 问题：企业过度成迷优化旧产品（如床仅用于睡觉，餐桌忽略电器需求），忽视新需求。案例： 烤匠麻辣烤鱼：从“好吃”迁移到“情绪价值”（黑金风格、花椒冰淇淋、生日惊喜）。 AirBuggy: 从婴儿车转型到宠物车，核心能力是“为无法言说的爱设计产品”。方法： 停下笔（反思旧需求是否仍然重要）-换卷子（识别用户新抱怨）- 答新题（用能力满足新需求）。 第一，关于“停下笔”：我们产品的本质，到底是为了解决哪个“旧问题”？这个旧问题，今天还重要吗？ 第二，关于“换卷子”：今天，用户正在抱怨的“新问题”是什么？哪些正在涌现？哪些正在变强？ 第三，关于“答新题”：如果我们放下对现有产品的执念，从零开始创造一个新“物种”，它会是什么样子？ 主题二：价值大迁徙，消费心理重构 价值重排，指的是同一个人。同一个人，在心中对一类商品，越来越谨慎，但对另一类商品，花钱不眨眼。价值重排，讲的是“你”和“另一个你”的斗争。是同一个你的内心挣扎。以及挣扎之后的，“该省省，该花花，骑着自行车去酒吧”。 现象：消费者“该省省该花花”（如省打车费，却愿为Labubu花99元），本质是“价值重排” 该省省账户（生存型支出）：追求性价比，如5.9元咖啡。 该花花账户（意义型支出）：追求“心价比”，为热爱情感买单。 方法：提供三种情绪解药： 多回掌控感（如保健品零食化：东阿阿胶软糖） 共鸣软反抗（如哪咤，Labubu的叛逆审美） 创造小满足（如地铁卡做成玉玺造型） 主题三：模式大迁徙，回归能力原点 产品被淘汰，可能只是表象；只要能力不被淘汰，你就还有机会。 关键：区分“产品”（可能被淘汰）和“能力”（可迁移）案例： 京都纹付（百年染坊）：从和服染黑转型二手服装染黑，核心是“深黑染色技术”。 味之素（味精厂）：利用氨基酸技术生产芯片绝缘体（市占率99%）。 微软埋粪案：废物公司Vaulted Deep用深井技术封存碳，卖“碳抵消指标”给微软。方法： 能力盘点（行业消失后还剩什么？）- 市场扫描（谁需要这能力？）- 模式验证（新业务如何赚钱？） 主题四：出海大迁徙，规避盲区 成功的路径，往往千差万别；但失败的陷进，却总是惊人地相似。 心态盲区： 勿用”故乡的地图“导航全球（如印尼用煤气罐而非管道煤气） 勿用”一把尺管理“全球员工（如肯尼亚重族群认可） 战术盲区： 忌“刻舟执念”（强卖中国产品） 忌“焦土竞争”（价格战毁市场） 忌“淘金客心态”（不贡献本地就业税收） 格局盲区： 克服”荒野恐惧”（肯尼亚电商需自建提货点、支付系统） 补“合规短板”（ESG规则） 终极忠告： 反对“遥控式出海”，必须肉身实地调研。 主题五：智能大迁徙，与AI智能体共生 我们以为数字分身的上限是“以假乱真”，但其实它的起点是“解放真人”。AI不会让新手凭空起飞，但它能给高手插上翅膀。 智能体分五级（L1-L5),核心是让AI从“聊天工具”变成“行动主体”： L1数字分身（模仿）：如罗永浩数字人直播带货5500万。 L2创作伙伴（思考）：如AI生成视频（角马过河），人做导演。 L3执行助手（执行）：如Base 44 用AI 3分钟开发销售培训APP。 L4协作团队（组队）：如AI写作团（主编、写手、资料员协作改稿）。 L5具身智能（物理改造）：如人形机器人（宇树科技）兼容人类环境。 主题六：人口大迁徙，服务银发经济 年轻时，我们想尽办法省时间；等到老了，我们又得想尽办法花时间。用看见，去回应“奋斗者”的渴望。应该是，用陪伴，去驱散“自由者”的孤独。应该是，用科技，去接住“托付者”的尊严。 中国3亿“婴儿潮”（1962-1975生）进入50+岁，分为三类： 奋斗者（50-65岁）：忙累需被看见（如中老年短剧《闪婚老伴是豪门》充值3000万）。 自由者（65-75岁）：孤独需陪伴（如健身教练上门陪老人并记录“口述史”）。 托付者（75+岁）：失能需尊严（如助浴仓用雾化技术免水洗）。 结尾：第四百万零一种活法 所以，进化，是人类最好的战略顾问。 刘润老师表达了在充满不确定性的时代，企业不应只模仿“狮子”或“角马”这两种显而易见的生存策略，而应该从自然界400万种物种的进化智慧中，找到属于自己的第400万零一种活法。 向进化求战略 哨刺金合欢的“空房子”策略：共生​ 曾聪明的增长不是竞争，而是共生 织布鸟的“样板间”策略：终局体验​ 最精彩的开局，是让客户直接看到终局。 其他生物的生存智慧​ 汤姆逊瞪羚​​：通过夸张的跳跃（炫耀体力）来告诉猎豹“我很强，别追我”，从而​​避免战斗​​。 ​斑马​​：依靠黑白条纹在群体奔跑时让狮子眼花缭乱，从而​​模糊目标​​。 ​疣猪​​：因腿长脖子短，采用“跪姿”吃草，以​​妥协​​解决自身身体结构的短板。 一些精彩的观点 当水草不在丰美，停在原地不是坚守，而是一种错付。 大迁徙的终点，有时，不是让你去到一个新地方，而是让你成为一个新自己。 所有产品的价值，都在于满足用户的需求。“产品”本身没有价值，能满足“需求”的产品，才有价值。但是，消费者的需求从来不止一个。他们彼此交织，错综复杂，就组成了“需求空间”。 完美适应“昨天”的代价，是失去拥抱“明天”的能力。一旦需求变化，最先陷入变动的，就是曾经的王者。 停下笔，换卷子，答新题。 该省省，该花花，骑着自行车去酒吧。（骑着自行车去酒吧，省下的是路费，花掉的是对苟且生活的反抗） 为什么？因为黑色，是所有颜色的终点，也是所有故事的起点。我们常说，一白遮百丑；但在这里，是一黑获新生。 我们经常犯的一个致命错误，就是把“产品”当成了“能力”。，然而，能力是“根”，产品是“花”。 你眼里的“副产品”，可能是别人眼里的“救命稻草”。 不要拿着“故乡的地图”，去导航全世界。放下想当然，学会看当地的地图。 什么是荒野的恐惧？就是只敢走别人走过的路，不敢踏入没人开垦过的荒野。 所谓“契约优先”，就是：关系是“易碎品”，规则才是“承重墙”。 AI不会让新手凭空起飞，但它能给高手插上翅膀。 过去，人的价值在于“解决问题”；未来，人的价值在于“定义问题”。 年轻时，我们想尽办法省时间；等到老了，我们又得想尽办法花时间。 当想象的成本降为零，决策的质量，就无限接近100%。","categories":["时笺"]},{"title":"算法之【排序】","path":"/2025/09/23/dsa/algo_sort/","content":"排序的概述排序算法 常用的： 冒泡排序 插入排序 选择排序 归并排序 快速排序 计数排序 基数排序 桶排序 不常用的： 猴子排序 睡眠排序 面条排序 排序算法分类 按照时间复杂度进行分类 冒泡、插入、选择 ： O(n^2) 快排、归并： O(nlogn) 桶、计数、基数：O(n) 如何分析一个排序算法？ 学习排序算法，除了学习算法原理、代码实现之外，更重要的是要学会如何评价、分析一个排序算法 分析执行效率 最好情况、最坏情况、平均情况时间复杂度 对于要排序的数据，有的接近排序，有的完全无序，有序度不同的数据，对于排序的执行时间效率会差别很大 时间复杂度的系数、常数、低阶 在数据规模很大的时候，通常会忽略系数、常数、低阶，但实际开发中，通常需要排序的数据量级是相对较小的，所以此时需要考虑系数、常数、低阶对排序的执行时间效率的影响。 比较次数和交换（或移动）次数 基于比较的排序算法，通常会涉及两种操作，元素大小比较和元素交换移动， 分析内存消耗 算法的内存消耗可以通过空间复杂度来衡量，在排序算法的空间复杂度中，引入了原地排序概念，对于原地排序算法，就是特指空间复杂度是O(1)的排序算法。 冒泡、插入、选择都是原地排序算法。 分析稳定性 稳定性是衡量排序算法好坏的重要指标。 稳定的排序算法：经过排序后，相等元素之间的原有的先后顺序不变 不稳定的排序算法：经过排序后，相等元素之间的位置发生改变 冒泡排序（Bubble Sort）分析 概念： 冒泡排序只会操作相邻的两个数据。 每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求，如果不满就让他俩互换。 一次冒泡会让至少一个元素移动到它应该在的位置，重复n次，就完成了n个数据的排序。 特点： 冒泡排序是原地排序算法. 冒泡过程只涉及相邻数据的交换操作，只需要常量级的临时空间，所以空间复杂度是O(1) 最好情况时间复杂度：O(n) 最坏情况时间复杂度：O(n^2) 平均时间复杂度：O(n^2) 结合概率论知识进行分析 引入有序度和逆序度进行分析 有序度： 有序度是数组中具有有序关系的元素对的个数。 有序元素对：a[i] = a[j], 如果 i j。 满有序度：完全有序的数组的有序度叫做满有序度 逆序度： 和有序度相反 逆序元素对：a[i] a[j], 如果 i j。 公式分析： 逆序度 满有序度 - 有序度 排序的过程其实就是一种增加有序度，减少逆序度的过程，最后达到 满有序度，就说明排序完成。 冒泡排序包含两个操作原子，比较和交换，每交换一次，有序度加1，不管算法怎么改变，交换次数总是确定的，即为逆序度，也就是：n*(n-1)2–初始有序度。 插入排序（Insertion Sort）分析","tags":["DSA"],"categories":["数据结构与算法"]},{"title":"Python开发必备：解锁魔法方法的实用技巧大全","path":"/2025/09/23/tech/python/250923-py-magic-methods/","content":"Python的魔法方法是Python面向对象编程的精髓，理解魔法方法不仅能让开发者写出更优雅的代码，更重要的是能让你深入理解Python对象模型的工作原理。那么今天我们就来系统性的梳理一下Python魔法方法的基础知识、常用魔法方法与分类，以及应用场景实践。 官方文档：点击前往 -魔法方法介绍 什么是魔法方法？想象一下这样的场景：当你写下 len([1, 2, 3]) 时，Python是如何知道要返回3的？当你使用 + 连接两个字符串时，背后发生了什么？答案就藏在魔法方法中。 # 这些看似普通的操作，背后都有魔法方法在工作print(len([1, 2, 3])) # 调用了 list.__len__()print(hello + world) # 调用了 str.__add__()print(str(42)) # 调用了 int.__str__() 魔法方法是Python中以双下划线开头和结尾的特殊方法，也称为双下划线方法（如__init__、__str__、__new__等）。它们是Python中的一种协议机制，也是面向对象编程的核心机制之一，赋予了开发者自定义类行为的能力，让自定义类能像内置类型一样与Python语法无缝集成。 了解并理解魔法方法对于我们Pythoner来说，不要把它们看作简单的”特殊方法”，而应该理解为： 是协议的实现：魔法方法是Python鸭子类型的核心体现 是语言的扩展点：通过它们可以扩展Python语言本身的能力 是语法的委托机制：Python的语法很大程度上委托给对象本身来定义正如Python之禅所说：”Python应该提供一种方法，但最好是只有一种明显的方法来做这件事。”魔法方法正是这种哲学的完美体现。 魔法方法的特点与分类魔法方法是由Python解释器在特定场景下自动调用，无需显示调用，例如当你使用len(obj) 时，Python会调用对象的__len__方法。由于魔法方法种类繁多，根据功能可以分为以下几类： 对象生命周期管理 对象表示与格式化 运算符重载 属性访问控制 容器与迭代器 同（异）步上下文管理 可调用对象与描述符 一些不常见但很有用的用法 下面将针对常用的场景进行展开分析 魔法方法在不同场景中的分析与实践场景1：对象生命周期管理下列这些魔法方法控制对象的创建，初始化和销毁。 __new__(cls, *args, **kwargs)：控制对象的创建，是真正的构造器，是对象创建过程中的第一个方法（静态方法），它在__init__之前调用。通常情况下，是不需要直接使用__new__，但如果你需要控制对象的创建过程（如：实现单例模式或自定义元类），可以重写它。 __init__(self, *args, **kwargs)：是初始化器，初始化对象的，对象在内存已经分配，即self已经存在，接收参数并设置初始状态，它在每次创建对象时被自动调用，几乎每个类都会实现它。 __del__(self)：定义对象被垃圾回收时的行为。需要注意的是，最好谨慎使用，如果使用不当，可能引发资源管理问题。因为该方法不保证一定被调用，其主要依赖垃圾回收机制的策略。 class Resource: def __new__(cls, *args, **kwargs): 在__init__之前调用 print(__new__: 创建对象实例) instance = super().__new__(cls) return instance def __init__(self, name): 设置对象初始状态 print(f__init__: 初始化资源 name) self.name = name self._resource = self._acquire_resource() def __del__(self): 对象被垃圾回收时调用 print(f__del__: 清理资源 self.name) self._release_resource() def _acquire_resource(self): print(获取系统资源...) return 模拟的资源句柄 def _release_resource(self): print(释放系统资源...)# 使用示例resource = Resource(数据库连接)# 当对象被垃圾回收时，__del__会自动调用 场景2：对象表示与格式化下列这些魔法方法控制对象的字符串表示，用于调试或展示。 __str__(self)：为最终用户设计，要求可读性好，定义str(obj)或print(obj)时的行为，也就是常被用于输出或打印，这个方法应该返回一个易于理解的字符串，用于展示对象的“外观”。 __repr__(self)：为开发者设计，要求明确无误，定义repr(obj)或交互环境中对象的表示，通常用于开发者调试，返回详细信息，它的目标是生成一个可以通过eval()恢复的字符串（即反向构造对象）。 __format__(self, format_spec)：定义format(obj, spec)或f-string 中的格式化的行为，支持自定义格式化。 class Student: def __init__(self, name, age): self.name = name self.age = age def __repr__(self): 这是对象的官方字符串表示，主要给开发者看 return fStudent(name=self.name, age=self.age) def __str__(self): 这是对象的非正式字符串表示，主要给用户看 return fself.name，self.age岁ssw = Student(BluesSen, 32)print(repr(tom)) # Student(name=BluesSen, age=32)print(str(tom)) # ssw，32岁print(tom) # ssw，32岁 场景3：属性访问控制下列这些魔法方法控制对象的访问、设置和删除行为，适合实现动态属性或代理模式。 __getattr__(self, name)：当访问不存在的属性时调用，适合实现灵活的属性延迟加载或代理模式,比如实现一个ORM模型。 __setattr__(self, name, value)：设置属性时调用，但需要注意避免无限递归的问题，实际开发中建议使用 super().__setattr__ 来避免递归。 __delattr__(self, name)：删除属性时调用。 __getattribute__(self, name)：无条件访问任何属性时都会调用。极其强大，也极其危险，容易引发无限递归（必须通过super().__getattribute__来访问属性）。 class LazyObject: def __init__(self): self.exists = 5 def __getattr__(self, name): 只有在找不到已存在的属性时才会调用 print(f__getattr__被调用，尝试获取不存在的属性: name) value = f这是动态生成的属性 name 的值 setattr(self, name, value) # 将其设置为实例属性，下次就不用走__getattr__了 return value # 谨慎使用 __getattribute__，容易导致无限递归！ # def __getattribute__(self, name): # print(f__getattribute__被调用，获取属性: name) # # 必须通过super()来避免递归 # return super().__getattribute__(name)obj = LazyObject()print(obj.exists) # 输出：5 (正常访问，不会触发__getattr__)print(obj.foo) # 输出：__getattr__被调用... - 这是动态生成的属性 foo 的值print(obj.foo) # 输出：这是动态生成的属性 foo 的值 (第二次访问，foo已存在，不再触发__getattr__) 场景4：容器与迭代器首先需要明确的什么是容器、可迭代对象、迭代器对象： 容器：详见Python中的数据类型用法剖析：从底层实现到高效应用 可迭代对象：实现 __iter__，返回迭代器对象（通常 return self） 迭代器对象：实现 __iter__ 和 __next__，返回下一个值，结束时抛出 StopIteration。下列这些魔法方法让类支持容器操作（如in、索引)和迭代。 __len__(self)：定义len(obj)的行为。 __getitem__(self, key)：可以实现自定义对象的下标访问行为，但需要注意的是在实现切片支持时，__getitem__需要处理slice对象。 __setitem__(self, key, value)：可以实现自定义对象的下标赋值行为。 __delitem__(self, key) ：用于实现删除通过键或索引访问的元素的操作。 __iter__(self)、__next__(self) ：使对象可迭代（即支持for循环），这二者可与生成器结合优化内存。 __contains__(self, item) ：定义 in 操作符的行为。 class MyList: def __init__(self, items): self.items = list(items) def __len__(self): return len(self.items) def __getitem__(self, index): return self.items[index] def __iter__(self): return iter(self.items) def __contains__(self, item): return item in self.itemsml = MyList([1, 2, 3])print(len(ml)) # 3print(ml[1]) # 2print(2 in ml) # Truefor x in ml: print(x) # 1, 2, 3 场景5：同步-上下文管理下列这些魔法方法让类支持with语句，用于资源管理。 __enter__(self)：进入with时调用，返回上下文对象。 __exit__(self, exc_type, exc_value, traceback)：退出with时调用，处理异常，需要提醒的是，该方法返回True时可抑制异常，也就是说异常会被“吞掉”，上下文管理器外的代码不会收到异常。这是一个强大但危险的特性。很适合事务处理，如果结合contextlib模块可简化上下文管理器的实现。 class Timer: 一个计时器上下文管理器 def __enter__(self): import time self.start = time.time() return self # as 后面的变量得到的是这个返回值 def __exit__(self, exc_type, exc_val, exc_tb): import time self.elapsed = time.time() - self.start print(f代码块运行耗时: self.elapsed:.4f 秒) # 返回 False 表示不压制异常；返回 True 则异常会被吞掉，外部不会收到异常。 return False# 使用上下文管理器with Timer() as t: # 模拟一些耗时操作 sum(i for i in range(1000000))# 退出 with 块后会自动打印时间# 并且我们还可以访问 t.elapsed 这个属性print(f外部访问耗时: t.elapsed秒) 场景6：异步-上下文管理下列这些魔法方法必须定义在支持异步的类中。 __await__(self)(可等待对象)：必须返回一个迭代器（通常通过 iter() 包装一个生成器）,可用于实现自定义可等待对象（如 asyncio.Future）。 __alter__(self)(异步迭代器)：返回异步迭代器对象，通常return self。 __anext__：必须返回一个awaitable 对象，（通常是 coroutine 或 Task）。 __aenter__(self)(异步上下文管理器)：返回进入时的资源，常为await self.connect(). __aexit__(self, exc_type, exc_value, traceback)：用于异步清理（如关闭连接），可处理异常 StopAsyncIteration 是异步迭代结束的信号（不要手动抛出，除非你控制迭代逻辑）。 # 自定义异步上下文管理器import asyncioclass AsyncDatabase: async def connect(self): print(正在连接数据库...) await asyncio.sleep(1) print(数据库连接成功) async def disconnect(self): print(正在断开数据库...) await asyncio.sleep(0.5) print(数据库已断开) # 异步上下文管理器 def __aenter__(self): return self.connect() # 返回一个协程 def __aexit__(self, exc_type, exc_value, traceback): return self.disconnect() # 返回一个协程# 使用示例async def main(): async with AsyncDatabase() as db: print(数据库操作中...)asyncio.run(main())# 自定义异步迭代器class AsyncCounter: def __init__(self, limit): self.limit = limit self.current = 0 def __aiter__(self): return self async def __anext__(self): if self.current self.limit: await asyncio.sleep(0.1) # 模拟异步操作 self.current += 1 return self.current else: raise StopAsyncIterationasync def main(): async for num in AsyncCounter(5): print(num)asyncio.run(main()) 场景7：可调用对象下列魔法方法让普通对象变为可调用对象，可用于实现函数式接口、装饰器类、回调等。 __call__(self, *args, **kwargs) ：让类的实例像函数一样被调用，适合实现装饰器或函数式接口。 class Counter: 一个计数器，每次调用都会递增 def __init__(self, start=0): self.count = start def __call__(self): 使得实例可以像函数一样被调用：obj() self.count += 1 print(f当前计数: self.count) return self.count# 创建一个计数器实例counter = Counter(10)counter() # 输出：当前计数: 11counter() # 输出：当前计数: 12# 它是有状态的class CallLogger: 记录函数调用信息的装饰器类 def __init__(self, func): self.func = func self.call_count = 0 def __call__(self, *args, **kwargs): self.call_count += 1 print(f开始执行第 self.call_count 次调用: self.func.__name__) result = self.func(*args, **kwargs) print(f函数 self.func.__name__ 执行完毕) return result@CallLoggerdef say_hello(name): print(fHello, name!)say_hello(Alice)# 输出：# 开始执行第 1 次调用: say_hello# Hello, Alice!# 函数 say_hello 执行完毕 场景8：描述符协议下列这些魔法方法，用于创建描述符类，控制属性的访问，本质是属性代理。简单地说，它允许一个对象在作为另一个对象的属性时，自定义其获取、设置、删除行为。常被用于属性验证、懒加载、ORM 字段等场景。 __get__(self, instance, owner)：获取属性值。 __set__(self, instance, value)：设置属性值。 __delete__(self, instance) ：输出属性。 __set_name__(self, owner, name)：在类创建时设置描述符名称（Python3.6+）值得一提：它们是 @property、@classmethod等装饰器的底层实现。在Django中被广泛使用。需要注意：描述符是实现了__get__、__set__或__delete__中至少一个方法的类。 class PositiveNumber: 一个描述符，确保数值是正数 def __init__(self, name): self.name = name def __get__(self, instance, owner): # instance 是使用描述符的类的实例（如Rectangle的实例） # owner 是使用描述符的类本身（如Rectangle类） return instance.__dict__.get(self.name, 0) def __set__(self, instance, value): if value = 0: raise ValueError(值必须是正数) # 将值存储在实例的 __dict__ 中，而不是描述符实例自身 instance.__dict__[self.name] = valueclass Rectangle: width = PositiveNumber(width) # 描述符实例 height = PositiveNumber(height) # 描述符实例 def __init__(self, w, h): self.width = w # 触发 PositiveNumber.__set__ self.height = h # 触发 PositiveNumber.__set__ @property def area(self): return self.width * self.height # 触发 PositiveNumber.__get__r = Rectangle(5, 3)print(r.area) # 输出：15r.width = 10 # 正常设置print(r.area) # 输出：30# r.width = -5 # 会抛出 ValueError: 值必须是正数 上述代码分析： 描述符PositiveNumber的实例作为类属性（width, height）被赋值给Rectangle类。 当对实例r的属性（如r.width）进行访问或赋值时，实际上被拦截，转而调用描述符的__get__或__set__方法。 @property本质上就是一个实现了__get__和__set__的描述符。总结：描述符是实现精细属性管理、ORM框架映射关系等高级功能的利器。 场景9：运算符重载下列这些魔法方法允许自定义类支持Python的运算符（如：+、-、等）。(一) 实现算术运算符： __add__(self, other)：实现加法运算符 +。 __sub__(self, other)：实现减法运算符 -。 __mul__(self, other)：实现乘法运算符 *。 __truediv__(self, other)：实现真除法运算符 /。 __floordiv__(self, other)：实现整数除法运算符 //。 __mod__(self, other)：实现取模运算符 %。 __pow__(self, other[, modulo])：实现幂运算符 **。 __iadd__(self, other)：实现复合运算符 += __isub__(self, other)：实现复合运算符 -= __imul__(self, other) ：实现复合运算符 *=（二）实现位运算符（不常用） __lshift__(self, other)：实现左移位运算符 。 __rshift__(self, other)：实现右移位运算符 。 __and__(self, other)：实现按位与运算符 。 __xor__(self, other)：实现按位异或运算符 ^。 __or__(self, other)：实现按位或运算符 |。（三）实现比较运算符： __lt__(self, other)：实现小于运算符 。 __le__(self, other)：实现小于等于运算符 =。 __eq__(self, other)：实现等于运算符 ==。 __ne__(self, other)：实现不等于运算符 !=。 __gt__(self, other)：实现大于运算符 。 __ge__(self, other)：实现大于等于运算符 =。**友情提醒：**实现比较运算符时，建议使用 functools.total_ordering 装饰器，只需定义 __eq__ 和 __lt__ 即可自动生成其他比较方法。 （四）实现容器类型类似方法（如序列、映射等）： 这些方法允许对象像列表或字典一样被操作。 __getitem__(self, key)：实现通过键或索引获取元素，如 self[key]。 __setitem__(self, key, value)：实现通过键或索引设置元素，如 self[key] = value。 __delitem__(self, key)：实现通过键或索引删除元素，如 del self[key]。 __contains__(self, item)：实现成员测试运算符 in。 __len__(self)：实现内置函数 len()，返回容器的长度。由于方法太多，这里就简单的实现一个计算器功能吧 # 让对象参与数学运算，支持向量的加减法class Vector: def __init__(self, x=0, y=0): self.x = x self.y = y def __repr__(self): return fVector(self.x, self.y) def __add__(self, other): 实现 v1 + v2 或 v1 + number if isinstance(other, Vector): return Vector(self.x + other.x, self.y + other.y) elif isinstance(other, (int, float)): # 标量加法，x和y都加 return Vector(self.x + other, self.y + other) else: return NotImplemented # 告诉Python此操作不支持，让Python尝试 other.__radd__ def __radd__(self, other): 实现 number + v1（当数字在左边时调用） # 向量加法满足交换律，所以直接调用 __add__ return self.__add__(other) def __iadd__(self, other): 实现 v1 += v2 或 v1 += number（就地修改） if isinstance(other, Vector): self.x += other.x self.y += other.y elif isinstance(other, (int, float)): self.x += other self.y += other else: return NotImplemented return self # 就地运算必须返回selfv1 = Vector(1, 2)v2 = Vector(3, 4)print(v1 + v2) # 输出：Vector(4, 6) (调用 __add__)print(v1 + 5) # 输出：Vector(6, 7) (调用 __add__)print(5 + v1) # 输出：Vector(6, 7) (先尝试 int.__add__(5, v1) 失败，然后调用 v1.__radd__(5))v1 += v2print(v1) # 输出：Vector(4, 6) (调用 __iadd__，就地修改) 场景10：一些不常见但很有用的方法 __slots__ ：限制类可添加的属性，优化内存和性能。 __class__ ：获取或修改对象的类，动态性极强。 __copy__(self)：copy.copy(obj)，浅拷贝 __deepcopy__(self, memo)：copy.deepcopy(obj)，深拷贝 __getstate__(self)：pickle 序列化时获取对象状态 _setstate__(self, state)：pickle 反序列化时恢复状态 __dir__(self)：dir(obj)，自定义属性列表 __sizeof__(self)：sys.getsizeof(obj)，返回对象内存大小， __hash__(self)：hash(obj)，用于哈希表（如字典键）， __instancecheck__(self, instance)、__subclasscheck__(self, subclass) ：自定义 isinstance 和 issubclass 行为，通常在元类中使用。 一些开箱即用的实践案例案例1：实现一个像列表和字典一样的类class Playlist: 一个简单的播放列表类 def __init__(self, *songs): self._songs = list(songs) def __len__(self): 实现 len(obj) return len(self._songs) def __getitem__(self, index): 实现 obj[index]，还支持切片！ # 直接返回内部列表的对应项，巧妙利用列表的切片功能 return self._songs[index] def __contains__(self, song): 实现 item in obj return song in self._songs def __iter__(self): 实现迭代，支持 for 循环 return iter(self._songs)my_playlist = Playlist(Song A, Song B, Song C)print(len(my_playlist)) # 输出：3 (调用 __len__)print(my_playlist[1]) # 输出：Song B (调用 __getitem__)print(my_playlist[0:2]) # 输出：[Song A, Song B] (切片也有效！)print(Song B in my_playlist) # 输出：True (调用 __contains__)for song in my_playlist: # 调用 __iter__ print(song) # 依次输出 Song A, Song B, Song C 案例2：实现一个支持对象支持比较类class Student: def __init__(self, name, age): self.name = name self.age = age def __repr__(self): return fStudent(name=self.name, age=self.age) def __eq__(self, other): 定义相等性：姓名和年龄都相同就认为是同一个学生 if not isinstance(other, Student): return False return self.name == other.name and self.age == other.age def __hash__(self): 如果两个对象相等，它们的hash值必须相同 return hash((self.name, self.age))tom1 = Student(Tom, 20)tom2 = Student(Tom, 20)print(tom1 == tom2) # True# 现在可以把学生对象放进set或用作dict的key了students = tom1, tom2print(len(students)) # 1，因为两个Tom被认为是同一个 案:3：实现一个缓存器from functools import wrapsfrom collections import OrderedDictimport timeclass LRUCache: LRU缓存实现 def __init__(self, maxsize=128, ttl=None): self.maxsize = maxsize self.ttl = ttl self.cache = OrderedDict() self.times = if ttl else None def __call__(self, func): @wraps(func) def wrapper(*args, **kwargs): key = self._make_key(args, kwargs) # 检查缓存 if key in self.cache: if self._is_valid(key): # 移到末尾（最近使用） self.cache.move_to_end(key) return self.cache[key] else: # 过期，删除 self._delete_key(key) # 计算结果 result = func(*args, **kwargs) # 存储结果 self._store_result(key, result) return result wrapper.cache_info = self.cache_info wrapper.cache_clear = self.cache_clear return wrapper def _make_key(self, args, kwargs): 创建缓存键 key = args if kwargs: key += tuple(sorted(kwargs.items())) return key def _is_valid(self, key): 检查缓存是否有效 if self.ttl is None: return True return time.time() - self.times[key] self.ttl def _delete_key(self, key): 删除缓存项 del self.cache[key] if self.times: del self.times[key] def _store_result(self, key, result): 存储缓存结果 # 检查容量 if len(self.cache) = self.maxsize: # 删除最老的项 oldest_key = next(iter(self.cache)) self._delete_key(oldest_key) self.cache[key] = result if self.times: self.times[key] = time.time() def cache_info(self): 缓存信息 return fCache size: len(self.cache)/self.maxsize def cache_clear(self): 清空缓存 self.cache.clear() if self.times: self.times.clear()# 使用示例@LRUCache(maxsize=100, ttl=300)def expensive_function(x, y): time.sleep(1) # 模拟耗时操作 return x * y + x ** 2 案例4：实现一个配置管理器class Config: 支持点号访问和字典访问的配置管理器 def __init__(self, **kwargs): self._data = for key, value in kwargs.items(): self[key] = value def __getitem__(self, key): 字典式访问：config[database_url] return self._data[key] def __setitem__(self, key, value): 字典式赋值：config[database_url] = sqlite:///app.db self._data[key] = value def __delitem__(self, key): 删除配置项：del config[database_url] del self._data[key] def __contains__(self, key): 检查是否存在：database_url in config return key in self._data def __len__(self): 配置项数量：len(config) return len(self._data) def __iter__(self): 迭代配置项：for key in config return iter(self._data) def __getattr__(self, name): 点号访问：config.database_url try: return self._data[name] except KeyError: raise AttributeError(f配置项 name 不存在) def __setattr__(self, name, value): 点号赋值：config.database_url = sqlite:///app.db if name.startswith(_): # 私有属性正常处理 super().__setattr__(name, value) else: # 公共属性存储到配置中 if not hasattr(self, _data): super().__setattr__(_data, ) self._data[name] = value def __str__(self): items = [fk=v!r for k, v in self._data.items()] return fConfig(, .join(items)) def keys(self): return self._data.keys() def values(self): return self._data.values() def items(self): return self._data.items() def get(self, key, default=None): return self._data.get(key, default) def update(self, **kwargs): for key, value in kwargs.items(): self[key] = value# 使用示例config = Config( database_url=sqlite:///app.db, debug=True, max_connections=100)# 多种访问方式print(config[database_url]) # sqlite:///app.dbprint(config.database_url) # sqlite:///app.dbprint(debug in config) # True# 动态添加配置config.secret_key = my-secret-keyconfig[cache_timeout] = 300# 迭代配置for key in config: print(fkey: config[key])print(f共有 len(config) 个配置项) 案例5：实现一个单例类class SingletonMeta(type): 单例模式元类 _instances = _lock = def __new__(mcs, name, bases, namespace, **kwargs): 创建新类时调用 print(f创建类 name) cls = super().__new__(mcs, name, bases, namespace) # 为每个类创建一个锁 mcs._lock[cls] = __import__(threading).Lock() return cls def __call__(cls, *args, **kwargs): 创建实例时调用 if cls not in cls._instances: with cls._lock[cls]: # 双重检查 if cls not in cls._instances: print(f创建 cls.__name__ 的第一个实例) instance = super().__call__(*args, **kwargs) cls._instances[cls] = instance else: print(f返回 cls.__name__ 的现有实例) else: print(f返回 cls.__name__ 的现有实例) return cls._instances[cls]class DatabaseConnection(metaclass=SingletonMeta): 数据库连接类（单例） def __init__(self, host=localhost, port=5432): if hasattr(self, _initialized): return self.host = host self.port = port self.connected = False self._initialized = True print(f初始化数据库连接: host:port) def connect(self): if not self.connected: print(f连接到数据库 self.host:self.port) self.connected = True else: print(已经连接到数据库) def query(self, sql): if not self.connected: self.connect() print(f执行查询: sql) return f查询结果 for sql# 测试单例print(=== 创建数据库连接实例 ===)db1 = DatabaseConnection(192.168.1.100)db2 = DatabaseConnection(localhost) # 参数被忽略db3 = DatabaseConnection()print(fdb1 is db2: db1 is db2) # Trueprint(fdb1.host: db1.host) # 192.168.1.100db1.connect()db2.query(SELECT * FROM users) # 使用同一个连接 一些好的实践与建议 开发调试阶段： __repr__ 比 __str__ 更重要，调试时非常有用。 建议使用： 实现 __eq__ 时，通常也要实现 __hash__（除非对象是可变的）。 使用 @total_ordering 装饰器可以减少比较方法的编写量（只需实现 __eq__ 和一个如 __lt__）。 在设计类时最好做到总是检查参数类型，使用NotImplemented而不是抛出异常。 在设计类时考虑使用@dataclass或@attrs来减少样板代码 魔法方法调用有开销,在性能敏感的循环中，直接调用方法可能比操作符更快。 谨慎使用： 在__getattribute__、__setattr__中直接使用self.attr会导致无限递归。必须使用super()或直接操作__dict__。 总结一下 类别 主要方法 应用场景 对象生命周期 __new__, __init__, __del__ 对象创建、初始化、销毁 对象表示与格式化 __str__, __repr__, __format__ 字符串表示、调试输出 运算符重载 __add__, __eq__, __lt__等 数学运算、比较操作 属性访问控制 __getattr__, __setattr__, __getattribute__ 动态属性、代理模式 容器与迭代器 __len__, __getitem__, __iter__ 序列、映射、迭代 上下文管理 __enter__, __exit__ 资源管理、with语句 可调用对象 __call__ 函数式接口、装饰器类 描述符协议 __get__, __set__, __delete__ 属性验证、ORM字段 异步编程 __aenter__, __aexit__, __anext__ 异步上下文、迭代 Python的魔法方法不是“黑魔法”，而是一套精心设计的、用于扩展语言能力的协议系统。作为Python开发者，深入理解并恰当地运用它们，能够让我们设计出API更清晰、更符合Python风格、更强大的代码库。 然而，常言道：能力越大，责任越大…… 魔法方法虽然功能强大，但滥用魔法方法会让代码变得难以理解和调试。我们应该始终遵循“明确胜于隐晦”的Python之禅，只在真正需要让对象模拟内置行为或实现特定协议时才使用它们。","tags":["Python核心知识"],"categories":["Python"]},{"title":"遇见Stellar：我的博客站点的又一次翻新之旅","path":"/2025/09/22/tools/250922-blog-theme-migrate/","content":"最近，无意中看到一个博客主题：hexo-theme-stellar ，第一眼就被它优雅的设计所吸引。随后我仔细阅读了它的使用文档和一些实际案例，越发觉得它强大而富有美感，尤其触动我的是文档中的一句话： 真正的简约远不止删繁就简，而是在纷繁中建立秩序 这句话彻底动摇了我之前一直追求的极简主义理念，之前我使用的是hexo-theme-A4，它确实非常简洁，但在功能和内容组织上略显单一。而 Stellar 不仅外观现代，还内置了丰富的模块和组件，支持博客、知识库、专栏、笔记等多种内容形式，真正做到了“简约而不简单”。于是我决定动手试一试。按照官方文档，我把感兴趣的配置都体验了一遍后，经过几个小时冷静思考，决定当晚就把博客迁移到Stellar，并借此机会好好梳理一下自己的建站历程——毕竟之前尝试过不少博客平台和主题，却一直没有系统的记录，有些细节已经模糊甚至遗忘了。 前置说明 内容排版：Markdown语法介绍，Markdown教程 站点框架：Hexo 站点主题：hexo-theme-stellar 主题配置文档：Stellar开启您全新的博客之旅 主题介绍Stellar 是一个极为强大的综合型 Hexo 主题，包含博客系统、知识库系统、专栏系统、笔记系统，内置海量的标签和动态数据组件。 迁移流程由于迁移前后站点的框架都是hexo,这个我的迁移工作降低了不少难度和时间。 主要迁移流程: # 1. 初始化新站点hexo init blogsite# 2.安装Stellar主题npm i hexo-theme-stellar# 3.在_config.yml中，修改配置theme: stellar# 4.创建主题配置文件（在根目录新建 `_config.stellar.yml`，所有主题相关配置在此编辑。）touch _config.stellar.yml 接下来，将原博客source目录下的所有内容全部拷贝到新站点对应目录中，基本实现无缝切换（主要还是因为我使文档中使用的语法都是通用语法，基本没有使用主题定制语法）。 发布流程我采用Github + CI 的自动化发布方式： 私有仓库 blogsite：存放 Hexo 源码，并设置 GitHub Actions 工作流 公开仓库 sswfive.github.io：存放生成的静态页面，用于 GitHub Pages 展示撰写新文章后，推送至私有仓库触发 CI 流程（如 GitHub Actions），自动构建并发布到公开仓库，即可通过 https://sswfive.github.io 访问。 核心配置步骤 生成部署密钥对 ssh-keygen -t rsa -b 4096 -C your_email@example.com -f github-actions-deploy 将生成的 github-actions-deploy.pub 内容添加到公开仓库的 Deploy Keys（勾选允许写入） 将 github-actions-deploy 私钥内容添加到私有仓库的 Secrets，命名为 DEPLOY_PRIVATE_KEY 创建 GitHub Actions 工作流文件 在私有仓库创建 .github/workflows/deploy.yml name: Deploy to GitHub Pageson: push: branches: [main] workflow_dispatch:jobs: hexo-deployment: runs-on: ubuntu-latest env: TZ: Asia/Shanghai steps: - name: 1. checkout 分支 uses: actions/checkout@v4 with: fetch-depth: 0 - name: 2. Node 环境安装 uses: actions/setup-node@v4 with: node-version: 20 cache: npm - name: 3. 安装依赖和主题 run: | npm ci --no-audit --no-fund npm i hexo-theme-stellar echo ✅ 依赖安装完成 - name: 4. 恢复文章修改时间 run: | git ls-files source/_posts/**/*.md -z | while IFS= read -r -d file; do ts=$(git log -1 --format=%ct -- $file || true) if [ -n $ts ]; then touch -d @$ts $file; fi done - name: Generate static files run: npm run build # 或 hexo generate - name: Deploy to GitHub Pages uses: peaceiris/actions-gh-pages@v3 with: deploy_key: $ secrets.DEPLOY_PRIVATE_KEY external_repository: sswfive/sswfive.github.io # 你的公开仓库 publish_branch: main # 公开仓库的分支 publish_dir: ./public # Hexo 生成目录 commit_message: 🚀 Deploy from private blogsite repo 主要插件与配置","tags":["博客建站"],"categories":["博客建站"]},{"title":"我在Mac上使用的字体搭配方案","path":"/2025/09/22/tools/250922-mac-font-install/","content":"一、字体选择JetBrains Mono 来源：JetBrains官方出品，专为开发者设计 优点：对编程连字符支持好，字符形态清晰，能有效减少视觉疲劳。 霞鹜文楷屏幕阅读版 来源：基于开源字体「文楷」优化，专为屏幕显示打造。 优点：中文显示清晰锐利，字形现代，阅读体验舒适，并提供等宽版本，完美适配代码编辑器。 在 GitHub releases页面上，可以根据下面字体特点和需求进行选择： 文件名 特点 推荐给 LXGWWenKaiGBScreen.ttf 大陆标准，非等宽 主要进行文档阅读的大陆用户 LXGWWenKaiMonoGBScreen.ttf 大陆标准 + 等宽 中国大陆的程序员（强烈推荐） LXGWWenKaiScreen.ttf 通用版，非等宽 普通用户 LXGWWenKaiMonoScreen.ttf 通用版 + 等宽 需要等宽字体的用户 二、安装方法方法1：手动安装 下载字体： JetBrains Mono：访问 官网 下载 霞鹜文楷屏幕阅读版：访问 GitHub Releases 页面，从 Assets 区选择所需文件 安装： 双击下载的 .ttf 文件，用「字体册」应用打开并点击“安装字体”。 方法 2：使用 Homebrew# JetBrains Monobrew install --cask font-jetbrains-mono# 霞鹜文楷目前还是建议使用手动安装 三、在 VS Code 中启用字体安装后，需在编辑器中设置才能生效。以 VS Code 为例： 打开设置 (Cmd + ,)。 搜索 font family。 在设置中添加你的字体，确保将中文字体放在首位： editor.fontFamily: LXGW WenKai Mono GB Screen, JetBrains Mono, Menlo, Monaco, Courier New, monospace 配置逻辑：系统会优先使用「霞鹜文楷」渲染所有字符（包括英文），fallback 到「JetBrains Mono」确保特殊符号正常显示。","tags":["Mac"],"categories":["工具"]},{"title":"算法之【递归】","path":"/2025/09/20/dsa/algo_recursion/","content":"递归（Recursion）概述概念 递归是一种算法，也是一种编程技巧，应用非常广泛。 递归求解问题的分解过程：去的过程叫“递”，回来的过程叫“归” 应用：DFS深度优先搜索、前中后序二叉树遍历 优缺点： 优点： 递归代码表达能力强，写起来简洁 缺点： 空间复杂度高 有堆栈溢出的风险 存在重复计算 过多的函数调用会耗时较多 递归公式Demof(n)=f(n-1)+1 其中，f(1)=1f(1) = 1;f(2) = 2;f(n) = f(n-1)+f(n-2) 什么场景的问题适合用递归解决 需要满足以下三个条件 一个问题的解可以分解为几个子问题的解 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样 存在递归终止条件 不能出现无限循环 如何编写递归代码 核心思想：写出递推公式，找到终止条件，然后将递推公式转化为代码 方法： 找到如何将大问题分解为小问题的规律，并且写出递推公式 推敲终止条件 将递归公式和终止条件翻译成代码 注意： 只要遇到递归，就把它抽象成一个递推公式，不用想一层层的调用关系，不要视图用人脑去分解递归的每个步骤（重复的步骤交给计算机完成） 递归常见问题 递归代码要警惕堆栈溢出 可以声明一个全局变量来控制递归深度，可以一定程度避免堆栈溢出 递归代码要警惕重复计算 通过某种数据结构来保存已经求解过的值，从而避免重复计算 过多的函数调用会耗时较多，空间复杂度高 递归代码改非递归代码 递归代码都可以改为迭代循环的非递归写法 本质上是：将自动递归改为手动递归，本质问题没有解决 实现： 抽象递推公式 初始值和边界条件 用迭代循环实现","tags":["DSA"],"categories":["数据结构与算法"]},{"title":"工具应用：终端的选择从iTerm2到Alacritty","path":"/2025/09/19/tools/250919-terminal-alacritty/","content":"最近把用了五年的 iTerm2 换成了 Alacritty——一个基于 Rust 编写、使用 OpenGL 加速的跨平台终端，大小仅 6M，号称“全球最快的终端仿真器”。实际体验下来，响应速度确实非常流畅。 安装 Alacritty你可以从 GitHub Releases 下载对应平台的安装包，也可以通过 Homebrew 快速安装： brew install --cask alacritty 替换应用图标默认的 Alacritty 图标比较朴素，我选择从 Freepik 下载了一个更符合审美的 .icns 图标。替换方法也很简单： 在「应用程序」中找到 Alacritty.app 按下 Command + i 打开简介窗口 将下载的 .icns 文件拖到左上角的图标上 重启 Alacritty，新图标就会生效 个性化配置我个人偏好沉浸式头部和 JetBrains Mono 字体，以下是配置方法： 安装 JetBrains Mono 字体brew tap homebrew/cask-fontsbrew install --cask font-jetbrains-mono 也可手动安装： 访问 JetBrains Mono 官网下载字体文件 解压后双击 .ttf 或 .otf 文件安装 验证是否安装成功： fc-list | grep JetBrains Mono 导入配置文件 参考一个大佬的配置：点击下载 我将常用配置整合在 alacritty.yml 中，执行以下命令一键导入： mkdir -p ~/.config/alacritty \\curl -fLo ~/.config/alacritty/alacritty.yml \\ https://gw.alipayobjects.com/os/k/j2/alacritty.yml 如果配置与当前版本不兼容，可运行 alacritty migrate 进行迁移。 创建命令行快捷方式建议将 Alacritty 链接到 PATH，方便随时调用： sudo ln -sf /Applications/Alacritty.app/Contents/MacOS/alacritty /usr/local/bin/alacritty 验证安装： alacritty --version 常用快捷键配置在配置中定制了几个高频快捷键： Command + r：清屏 Command + w：隐藏窗口（原为退出） Command + t：新建窗口 Command + Shift + w：关闭当前窗口 Command + Delete：删除整行 Command + f：搜索关键字 Command + ←/→：跳至行首行尾 搭配 Tmux 使用还整合了 Tmux 配置，实现更强大的终端管理： # 安装 Tmuxbrew install tmux# 下载 Tmux 配置curl -fLo ~/.tmux.conf https://gw.alipayobjects.com/os/k/8b/.tmux.conf# 更新 Alacritty 配置以适配 Tmuxcurl -fLo ~/.config/alacritty/alacritty.yml https://gw.alipayobjects.com/os/k/l9/alacritty.yml 参考 https://tw93.fun/2023-02-06/alacritty.html","tags":["终端","Alacritty"],"categories":["ToolBox"]},{"title":"算法之【哈希】","path":"/2025/09/18/dsa/algo_hash/","content":"概念分析:Hash：通常翻译为“散列”，或“哈希”；就是把任意长度的二进制数据，通过散列算法，变换成固定长度的输出，即该输出就是散列值。 哈希冲突（碰撞）: 指不同的数据，得到了同样的散列值，即为发生冲突（碰撞）。 Hash中的算法 也称消息摘要算法消息摘要（Message Digest）又称数字摘要（Digital Digest）,该摘要是一个唯一对应一个消息或文本的固定长度的值，，它由一个单向Hash加密函数对消息进行作用而生成，本质就是：散列表或者哈希值。 SHA算法SHA（Secure Hash Algorithm）算法称为安全散列算法,能计算出一个数字消息对应到的长度固定的字符串（又称消息摘要）的算法。该算法又细分为：SHA-1、SHA-224、SHA-256、SHA-384、SHA-512这五种算法，其中后四种并称为SHA-2算法。 SHA-1在许多安全协定中广为使用，包括TLS和SSL、PGP、SSH、SMIME和IPsec，曾被视为是MD5的后继者但SHA-1的安全性如今被密码学家严重质疑；虽然至今尚未出现对SHA-2有效的攻击，它的算法跟SHA-1基本上仍然相似 特点： 由讯息摘要反推原输入讯息，从计算理论上来说是很困难的。 不可逆 想要找到两组不同的讯息对应到相同的讯息摘要，从计算理论上来说也是很困难的。 任何对输入讯息的变动，都有很高的机率导致其产生的讯息摘要迥异。无冲突 MD5算法MD5即Message-Digest Algorithm 5（信息-摘要算法5），用于确保信息传输完整一致。是计算机广泛使用的杂凑算法之一（又译摘要算法、哈希算法），主流编程语言普遍已有MD5实现。 特点： 1、压缩性：任意长度的数据，算出的MD5值长度都是固定的。 2、容易计算：从原数据计算出MD5值很容易。 3、抗修改性：对原数据进行任何改动，哪怕只修改1个字节，所得到的MD5值都有很大区别。 4、强抗碰撞：已知原数据和其MD5值，想找到一个具有相同MD5值的数据（即伪造数据）是非常困难的。","tags":["DS&A"],"categories":["数据结构与算法"]},{"title":"数据结构之【栈】","path":"/2025/09/17/dsa/250917-stack/","content":"问题向导 问题1：如何实现浏览器的前进和后退功能？ 问题描述：浏览器的前进、后退功能，我想你肯定很熟悉吧？当你依次访问完一串页面 a-b-c 之后，点击浏览器的后退按钮，就可以查看之前浏览过的页面 b 和 a。当你后退到页面 a，点击前进按钮，就可以重新查看页面 b 和 c。但是，如果你后退到页面 b 后，点击了新的页面 d，那就无法再通过前进、后退功能查看页面 c 了。 思路： 使用两个栈来实现 实现： 使用两个栈，X和Y,把首次浏览的页面依次压入栈X, 当点击后退按钮时，再依次从栈X中出栈，并将出栈的数据依次放入栈Y。当点击前进按钮时，依次从栈Y中取出数据，放入栈X中。当栈X中没有数据时，那就说明没有页面可以继续后退浏览了。当栈Y中没有数据，那就说明没有页面可以点击前进按钮浏览了。 栈的概述： 定义： 后进者先出，先进者后出的数据结构称为“栈” 理解： 一摞叠在一起的盘子，放盘子的时候都是从下往上一个一个放，取的时候都是从上往下一个一个的取，不能从中间任意取 操作特性： 栈是一种“操作受限”的线性表，只允许在一端插入和删除数据 类别： 从功能上，数据或链表都能替代栈，但特定的数据结构是对特定场景的抽象，另一方面，数组或链表暴露了太多的操作接口，操作上的确灵活自由，但使用时比较不可控，就会容易出错 使用场景： 当某个数据集合只涉及到在一端插入和删除数据，并且满足后进先出，先进后出的特性，就可以使用栈 栈的复杂度： 空间复杂度：O(1) 时间复杂度：O(1) 实现： 数组实现-顺序栈 链表实现-链式栈 动态扩容的顺序栈实现： 实现： 只需要底层依赖一个支持动态扩容的数组就可以 了。当栈满了之后，我们就申请一个更大的数组，将原来的数据搬移到新数组中。 复杂度 出栈：O(1) 不会涉及到内存的重新分配和数据的搬迁 入栈：最坏：O(n), 最好：O(1) 分析方法：摊还分析法， 摊还分析法细节：为了分析的方便，我们需要事先做一些假设和定义： 栈空间不够时，我们重新申请一个是原来大小两倍的数组； 为了简化分析，假设只有入栈操作没有出栈操作； 定义不涉及内存搬移的入栈操作为 simple-push 操作，时间复杂度为 O(1)。 如果当前栈大小为 K，并且已满，当再有新的数据要入栈时，就需要重新申请 2 倍大小的内存，并且做 K 个数据的搬移操作，然后再入栈。但是，接下来的 K-1 次入栈操作，我们都不需要再重新申请内存和搬移数据，所以这 K-1 次入栈操作都只需要一个 simple-push 操作就可以完成。为了让你更加直观地理解这个过程，我画了一张图。你应该可以看出来，这 K 次入栈操作，总共涉及了 K 个数据的搬移，以及 K 次 simple-push 操作。将 K 个数据搬移均摊到 K 次入栈操作，那每个入栈操作只需要一个数据搬移和一个 simple-push 操作。以此类推，入栈操作的均摊时间复杂度就为 O(1)。通过这个例子的实战分析，也印证了前面讲到的，均摊时间复杂度一般都等于最好情况时间复杂度。因为在大部分情况下，入栈操作的时间复杂度 O 都是 O(1)，只有在个别时刻才会退化为 O(n)，所以把耗时多的入栈操作的时间均摊到其他入栈操作上，平均情况下的耗时就接近 O(1)。 栈的操作 入栈push() 出栈pop() 栈的应用一、在函数中的应用 函数调用栈 操作系统给每个线程分配了一个独立的内存空间，这块内存被组织成为“栈”这种结构，用来存储函数调用时的临时变量，每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。 从代码中我们可以看出，main() 函数调用了 add() 函数，获取计算结果，并且与临时变量 a 相 加，最后打印 res 的值。为了让你清晰地看到这个过程对应的函数栈里出栈、入栈的操作。 二、在表达式求值中的应用 eg: 34+13*9+44-123 实现思路： 编译器就是通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。从左向右遍历表达式，当遇到数字，就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。 如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。 三、在括号匹配中的应用 我们同样简化一下背景。我们假设表达式中只包含三种括号，圆括号 ()、方括号[]和花括号{}，并且它们可以任意嵌套。比如，[] ()[]或[()([])]等都为合法格式，而[()]或[()]为不合法的格式。那我现在给你一个包含三种括号的表达式字符串，如何检查它是否合法呢？ 使用栈来检查表达式中的括号是否匹配：[] ()[]或[()([])] 实现思路 用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“”跟“”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。当所有的括号都扫描完成之后， 如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为非法格式。","tags":["DSA"],"categories":["数据结构与算法"]},{"title":"数据结构之【队列】","path":"/2025/09/15/dsa/250915-equeue/","content":"问题向导 问题1：线程池没有空闲线程时，新的任务请求 线程资源时，线程池该如何处理？各种处理策略又是如何实现的呢？ 思路： 处理策略一：非阻塞的处理方式，直接拒绝任务请求 处理策略二：阻塞的处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理。 实现： 对于处理策略二,引入队列来解决存储排队请求。 对于此队列的实现，有两种方式，基于链表实现和基于数组实现，这两种方式实现思路如下： 链式队列（基于链表实现）：实现一个支持无限排队的无界队列，但随着请求数的增加，请求排队和处理的响应时间也会随之增加，这对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的。 有界队列（基于数组实现）：队列的大小有限，故线程池中排队的请求超过队列大小时，后续请求将会被拒绝，此种方式对相应时间比较敏感的系统来说，就比较合适，但存在另外一个问题，如何设置队列的大小，就很考验开发者的经验和能力（队列太大导致请求太多，队列太小会导致无法充分利用系统资源，发挥最大性能）。 队列概述 概念： 先进者先出的数据结构，就是队列 类比排队买东西 是一种抽象的数据结构 是一种操作受限的线性表数据结构（栈也是如此） 特点： 先进先出 支持在队尾插入元素，在队头删除元素 实现 数组实现-顺序队列 链表实现-链式队列 队列的操作 入队enqueue() 放一个数据到队列的尾部 出队dequeue() 从队列的头部取一个元素 队列的种类 顺序队列 链式队列 循环队列 阻塞队列 并发队列 队列—顺序队列 使用数组来实现 实现思路： 引入两个指针： head指针：指向队头 tail指针：指向队尾 入队操作时：head指针指向下标为0的位置，tail顺次往后移动，当tail移动到最后，表明数组不能继续添加数据了 出队操作时：tail指针指向队列的尾部，head指针顺次往后移动，当head移动tail的位置，表明数组已没有元素。 注意：当tail指针移动到最后，即使数组里还有空闲空间，队列也无法插入数据，这是一个问题 解决方法：对数据进行搬移，在每次进行出队操作时，将出队的元素标记为删除，当队列没有空间时，再集中出发数据搬移操作， 队列—链式队列 使用链表实现 实现思路： 引入两个指针： head指针：指向链表的第一个结点 tail指针：指向链表的最后一个节点 入队操作时： tail-next new_node, tail tail-next； 出队操作时： head head-next 队列—循环队列 主要是解决顺序队列数据搬移的问题，而是实现的一种队列 循环队列：把数组的收尾相连，折成一个环 实现思路： 队列的大小为 8，当前 head4，tail7。当有一个新的元素 a 入队 时，我们放入下标为 7 的位置。但这个时候，我们并不把 tail 更新为 8，而是将其在环中后移一位，到下标为 0 的位置。当再有一个元素 b 入队时，我们将 b 放入下标为 0 的位置，然后 tail 加 1 更新为 1 注意点：需要确定好队空和队满的判定条件。 对空判定： head tail 队满判定：(tail+1)%nhead。 队满时，tail 指向的位置实际上是没有存储数据的。故循环队列 会浪费一个数组的存储空间。 队列—阻塞队列 在队列的基础上增加了阻塞操作， 若在队列为空时，从队头取数据会被阻塞，若在队列已满时，从队尾插入数据会被阻塞，直到队列中有空闲位置在插入数据，然后再返回。 实际应用： 生产者-消费者模型 队列—并发队列 线程安全的队列就是并发队列 基于数组的循环队列，利用CAS原子操作，可以实现高效的并发队列。","tags":["DSA"],"categories":["数据结构与算法"]},{"title":"Python中的数据类型用法剖析：从底层实现到高效应用","path":"/2025/09/14/tech/python/250914-py-data-struct/","content":"在Python的世界里，我们每天都在和 list、tuple、dict、set 打交道。但你有没有想过： 为什么元组比列表快？ 字典为什么查找那么高效？ 列表扩容背后的策略是什么？ set 真的是“无序”的吗？ 这些看似基础的问题，其实都源于Python对数据类型的底层设计。接下来，我们就来深入剖析Python中常见数据类型的本质，从“序列”、“容器”这些抽象概念讲起，到 list 与 tuple、dict 与 set 的对比实践，以及最后的相关的底层实现原理。 理清概念：这些“类型名词”是什么刚开始学Python时，你可能听过这些术语：list 是可变序列”、“str 是扁平序列”、“dict 是容器类型……听起很玄乎，其实他们只是从不同视角对数据结构的分类，我们来一一拆解。 1. 序列类型(Sequence)提到 “序列”，你可以先联想日常生活中的 “排队”—— 每个人（数据）按顺序站好，有自己的位置（索引），这就是序列的核心特征。 什么是序列？广义上，序列是一种连续存储的数据格式，就像把书按顺序摆放在书架的同一层，每个数据都有固定的 “位置编号”，通过编号能快速找到对应数据。简单说，序列就是一组按照顺序排列的数据集合。 什么是Python中的序列类型？ Python对序列做了一层抽象封装，形成了“序列类型”，只要某个数据类型支持一下操作，就可以归为序列类型： 支持索引访问：seq[0] 支持切片操作：seq[1:3] 支持连接与重复：seq1 + seq2、seq * 3 支持成员判断：x in seq Python 中常见的序列类型有：list（列表）、tuple（元组）、str（字符串）、bytes（字节串）、array（数组）。 # 序列通用操作示例demo_list = [1, 2, 3, 4, 5]print(demo_list[2]) # 索引: 3print(demo_list[1:4]) # 切片: [2, 3, 4]print(3 in demo_list) # 成员检测: True 序列类型的两种分类方式 可变序列 vs 不可变序列 类型 特点 示例 可变序列(MutableSequence) 可以动态增删改 list, bytearray, array.array 不可变序列(Sequence) 一旦创建就不能修改，任何“修改”操作都会返回新对象 tuple, str, bytes Tips：tuple虽然是不可变，但如果它内部包含list，你依然可以修改那个list,因为 tuple只保证“引用不变”，不保证“内容不变”。 容器序列 vs 扁平序列 类型 存储内容 特点 示例 容器序列 存储对象的引用 可以嵌套任意类型 list, tuple 扁平序列 存储对象的值本身 更紧凑，效率更高 str, bytes, array 举个例子：[a, b, c] 中每个元素都是对字符串对象的引用；而 abc 是连续的字符值存储在内存中，更节省空间。 2. 数值类型（Number）数值类型是Python中最基础的数据类型之一，专门用来表示数字，主要包括三类： 整数（int）：比如 10、-5、0，Python 的 int 没有大小限制，能存非常大的数（比如 10**100 这样的 “天文数字”）。 布尔值（bool）：特殊的 int 子类，只有 True（等价于 1）和 False（等价于 0）两个值，常用于条件判断。 浮点数（float）：带小数点的数字，比如 3.14、-0.5，注意浮点数有精度限制（比如 0.1 + 0.2 不等于 0.3）。 复数（complex）：形如 a + bj 的数字（j 是虚数单位），比如 2 + 3j，主要用于科学计算场景。 需要注意的是，它们不属于“容器”，因为不包含其他对象，是原子性的。 3. 容器类型(Container)这里的 “容器类型” 和前面提到的 “容器序列” 不一样 —— 它是从 “功能” 角度分类，指所有能 “容纳其他对象” 的数据结构。简单说，只要一个数据类型能把多个对象 “打包” 在一起，就是容器类型。 容器序列：特指 list、tuple 这类支持索引的序列。 容器类型：泛指能容纳其他对象的数据结构，比如 dict、set、list。 Python 中最常用的容器类型有 4 个： list（列表）：可变、有序，能存任意类型数据 tuple（元组）：不可变、有序，能存任意类型数据 dict（字典）：可变、键值对结构，3.7 + 后有序 set（集合）：可变、无序，元素唯一 接下来我们就聚焦这 4 个核心容器类型，从语法、原理、性能到场景，做一次全方位对比。 list vs tuple：动态与静态的选择基本用法对比# list: 可变，灵活demo_list = [AA, BB, 3, True]demo_list = [] # 创建、初始化demo_list = list() # 创建、初始化demo_list.append(3.14) # 添加元素# tuple: 不可变，稳定 demo_tuple = tuple() # 空元组demo_tuple = (1,) # 单元素元组必须有逗号demo_tuple = (1, hello, True) # 多元素元组demo_tuple[0] = 2 # 报错！不支持修改# tuple新增实际是创建新元组demo_tuple_raw= (1, 2.0, a)demo_tuple_new = demo_tuple_raw + (True,) # 创建新元组 (1, 2.0, a, True) 特性 list tuple 是否可变 是 否 是否支持增删改 是 否 是否支持索引负数索引切片 是 是 内存占用 较大 较小 创建速度 慢 快 是否相互转换 tuple() list() list的实现机制Python的list实际上是一个动态扩容顺序表，采用“分离式结构”： 表头（对象元信息）和数据区分开存储。 初始分配8个元素空间，不够时自动扩容。 扩容策略： 小列表（50000）：扩容为原来的 4倍 大列表（≥50000）：扩容为原来的 2倍 这种“过度分配”（over-allocation）策略保证了 append() 操作的均摊时间复杂度为 O(1)。 demo_list = []print(demo_list.__sizeof__()) # 40 字节（空列表）demo_list.append(1)print(demo_list.__sizeof__()) # 72 → 分配了4个元素的空间 (72-40)/8 = 4demo_list.extend(range(4)) # 加满4个print(demo_list.__sizeof__()) # 72demo_list.append(5)print(demo_list.__sizeof__()) # 104 → 扩容！def check_memory_usage(): 查看列表动态扩容过程 lst = [] print(f空列表大小: lst.__sizeof__() bytes) for i in range(10): lst.append(i) print(f添加i后: lst.__sizeof__() bytes, 元素数: len(lst))check_memory_usage() tuple的实现机制tuple 使用“一体式结构”的顺序表，创建后大小固定，不能扩容。 正因为不可变，Python 可以对小元组进行缓存。比如 (1,2,3) 第一次创建后会被缓存，下次再创建相同的元组，直接复用内存，极大提升性能。 list 与 tuple 性能对比使用timeit模块进行测试： # 创建 速度对比python3 -m timeit x=(1,2,3,4,5,6)# 20000000 loops, best of 5: 9.97 nsec per looppython3 -m timeit x=[1,2,3,4,5,6]# 5000000 loops, best of 5: 50.1 nsec per loop# 访问 速度对比python3 -m timeit -s x=[1,2,3,4,5,6] y=x[3]# 10000000 loops, best of 5: 22.2 nsec per looppython3 -m timeit -s x=(1,2,3,4,5,6) y=x[3]# 10000000 loops, best of 5: 21.9 nsec per loop 结论：tuple的创建速度更优，访问速度略快。其原因在于： 静态结构，无需维护扩容信息 Python缓存常用tuple，减少内存的分配开销 更少的功能意味着更小的开销 应用场景选择 场景 推荐类型 存储固定数据（如坐标、配置） tuple 函数返回多个值 tuple（return x, y） 需要频繁增删改的数据 list 用作字典的键 tuple（不可变） 作为集合元素 tuple 使用元组的场景 # 1. 返回经纬度坐标def get_location(): return (longitude, latitude) # 使用元组保证数据不可变# 2. 函数返回多个值def get_user_info(user_id): # 返回用户姓名、年龄、邮箱 return (BluesSen, 32, bluessen@email.com)# 3. 配置信息存储DATABASE_CONFIG = (localhost, 3306, my_db, user, password) 使用列表的场景 # 1. 记录用户一周内看过的帖子IDviewer_owner_id_list = []records = queryDB(viewer_id) # 查询数据库for record in records: viewer_owner_id_list.append(record.id) # 动态添加元素 # 2. 动态数据收集user_actions = [] # 收集用户操作日志def log_action(action): user_actions.append(action) # 动态添加# 3. 需要修改的数据shopping_cart = [apple, banana, milk]shopping_cart.remove(banana) # 修改购物车shopping_cart.sort() # 排序 常用操作汇总list操作： index()：返回指定元素下标 count()：统计元素出现次数 len()：获取列表长度 append()：末尾追加元素 extend()：扩展列表（添加序列中的所有元素） insert()：指定位置插入元素 pop()：删除并返回指定位置元素（默认最后） remove()：移除第一个匹配项 clear()：清空列表 reverse()和sort()：原地反转和排序 tuple操作： index()：查找元素位置 count()：统计元素出现次数 len()：获取元组长度 通用函数： reversed()：返回反转后的迭代器 sorted()：返回排序后的新列表 注意：如果想给元组排序 反转，可以用全局函数 sorted() 和 reversed()，它们会返回新列表 迭代器，不修改原元组： t = (3,1,2)sorted_t = sorted(t) # 输出 [1,2,3]（列表）reversed_t = list(reversed(t)) # 输出 [2,1,3]（转成列表） dict vs set：键值对与去重利器基本用法对比# 字典：键值对demo_dict = # 空字典demo_dict = dict() # 空字典user = name: Alice, age: 25, city: Beijing# 集合：唯一元素demo_set = set() # 空集合只能用set()demo_set = 10, aa, 20.0, False, 50tags = python, web, backend# 成员检测效率对比large_list = list(range(1000000))large_set = set(large_list)# 列表检测: O(n)时间复杂度# 集合检测: O(1)时间复杂度 关键区别：空字典用 或 dict()，空集合只能用 set()—— 因为 优先被解析为字典。 核心特性对比 特性 Dict（字典） Set（集合） 存储结构 键值对（key: value） 单个元素（无键值） 元素唯一性 key 唯一，value 可重复 所有元素唯一（自动去重） 有序性 3.7+ 有序（插入顺序） 无序（无法通过索引访问） 索引访问 通过 key 访问，如 d[key] 无序，无索引 核心用途 高效查询（通过 key 找 value） 去重、集合运 时间复杂度（查增删） O(1) O(1) 哈希表结构演变老版本结构（紧凑但低效）： +-------------------------------+| hash | key | value |+-------------------------------+| 123 | a | 10 || 456 | b | 20 |+-------------------------------+ 新版本结构（稀疏索引 + 紧凑存储）： Indices: [None, 0, None, None, 1, ...]Entries:+------------------+| hash | key | val |+------------------+| 123 | a | 10 || 456 | b | 20 |+------------------+ 优势：节省内存、提升遍历速度、支持有序迭代。 dict和set 的实现机制dict 和 set 的核心都是哈希表（Hash Table），通过哈希函数将键映射到数组索引，实现近乎常数时间的查找。Python采用优化后的哈希表结构，提高了内存利用率和访问效率。 dict 的哈希表结构：存储“哈希值（hash）、键（key）、值（value）”三个元素，支持快速键值查找 text# 索引区（记录元素在数据区的位置）Indices: [None, 0, None, 2, None, 1, ...]# 数据区（存储实际的键值对和哈希值）Entries:[hash0, key0, value0] # 位置0[hash1, key1, value1] # 位置1[hash2, key2, value2] # 位置2 set 只存储 “哈希值（hash）、元素（element）”，因为没有 value，结构更简单, 专注于快速成员检测 textEntries:[hash0, element0] # 位置0[hash1, element1] # 位置1[hash2, element2] # 位置2 操作分析： 插入操作： 计算键的哈希值并与mask做与操作得到位置index 如果位置空，直接插入 如果位置被占用，比较哈希值和键 都相等：更新值（字典）或忽略（集合） 不相等（哈希冲突）：继续寻找空位 查找操作： 类似插入过程，找到对应位置后比较哈希值和键 删除操作： 标记删除位置，等待哈希表调整时真正删除 哈希冲突与扩容机制什么是哈希冲突？ 两个不同的键，计算出相同的哈希值（或索引），就会发生冲突。 Python采用开放寻址法解决冲突：如果位置被占，就找下一个空位。这可能会降低操作效率。为了保证高效性，哈希表始终保持至少13的剩余空间，当空间不足时自动扩容。 如何实现扩容机制 当哈希表使用率 23 时，触发扩容。 扩容为原来的 2~4 倍，并重新哈希所有元素。 这也是为什么 dict 插入操作虽然是 O(1)，但偶尔会“卡一下”——那是它在扩容。 应用实践与技巧 dict的用法实践 # 1. 字典推导式squares = x: x*x for x in range(6)# 0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25# 2. 合并字典dict1 = a: 1, b: 2dict2 = b: 3, c: 4merged = **dict1, **dict2 # a: 1, b: 3, c: 4# 3. 设置默认值from collections import defaultdictword_count = defaultdict(int)for word in words: word_count[word] += 1 # 4. 字典排序示例d = b: 1, a: 2, c: 10# 5. 按key排序d_sorted_by_key = sorted(d.items(), key=lambda x: x[0])# [(a, 2), (b, 1), (c, 10)]# 6. 按value排序d_sorted_by_value = sorted(d.items(), key=lambda x: x[1])# [(b, 1), (a, 2), (c, 10)] dict的场景实践 # 1. 配置管理app_config = debug: True, database: host: localhost, port: 5432, name: myapp , api_keys: google: abc123, aws: def456 # 2. 数据聚合def count_words(text): 统计词频 word_count = for word in text.split(): word_count[word] = word_count.get(word, 0) + 1 return word_count set的用法实践 # 1. 集合运算A = 1, 2, 3, 4B = 3, 4, 5, 6print(A | B) # 并集: 1, 2, 3, 4, 5, 6print(A B) # 交集: 3, 4print(A - B) # 差集: 1, 2# 2. 快速去重numbers = [1, 2, 2, 3, 4, 4, 5]unique = list(set(numbers)) # [1, 2, 3, 4, 5] set 的场景实践 # 1. 权限管理user_roles = admin, editor, viewercurrent_user_roles = editor# 检查权限if current_user_roles user_roles: print(有访问权限)# 2. 数据清洗def remove_duplicates(data): 快速去重并保持顺序 seen = set() return [x for x in data if not (x in seen or seen.add(x))] 常用操作汇总dict 操作： keys()：返回所有键 values()：返回所有值 items()：返回所有键值对 del：删除键值对 clear()：清空字典 遍历：for key in dict或for key, value in dict.items() set 操作： add()：添加元素 update()：添加多个元素（传入序列） remove()：删除指定元素（不存在则报错） discard()：删除指定元素（不存在不报错） pop()：随机删除并返回一个元素（因集合无序需谨慎使用） 如何选择合适的数据类型？场景1：频繁查找# 错误做法：使用列表查找names_list = [Alice, Bob, Charlie] * 1000if David in names_list: # O(n)时间复杂度 pass# 正确做法：使用集合查找 names_set = set(names_list)if David in names_set: # O(1)时间复杂度 pass 场景2：数据记录# 需要修改数据：用列表student_scores = [85, 92, 78] # 可能变化# 不需要修改数据：用元组student_info = (张三, 2021001, 计算机系) # 固定信息 场景3：内存使用优化# 使用__sizeof__()分析内存占用import sysdata_list = [1, 2, 3, 4, 5]data_tuple = (1, 2, 3, 4, 5)print(f列表占用: sys.getsizeof(data_list) bytes)print(f元组占用: sys.getsizeof(data_tuple) bytes) 总结一下 场景 推荐数据结构 原因 动态数据集合 列表(list) 支持频繁增删改 固定数据集合 元组(tuple) 更快的创建和访问速度 键值映射 字典(dict) 快速的键值查找 唯一值集合 集合(set) 快速成员检测和去重 配置信息 元组字典 根据是否需要修改选择 数据缓存 字典 快速的键值访问 记住这几个原则： 需要修改 → 选择列表 不需要修改 → 选择元组 需要快速查找 → 选择字典或集合 需要去重 → 选择集合 需要保持顺序 → 选择列表或元组 理解了这些数据结构的底层原理和特性，能够帮助我们在实际开发中做出更合理的选择，编写出更高效、更优雅的Python代码。","tags":["python工程实践","Python核心知识"],"categories":["Python"]},{"title":"数据结构之【链表】","path":"/2025/09/12/dsa/250912-linkedlist/","content":"问题向导： 问题1：如何实现LRU缓存淘汰算法？ 思路： 维护一个有序的单链表，越靠近链表尾部的结点是越早之前访问的，当有一个新的数据被访问时，从链表头开始顺序遍历链表。 实现： 如果在此数据之前已经被缓存在链表中了，遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。 如果此数据没有在缓存链表中，可以分两种情况考虑： 如果此时缓存未满，则将该结点直接插入到链表的头部。 如果此时缓存已满，则删除链表的尾结点，将新的数据结点插入到链表的头部。 优化实现： 引入散列表（Hash table）来记录每个数据的位置，将缓存访问的时间复杂度降到O(1). 其他实现： 使用数组实现LRU缓存淘汰策略 扩展1： 缓存利用的是空间换时间的设计思想 对于执行较慢的程序，可以通过消耗更多的内存（空间换时间）来进行优化。 对于消耗过多内存的程序，可以通过消耗更多时间（时间换空间）来降低内存的消耗。扩展2： 常见缓存淘汰策略 FIFO(First In, First Out):先进先出策略 LFU(Least Frequently Used): 最少使用策略 LRU(Least Recently Used): 最少使用策略 链表概述：概念： 链表是通过指针将一组零散的内存快串联在一起的一种数据结构。 内存块通常被称为链表的结点，每个链表的结点除了存储数据之外，还需要记录下一个节点的地址的指针（通常称为后继指针next） 对比: 数组：需要一块连续的内存空间来存储，对内存要求比较高 链表：不需要一块连续的内存空间，它通过指针将一组零散的内存块串联使用。 特点： 支持数据的查找、插入、删除操作 相对与数组，链表更适合插入和删除操作、查找的效率则没有数组高。 插入删除 数组O(n) 链表O(1) 随机访问 数组O(1) 链表O(n) 分类： 单链表 循环链表 双向链表 双向循环链表 链表详解：单链表： 特点： 只有一个方向 结点只有一个后继指针next指向后的结点 头结点记录链表的基地址 尾结点指针指向一个空地址NULL 常用操作的时间复杂度： 查找: O(n) 插入 : O(1) 删除: O(1) 循环链表： 特点： 是一种特殊的单链表 和单链表唯一的区别在我尾结点，尾结点指针指向连链表的头结点 和单链表相比，优点是从链尾到链头比较方便，当要处理的数据具有环形结构特点时，就适合采用循环链表（单链表实现相对比较繁琐）。比如：约瑟夫问题。 双向链表： 特点 支持两个方向 每个结点不止有一个后继指针next指向后面的结点，还有前驱指针pre指向前面的结点 相较于单链表，会占用更多的内存。 支持双向遍历，这样也带来了 双向链表操作的灵活性。提高的数据插入和删除的效率。 操作 删除的情况分析： 删除结点中“值等于某个给定值”的结点； 需要从链表头部依次遍历，知道找到值等于给定值的结点，O(n)，然后执行删除操作O(1)，所以时间复杂度为O(n) 删除给定指针指向的结点； 此情况需要知道该结点的前驱结点，而单链表不支持获取前驱结点，故需要从头开始遍历O(n)，直到到 p-nextq，说明 p 是 q 的前驱结点。与之对比，使用双向链表操作比较高效，时间复杂度为O(1)。 双向循环链表 整合了循环链表和双向链表的特点形成了一种链表 首节点的前驱指针指向尾结点，尾结点的后驱指针指向首节点。 链表代码编写技巧指南技巧一：理解指针或引用的含义 指针的理解： 将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。 eg: p-nextq。这行代码是说，p结点中的next指针存储了q结点的内存地址。 p-nextp-next-next。这行代码表示，p结点的next指针存储了p结点的下下一个结点的内存地址。 技巧二：警惕指针丢失和内存泄露 eg: 单链表的插入操作 p-next = x; // 将 p 的 next 指针指向 x 结点；x-next = p-next; // 将 x 的结点的 next 指针指向 b 结点； 分析： p-next 指针在完成第一步操作之后，已经不再指向结点 b 了，而 是指向结点 x。第 2 行代码相当于将 x 赋值给 x-next，自己指向自己。因此，整个链表也就 断成了两半，从结点 b 往后的所有结点都无法访问到了 正确做法： 先将结点 x 的 next 指针指向结点 b，再把结点 a 的 next 指针指向结点 x，这样才不会丢失指针，导致内存泄漏。所以，对于刚刚的插入代码，我们只需要把第 1 行和第 2 行代码的顺序颠倒一下就可以了。 总结： 插入结点时，一定要注意操作的顺序。 删除链表结点时，也一定要记得手动释放内存空间。 技巧三：利用哨兵简化实现难度 哨兵：解决的是国家之间的边界问题，同理，在数据结构所说的哨兵也是解决“边界问题”的，不直接参与业务逻辑。 eg1: 向空链表中插入第一个结点 if (head == null) head = new_node; eg2:单链表结点删除操作 # 删除结点 p 的后继结点p-next = p-next-next;# 删除链表中的最后一个结点if (head-next == null) head = null; 普通实现的方法总结： 针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况进行特殊处理 哨兵方式实现： 引入哨兵结点，在任何时候，不管链表是否为空，head指针都会一直指向这个哨兵结点，也称这个哨兵结点的链表交带头链表。 哨兵结点是不存储数据的 插入排序、归并排序、动态规划都使用了哨兵的方式实现。 技巧四：重点留意边界条件处理检查链表代码是否正确的边界条件有如下几个： 如果链表为空时，代码是否能正常工作？ 如果链表只包含一个结点时，代码能否正常工作？ 如果链表只包含两个结点时，代码能否正常工作？ 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？ 技巧五：举例画图，辅助思考举例法和画图法： 可以找一个具体的例子，把它画在纸上，释放一些脑容量，留更多的给逻辑思考，这样就会感 觉到思路清晰很多。比如往单链表中插入一个数据这样一个操作，我一般都是把各种情况都举一 个例子，画出插入前和插入后的链表变化，如图所示： 技巧六：多写多练，没有捷径 单链表反转 链表中环的检测 两个有序的链表合并 删除链表倒数第n个结点 求链表的中间结点","tags":["DSA"],"categories":["数据结构与算法"]},{"title":"用了pathlib后，我再也不想碰os.path了！","path":"/2025/09/11/tech/python/250308-pylib-pathlib/","content":"在日常Python开发中，文件路径操作是绕不开的话题。你是否还在使用os.path.join()拼接路径？是否还在为Windows和Linux的路径分隔符头疼？是否期待一种更Pythonic的方式来处理路径？ 先来看一个直观的对比，即使你现在还不熟悉pathlib，也能一眼看出来区别： # 传统方式：繁琐且易出错import osbase_path = os.path.dirname(os.path.dirname(os.getcwd()))file_path = os.path.join(base_path, data, users, profile.json)if os.path.exists(file_path): with open(file_path, r) as f: content = f.read()# pathlib方式：简洁优雅from pathlib import Pathfile_path = Path.cwd().parent.parent / data / users / profile.jsonif file_path.exists(): content = file_path.read_text() 看出差异了吗？今天要介绍的，正是这个让文件路径操作变得优雅如诗的Python标准库——pathlib。 pathlib是什么？pathlib是Python 3.4中新增的标准库模块，提供了面向对象的文件系统路径操作方式。官网地址：点击前往 它的核心优势包括： 面向对象API：路径不再是字符串，而是Path对象，方法可以链式调用 跨平台统一：无缝兼容Windows和Unix系统，自动处理不同操作系统的路径差异 直观易懂：代码即文档，一看就懂 功能强大：覆盖了绝大部分文件路径操作需求，包括文件操作、路径查询、模式匹配等功能 日常开发中，我们最常用的是Path类，它可以完全替代os.path的功能。 为什么需要pathlib？ 传统os.path的三大痛点 1. 函数式编程，缺乏连贯性 # os.path: 需要不断传递路径字符串path = /Users/astonwang/projectsparent = os.path.dirname(path)grandparent = os.path.dirname(parent) 2. 跨平台兼容性问题 # Windows: C:\\Users\\Name\\Documents# Linux: /home/name/documents# 需要手动处理路径分隔符差异 3. 代码可读性差 # 这行代码在做什么？一眼看不出来full_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), .., data, config.json) pathlib的核心设计pathlib提供了丰富的类来处理不同场景下的路径操作： 两大分支：纯路径 vs 具体路径# 纯路径：只做路径运算，不访问文件系统from pathlib import PurePath, PurePosixPath, PureWindowsPath# 具体路径：可以访问文件系统，进行I/O操作 from pathlib import Path, PosixPath, WindowsPath 纯路径家族（Pure Path）- 理论派 只负责路径的逻辑运算 不会访问真实的文件系统 适合路径字符串的处理和转换 具体路径家族（Concrete Path）- 实践派 可以创建、读取、写入文件 能够检查文件是否存在 支持所有文件系统操作 # 纯路径操作（无I/O）PurePath├── PurePosixPath # Unix风格└── PureWindowsPath # Windows风格# 具体路径操作（带I/O）Path├── PosixPath # Unix系统└── WindowsPath # Windows系统 日常开发首选：Path类在99%的场景中，你只需要使用Path类： from pathlib import Path# Path会自动选择合适的子类# 在Linux/Mac上 - PosixPath # 在Windows上 - WindowsPathcurrent_dir = Path.cwd()print(type(current_dir)) # 自动适配当前系统 记住这个原则：除非有特殊需求，否则直接用Path就够了！ pathlib VS os.path 的API对比想知道pathlib到底比os.path强在哪里？来看看这个对比表： 功能 pathlib优雅写法 os.path传统写法 获取当前目录 Path.cwd() os.getcwd() 获取主目录 Path.home() os.path.expanduser(~) 路径拼接 Path(a) / b / c os.path.join(a, b, c) 获取绝对路径 path.absolute() os.path.abspath(path) 获取父目录 path.parent os.path.dirname(path) 获取文件名 path.name os.path.basename(path) 判断是否存在 path.exists() os.path.exists(path) 判断是否文件 path.is_file() os.path.isfile(path) 创建目录 path.mkdir(parents=True) os.makedirs(path) 读取文件 path.read_text() open(path).read() 写入文件 path.write_text(data) open(path, w).write(data) 一句话总结：pathlib让你的代码从”写给计算机看”变成”写给人类看”！ Pathlib基础实践指南—Path类pathlib的强大之处在于它丰富的方法库，让我们按使用场景来分类介绍： 路径探索操作获取特殊目录from pathlib import Path# 当前工作目录 - 相当于 os.getcwd()current_dir = Path.cwd()print(f当前目录: current_dir)# 用户主目录 - 相当于 os.path.expanduser(~) home_dir = Path.home()print(f主目录: home_dir)# 检查路径是否存在 - 相当于 os.path.exists()config_file = Path(config.json)if config_file.exists(): print(配置文件存在) 文件搜索pathlib的glob功能堪比终端的find命令，让文件搜索变得简单： # 查找当前目录所有Python文件python_files = list(Path(.).glob(*.py))print(f找到 len(python_files) 个Python文件)# 递归搜索所有Python文件 - 相当于 find . -name *.pyall_python_files = list(Path(.).rglob(*.py))# 查找特定模式的文件test_files = list(Path(.).rglob(test_*.py))config_files = list(Path(.).rglob(**/config/*.json)) 目录遍历# 列出目录内容 - 相当于 os.listdir()for item in Path(.).iterdir(): if item.is_file(): print(f文件: item.name) elif item.is_dir(): print(f目录: item.name)# 递归遍历目录树 - 相当于 os.walk()for dirpath, dirnames, filenames in Path(.).walk(): print(f当前目录: dirpath) print(f子目录: dirnames) print(f文件: filenames[:3]...) # 只显示前3个文件 路径转换# 获取绝对路径 - 相当于 os.path.abspath()relative_path = Path(../data/file.txt)absolute_path = relative_path.absolute()# 解析软链接 - 相当于 os.path.realpath()resolved_path = relative_path.resolve() 路径类型判断# 判断路径类型 - 比 os.path.isfile/isdir 更直观data_path = Path(data.txt)config_dir = Path(config/)# 基础判断if data_path.is_file(): print(这是一个文件)if config_dir.is_dir(): print(这是一个目录)# 比较两个路径是否指向同一个文件 - 相当于 os.path.samefile()if Path(file1.txt).samefile(Path(file2.txt)): print(两个路径指向同一个文件) 文件与目录操作目录操作# 创建目录 - 比 os.makedirs 更简洁new_dir = Path(project/src/utils)# 递归创建目录（相当于 mkdir -p）new_dir.mkdir(parents=True, exist_ok=True)# 删除空目录 - 相当于 os.rmdir()empty_dir = Path(temp)if empty_dir.exists() and empty_dir.is_dir(): empty_dir.rmdir() 文件操作pathlib最让人惊喜的地方就是内置的文件读写方法： # 创建空文件（相当于 touch 命令）log_file = Path(app.log)log_file.touch(exist_ok=True)# 一行代码读取文件 - 告别繁琐的open/closeconfig_text = Path(config.txt).read_text(encoding=utf-8)binary_data = Path(image.png).read_bytes()# 一行代码写入文件Path(output.txt).write_text(Hello, pathlib!, encoding=utf-8)Path(data.bin).write_bytes(bbinary data)# 文件重命名 - 相当于 os.rename()old_file = Path(old_name.txt)new_file = old_file.rename(new_name.txt) 文件信息查询# 获取文件详细信息 - 相当于 os.stat()file_path = Path(document.pdf)stat_info = file_path.stat()print(f文件大小: stat_info.st_size 字节)print(f修改时间: stat_info.st_mtime)print(f创建时间: stat_info.st_ctime) 权限管理# 修改文件权限 - 相当于 chmodscript_file = Path(deploy.sh)script_file.chmod(0o755) # 给脚本添加执行权限# 获取文件所有者信息（Unix/Linux系统）try: owner = script_file.owner() group = script_file.group() print(f所有者: owner, 组: group)except KeyError: print(无法获取所有者信息) pathlib工程实践技巧实践技巧一：链式调用的艺术pathlib最大的魅力在于可以进行优雅的链式调用： # 一行代码完成复杂操作result = (Path.cwd() .parent # 获取父目录 .parent # 再获取父目录 / data # 进入data文件夹 / config.json) # 指向配置文件# 链式判断和操作if (Path.home() / .ssh / id_rsa).exists(): print(SSH密钥存在)# 链式文件处理(Path(temp) .mkdir(exist_ok=True) # 创建临时目录 .joinpath(output.txt) # 创建文件路径 .write_text(Hello, World!)) # 写入内容 实践技巧二：优雅的错误处理def safe_read_config(config_path): 安全读取配置文件 path = Path(config_path) # 检查文件是否存在 if not path.exists(): print(f配置文件不存在: path) return # 检查是否为文件 if not path.is_file(): print(f路径不是文件: path) return # 安全读取 try: content = path.read_text(encoding=utf-8) return json.loads(content) except Exception as e: print(f读取配置失败: e) return 实践建议一：使用 Path 替代字符串拼接# 不推荐：字符串拼接config_path = os.path.join(os.path.expanduser(~), .config, app, settings.json)# 推荐：pathlib方式config_path = Path.home() / .config / app / settings.json 实践建议二：利用 操作符进行路径拼接# 推荐：使用 / 操作符，简洁直观project_root = Path(__file__).parent.parentdata_dir = project_root / dataoutput_file = data_dir / results / foutput_datetime.now().strftime(%Y%m%d).csv 实践建议三：使用上下文管理器# 推荐：安全的文件操作with Path(data.txt).open(w, encoding=utf-8) as f: f.write(重要数据)# 或者更简洁的方式Path(data.txt).write_text(重要数据, encoding=utf-8) 4. 充分利用内置方法# 推荐：使用pathlib内置方法files = [f for f in Path(.).iterdir() if f.suffix == .py]# 更推荐：使用globfiles = list(Path(.).glob(*.py)) pathlib的实践应用场景让我们通过几个真实场景，看看pathlib如何让复杂的文件操作变得简单。 场景一：项目结构分析器假设你要分析一个Python项目的结构，统计代码行数： from pathlib import Pathdef analyze_project(project_path): 分析Python项目结构 project = Path(project_path) # 统计不同类型的文件 py_files = list(project.rglob(*.py)) # 所有Python文件 test_files = list(project.rglob(test_*.py)) # 测试文件 config_files = list(project.rglob(*.json)) + list(project.rglob(*.yaml)) # 计算总代码行数 total_lines = 0 for py_file in py_files: try: lines = len(py_file.read_text(encoding=utf-8).splitlines()) total_lines += lines except Exception: continue print(f项目分析报告 - project.name) print(fPython文件: len(py_files) 个) print(f测试文件: len(test_files) 个) print(f配置文件: len(config_files) 个) print(f总代码行数: total_lines:, 行)# 使用示例analyze_project(/path/to/your/project) 场景二：智能文件整理器整理下载文件夹，按文件类型自动分类： def organize_downloads(downloads_dir): 智能整理下载文件夹 downloads = Path(downloads_dir) # 定义文件类型映射 type_mapping = images: [.jpg, .png, .gif, .jpeg, .svg], documents: [.pdf, .doc, .docx, .txt, .md], videos: [.mp4, .avi, .mkv, .mov], music: [.mp3, .wav, .flac, .m4a], archives: [.zip, .rar, .7z, .tar.gz] # 创建分类文件夹 for folder_name in type_mapping.keys(): (downloads / folder_name).mkdir(exist_ok=True) # 开始整理文件 moved_count = 0 for file_path in downloads.iterdir(): if file_path.is_file(): file_ext = file_path.suffix.lower() # 找到对应的分类 for folder_name, extensions in type_mapping.items(): if file_ext in extensions: new_path = downloads / folder_name / file_path.name file_path.rename(new_path) moved_count += 1 print(f file_path.name → folder_name/) break print(f整理完成！共移动了 moved_count 个文件)# 使用示例 organize_downloads(Path.home() / Downloads) 场景三：路径解析解析文件路径的各个组成部分： def analyze_file_path(file_path): 详细分析文件路径组成 path = Path(file_path) print(f路径分析：path) print(f所在目录: path.parent) print(f完整文件名: path.name) print(f主文件名: path.stem) print(f文件扩展名: path.suffix) print(f是否绝对路径: path.is_absolute()) # 显示目录层级 print(f目录层级:) for i, parent in enumerate(path.parents): print(f 级别 i+1: parent)# 示例analyze_file_path(/xxx/utils/helper.py) 场景四：配置文件管理器优雅地处理应用配置： class ConfigManager: 配置文件管理器 def __init__(self, app_name): self.app_name = app_name # 创建配置目录 self.config_dir = Path.home() / f.app_name self.config_dir.mkdir(exist_ok=True) # 定义配置文件路径 self.config_file = self.config_dir / config.json self.log_file = self.config_dir / app.log def save_config(self, config_data): 保存配置 import json self.config_file.write_text( json.dumps(config_data, indent=2, ensure_ascii=False) ) print(f配置已保存到: self.config_file) def load_config(self): 加载配置 if self.config_file.exists(): import json return json.loads(self.config_file.read_text()) return def log_message(self, message): 写入日志 from datetime import datetime timestamp = datetime.now().strftime(%Y-%m-%d %H:%M:%S) log_entry = f[timestamp] message # 追加写入日志 with self.log_file.open(a, encoding=utf-8) as f: f.write(log_entry)# 使用示例config_mgr = ConfigManager(myapp)config_mgr.save_config(theme: dark, language: zh-CN)config_mgr.log_message(应用启动) 写在最后从os.path到pathlib，这不仅仅是一个库的替换，更是Python在”优雅”和”可读性”道路上的又一次进化。 下次当你需要处理文件路径时，不妨试试pathlib。相信我，一旦习惯了这种优雅的写法，你就再也回不去os.path的繁琐世界了。 参考资料： 官方文档 PEP 428: The pathlib module","tags":["python库"],"categories":["Python"]},{"title":"数据结构之【数组】","path":"/2025/09/10/dsa/250910-array/","content":"概念与原理数组（Array）是一种线性表数据结构，它用一组连续的内存空间，来存储一组具有相同类型的数据。数组可以分为静态数组和动态数组两大类。 静态数组：是一块连续的内存空间，我们可以通过索引来访问这块内存空间中的元素，这也是数组的原始形态。 动态数组：是编程语言为了方便开发者使用，在静态数组的基础上添加了一些常用的API，如push, insert, remove 等方法,这些API可以让我们更方便的操作数组元素，不用自己写代码去实现。 数组的常用操作（增删改查） 数据结构的职责就是增删改查，再无其他。 数组的特性适合查询多修改少的操作，在使用时需要警惕数据访问越界的问题。。 查询效率快，因为数组的结构特点是，根据偏移量（下标）访问速度快，所以随机访问性好。 插入、删除效率低，由于其是连续的线性结构，在删除和插入时通常都会伴随着数据的搬迁。 数组的时间复杂度在考虑性能时，通常需要去分析不同API的复杂度；另外，数组和链表通常起到了互补的作用，来应对不同的数据处理场景。数组适合查询，链表适合插入和删除。以下列出了常规情况下不同的API的时间复杂度，但具体问题还需要具体分析。 查询(Access)： 根据下标随机访问的时间复杂度：O(1) 插入(Insert)： 最好情况（末尾插入）时间复杂度：O(1) 最坏情况（头部插入）时间复杂度：O(n) 平均情况时间复杂度：(1+2+…n)n O(n) 删除(Delete)： 最好情况（末尾删除）时间复杂度：O(1) 最坏情况（开头删除）时间复杂度：O(n) 平均情况时间复杂度：O(n) 修改(Update)： 给定指定索引，修改索引对应的元素的值，时间复杂度O(1)。 数组典型应用 随机访问：如果想随机抽取一些样本，那么可以用数组存储，并生成一个随机序列，根据索引实现随机抽样。 排序和搜索：数组是排序和搜索算法最常用的数据结构。快速排序、归并排序、二分查找等都主要在数组上进行。 查找表：当需要快速查找一个元素或其对应关系时，可以使用数组作为查找表。假如我们想实现字符到 ASCII 码的映射，则可以将字符的 ASCII 码值作为索引，对应的元素存放在数组中的对应位置。 机器学习：神经网络中大量使用了向量、矩阵、张量之间的线性代数运算，这些数据都是以数组的形式构建的。数组是神经网络编程中最常使用的数据结构。 数据结构实现：数组可以用于实现栈、队列、哈希表、堆、图等数据结构。例如，图的邻接矩阵表示实际上是一个二维数组。 实践应用实践一：插入操作优化在数组的k个位置插入一个数据，若不涉及排序操作，只被用来存储数据的集合，比较优选的方案是，将第k个位置的数据移动到数据的末尾，再将需要的数据插入到第k位，这样就避免了其他数据的移动，从而提高了插入效率，且时间复杂度被降到O(1).快排的实现思路就是如此。 实践二：删除操作优化通常情况下，删除第k个位置的数据，为了内存的连续性，k后面的数据都需要向前搬迁。但在不考虑数据的连续性的场景中，则可以将多次删除操作集中在一起操作，从而提升删除效率。具体的做法就是：将需要删除的元素进行标记，并不执行真正的删除操作，当数据没有了更多存储空间时，再触发一次真正的删除操作，这样就能减少因为数据搬迁的次数。JVM的标记清除垃圾回收算法就是此思想。 实践三：高级编程语言中封装的容器能否替代数组？如：python中的List， java中的ArrayList、C++中vector等；通常需要根据实际情况来考虑，通常的关注点是性能方面的，比如说： 若不是特别关注性能，使用编程语言中封装的对象即可，因为这些容器封装了常用操作，牺牲了性能，但让使用变得简单，可用于日常的业务开发 若特别关注性能，建议使用原生数组，因为原生性能好，但使用起来相对比较繁琐，一般用于底层框架的开发，如网络库的开发 相关算法题数组题目的解法技巧-双指针技巧在数组中没有真正意义上的指针，通常把索引当做数组中的指针， 左右指针：两个指针相向而行或者相背而行； 快慢指针：两个指针同向而行，一快一慢； 技巧一、快慢指针应用场景： 应用在【原地修改数组】类型的算法题目上 应用在【滑动窗口】类型的算法题目上实践思路； 让慢指针走在后面，快指针走在前面探路，在根据题目中的要求，写出相关判断条件。 对于滑动窗口的类型而言：慢指针在后，快指针在前，两个指针中间的部分就是【窗口】，算法通过扩大或缩小窗口来解决相关问题。 # 框架Demodef demo(nums: List[int], val: int) - int: fast, slow = 0, 0 while fast len(nums): if nums[fast] != val: nums[slow] = nums[fast] slow += 1 fast += 1 return slow 技巧二、左右指针常用算法: 二分查找 # 框架Demodef binary_search(nums: List[int], target: int) - int: # 一左一右两个指针相向而行 left = 0 right = len(nums)-1 while left = right: mid = (right + left) // 2 if nums[mid] == target: return mid elif nums[mid] target: left = mid + 1 else: right = mid - 1 return -1","tags":["DSA"],"categories":["数据结构与算法"]},{"title":"Python异步编程十年沉浮：且看3.14版本重要更新","path":"/2025/09/08/tech/python/250907-py-async-challenges/","content":"本周读到了一篇文章《Python has had async for 10 years – why isn’t it more popular?》，深有感触，文章提出了一个值得深思的问题：Python 早在 2015 年就引入了 async 和 await 关键字，距今已近十年，为什么异步编程在 Python 社区中的普及仍不及预期？ 这篇文章中指出了三大困境，也让我这个从Python2.7时代走过来的开发者深有共鸣。今天就结合我的实践经历，聊聊Python异步编程的过去、现在和未来。 异步编程的三座大山困境一：适用场景受限asyncio 在网络 IO 方面表现出色，但对其他类型的 IO 支持有限。即使是看似异步的文件操作，底层仍然依赖线程池来模拟异步行为。 # 看似异步，实际仍使用线程池async with aiofiles.open(large_file.txt, mode=r) as f: contents = await f.read() # 底层依然是线程池实现 生产环境中出于安全考虑，往往限制使用 io_uring 等系统级异步机制，进一步缩小了 asyncio 的适用范围。 困境二：GIL 的根本限制无论异步代码执行得多么高效，都无法突破 GIL（全局解释器锁）的天花板。asyncawait 虽能优化 IO 密集型任务，但对 CPU 密集型操作帮助有限。 更麻烦的是错误处理——忘记 await 可能导致协程静默失败，这种隐形问题调试起来很棘手。 # 单次调用性能相同sync_result = get_data_sync() # 耗时 1 秒async_result = await get_data_async() # 同样耗时 1 秒# 只有在并发场景下异步才显示优势results = await asyncio.gather(*[ get_data_async() for _ in range(10)]) # 总耗时约 1 秒（而非 10 秒） 困境三：双 API 的维护负担为了兼容性，开发者往往需要同时维护同步和异步两套 API，大大增加了开发和维护成本。 # 同步版本@propertydef user_records(self) - list[Record]: return fetch_from_database(self.user_id)# 异步版本 - API 设计不一致async def get_user_records(self) - list[Record]: return await async_fetch_from_database(self.user_id)# 使用时的心理负担：需要记住哪些方法需要 awaituser.name # 普通属性records = await user.get_user_records() # 异步方法 从魔法方法无法异步化，到属性访问方式的不一致，再到代码重复和测试复杂度增加，都让开发者望而却步。 异步编程的演进之路什么是异步编程？异步编程是一种非阻塞的编程范式，允许程序在等待某些操作（如IO）完成时继续执行其他任务，而不是干等着。 同步 VS 异步的直观对比： import timeimport asyncio# 同步版本 - 顺序执行def sync_demo(): def task(name, delay): time.sleep(delay) print(fname 完成) start = time.time() task(任务1, 1) task(任务2, 1) task(任务3, 1) print(f同步总耗时: time.time() - start:.2f秒)# 异步版本 - 并发执行 async def async_demo(): async def task(name, delay): await asyncio.sleep(delay) print(fname 完成) start = time.time() await asyncio.gather( task(任务1, 1), task(任务2, 1), task(任务3, 1) ) print(f异步总耗时: time.time() - start:.2f秒)# 运行对比sync_demo() # 输出：总耗时约3秒asyncio.run(async_demo()) # 输出：总耗时约1秒 三种实现方式的变迁方式一：回调函数（Callback）——原始方案兼容性好，但代码难以维护（坊间流传一个词：回调地狱） import requestsfrom concurrent.futures import ThreadPoolExecutorimport timedef callback_example(): 回调函数示例 - 早期的异步解决方案 def download_complete(future): 下载完成时的回调函数 try: result = future.result() print(f下载完成，状态码: result.status_code) print(f获取数据长度: len(result.text) 字符) except Exception as e: print(f下载失败: e) print(开始回调函数示例...) start_time = time.time() with ThreadPoolExecutor(max_workers=3) as executor: # 提交多个任务 futures = [] urls = [ https://httpbin.org/delay/1, https://httpbin.org/delay/2, https://httpbin.org/delay/1 ] for url in urls: future = executor.submit(requests.get, url) future.add_done_callback(download_complete) futures.append(future) # 等待所有任务完成 for future in futures: future.result() print(f回调函数示例总耗时: time.time() - start_time:.2f秒)# 运行示例callback_example() 方式二：生成器（Generator）—过渡方案虽然避免了回调地狱，代码变的更清晰了，但需要手动调度，实现变的更复杂了。 import timefrom types import coroutinedef generator_example(): 生成器实现协程 - Python 3.4时代的解决方案 @coroutine def async_sleep(delay): 模拟异步sleep start = time.time() while time.time() - start delay: yield return f休眠了delay秒 def run_coroutines(): 运行多个协程 tasks = [async_sleep(1), async_sleep(2), async_sleep(1)] results = [] while tasks: current = tasks.pop(0) try: next(current) tasks.append(current) # 如果还没完成，放回队列 except StopIteration as e: results.append(e.value) # 协程完成，获取返回值 return results print(开始生成器协程示例...) start_time = time.time() results = run_coroutines() print(f生成器协程结果: results) print(f生成器示例总耗时: time.time() - start_time:.2f秒)# 运行示例generator_example() 方式三：AsyncAwait——现代方案import asyncioimport timeasync def modern_async_example(): 现代async/await示例 async def fetch_data(url, delay): 模拟异步数据获取 print(f开始获取 url) await asyncio.sleep(delay) print(f完成获取 url) return furl 的数据（延迟delay秒） print(开始现代async/await示例...) start_time = time.time() # 并发执行多个异步任务 results = await asyncio.gather( fetch_data(https://api1.com, 1), fetch_data(https://api2.com, 2), fetch_data(https://api3.com, 1), fetch_data(https://api4.com, 1) ) print(所有任务完成结果:) for result in results: print(f - result) print(f现代async/await总耗时: time.time() - start_time:.2f秒)# 运行示例asyncio.run(modern_async_example()) asyncio：现代异步解决方案异步编程的核心优势在于：当一个协程等待 IO 操作时，事件循环可以切换执行其他协程，从而提高整体吞吐量。 asyncio的核心概念协程（Coroutine）：异步函数 是异步编程的基础模块，使用async def定义 import asyncio# 定义一个简单的协程async def say_hello(name): print(f开始向 name 问好) await asyncio.sleep(1) # 模拟耗时操作 print(f你好, name!) return fname 的问候完成# 运行协程的三种方式async def main(): # 方式1: 直接await result1 = await say_hello(Alice) # 方式2: 使用asyncio.create_task task = asyncio.create_task(say_hello(Bob)) result2 = await task # 方式3: 使用asyncio.gather并发执行 results = await asyncio.gather( say_hello(Charlie), say_hello(David), say_hello(Eve) ) return results# 运行主函数results = asyncio.run(main())print(results) 事件循环（Event Loop）： 异步引擎，也是asyncio的核心，负责调度和执行协程 import asyncioasync def demo_event_loop(): print(事件循环demo) # 获取当前事件循环 loop = asyncio.get_running_loop() print(f当前时间: loop.time()) # 安排回调函数 def callback(name): print(f回调函数被调用: name) # 安排定时回调 loop.call_later(2, callback, 2秒后) loop.call_soon(callback, 立即) await asyncio.sleep(3) print(演示结束)asyncio.run(demo_event_loop()) 任务（Task）： 协程的包装器，用于并发执行 async def task_management(): 任务管理演示 async def worker(name, seconds): print(fname 开始工作) await asyncio.sleep(seconds) print(fname 工作完成) return fname 工作了seconds秒 # 创建多个任务 tasks = [ asyncio.create_task(worker(工人1, 2)), asyncio.create_task(worker(工人2, 1)), asyncio.create_task(worker(工人3, 3)) ] print(所有任务已创建，开始并发执行...) # 等待所有任务完成 results = await asyncio.gather(*tasks) print(f所有任务完成: results) # 任务状态检查 for task in tasks: print(f任务 task.get_name(): 完成=task.done(), 结果=task.result())asyncio.run(task_management()) Future对象： 代表一个尚未完成的计算结果 import asyncioasync def future_demo(): # 创建Future对象 future = asyncio.Future() print(fFuture状态: future.done()) # 设置结果（通常在别处） def set_result(): print(设置Future结果) future.set_result(完成!) # 2秒后设置结果 asyncio.get_event_loop().call_later(2, set_result) # 等待Future完成 result = await future print(fFuture状态: future.done()) print(f结果: result) return resultasyncio.run(future_demo()) 低级API vs 高级API对比更全面的低级API的用法请：点进前往 更全面的高级API的用法请：点进前往 类别 API名称 说明 使用场景 低级API loop.create_future() 创建Future对象 需要精细控制时 loop.call_soon() 立即调度回调 优先级高的任务 loop.call_later() 延迟调度 定时任务 loop.run_in_executor() 在线程池中运行 CPU密集型任务 高级API asyncio.run() 运行协程 程序入口点 asyncio.create_task() 创建任务 并发执行协程 asyncio.gather() 并发运行多个任务 等待多个任务完成 asyncio.wait() 更灵活的任务等待 需要超时或优先完成时 asyncio.sleep() 异步等待 模拟IO操作 asyncio高级特性 信号量控制并发数 import asyncioasync def limited_concurrency(): semaphore = asyncio.Semaphore(3) # 最大并发数3 async def limited_task(i): async with semaphore: print(f任务 i 开始) await asyncio.sleep(1) print(f任务 i 完成) return i # 创建10个任务，但最多同时执行3个 tasks = [limited_task(i) for i in range(10)] results = await asyncio.gather(*tasks) print(f所有任务完成: results)asyncio.run(limited_concurrency()) 超时与取消控制 async def timeout_cancel_demo(): 超时和取消demo async def long_running_task(name, seconds): try: print(fname 开始运行) await asyncio.sleep(seconds) print(fname 正常完成) return fname 成功 except asyncio.CancelledError: print(fname 被取消) raise # 超时控制 try: result = await asyncio.wait_for( long_running_task(任务A, 5), timeout=2.0 ) print(f任务A结果: result) except asyncio.TimeoutError: print(任务A超时) # 手动取消 task_b = asyncio.create_task(long_running_task(任务B, 3)) await asyncio.sleep(1) task_b.cancel() try: await task_b except asyncio.CancelledError: print(任务B已取消)asyncio.run(timeout_cancel_demo()) 队列处理 import asyncioimport randomasync def queue_example(): queue = asyncio.Queue(maxsize=3) async def producer(): for i in range(5): await asyncio.sleep(random.random()) await queue.put(f消息 i) print(f生产: 消息 i) async def consumer(): while True: item = await queue.get() await asyncio.sleep(random.random() * 2) print(f消费: item) queue.task_done() # 启动生产者和消费者 producer_task = asyncio.create_task(producer()) consumer_task = asyncio.create_task(consumer()) await producer_task await queue.join() # 等待所有任务完成 consumer_task.cancel()asyncio.run(queue_example()) 常用场景的代码框架import asyncioimport aiohttpasync def advanced_patterns(): # 1. 限制并发数 semaphore = asyncio.Semaphore(5) async def limited_request(url): async with semaphore: async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.text() # 2. 超时控制 try: result = await asyncio.wait_for( limited_request(https://httpbin.org/delay/5), timeout=3.0 ) except asyncio.TimeoutError: print(请求超时) # 3. 任务取消 task = asyncio.create_task(limited_request(https://httpbin.org/delay/10)) await asyncio.sleep(1) task.cancel() try: await task except asyncio.CancelledError: print(任务被取消) 实践中的挑战与解决方案识别适合异步的场景import asyncioimport time# 适合异步：网络 I/O 密集型async def good_async_example(): async def fetch_api_data(api_url): # 模拟网络请求 await asyncio.sleep(1) return f数据来自 api_url start = time.time() # 并发执行多个网络请求 results = await asyncio.gather( fetch_api_data(api1), fetch_api_data(api2), fetch_api_data(api3) ) print(f异步网络请求耗时: time.time() - start:.2f秒) # 约1秒# 不适合异步：CPU 密集型def bad_async_example(): def cpu_intensive_task(n): total = 0 for i in range(n): total += i * i return total # 即使用异步包装，由于 GIL 限制，性能提升有限 async def async_cpu_task(n): return cpu_intensive_task(n)asyncio.run(good_async_example()) 混合使用异步与执行器针对 GIL 限制，我们可以结合异步和执行器来解决不同类型的任务： import asyncioimport concurrent.futuresimport timeasync def hybrid_solution(): def cpu_task(n): CPU 密集型任务 return sum(i * i for i in range(n)) def blocking_io_task(): 阻塞 I/O 任务 time.sleep(1) return 阻塞任务完成 loop = asyncio.get_running_loop() # CPU 密集型任务使用进程池 with concurrent.futures.ProcessPoolExecutor() as process_pool: cpu_futures = [ loop.run_in_executor(process_pool, cpu_task, 100000) for _ in range(4) ] # 阻塞 I/O 使用线程池 with concurrent.futures.ThreadPoolExecutor() as thread_pool: io_futures = [ loop.run_in_executor(thread_pool, blocking_io_task) for _ in range(4) ] # 并发执行所有任务 start_time = time.time() cpu_results = await asyncio.gather(*cpu_futures) io_results = await asyncio.gather(*io_futures) print(f混合方案总耗时: time.time() - start_time:.2f秒)asyncio.run(hybrid_solution()) 统一API设计模式针对双 API 维护问题，这里提供一个实用的解决方案： from typing import Union, Awaitable, overloadimport asyncioclass UnifiedDataService: 统一的数据服务，支持同步和异步调用 @overload def get_data(self, key: str, *, async_mode: bool = False) - str: ... @overload def get_data(self, key: str, *, async_mode: bool = True) - Awaitable[str]: ... def get_data(self, key: str, *, async_mode: bool = False) - Union[str, Awaitable[str]]: 根据 async_mode 参数决定返回同步还是异步结果 if async_mode: return self._async_get_data(key) else: return self._sync_get_data(key) def _sync_get_data(self, key: str) - str: return f同步获取: key async def _async_get_data(self, key: str) - str: await asyncio.sleep(0.1) # 模拟异步操作 return f异步获取: key# 使用示例service = UnifiedDataService()# 同步调用sync_result = service.get_data(key1, async_mode=False)print(sync_result)# 异步调用async def test_async(): async_result = await service.get_data(key1, async_mode=True) print(async_result)asyncio.run(test_async()) 持续关注：Python 3.14 的革命性变化正如文章中所预言的，Python 3.14 带来了两个具有历史意义的并发编程突破，有望彻底改变当前的格局。 PEP 779: Free-Threading 的历史性突破核心改变：移除 GIL 限制，实现真正的多线程并行执行。 # 在 Free-Threading 模式下，这些操作可以真正并行import threadingimport timedef cpu_intensive_task(name, n): total = sum(i * i for i in range(n)) print(f任务 name 完成: total)# 以前受 GIL 限制，现在可以真正并行threads = []start_time = time.time()for i in range(4): thread = threading.Thread( target=cpu_intensive_task, args=[fThread-i, 1000000] ) threads.append(thread) thread.start()for thread in threads: thread.join()print(f总耗时: time.time() - start_time:.2f 秒)# Free-Threading 模式下，4个线程可以真正并行 重大意义： 性能革命：CPU 密集型任务可以充分利用多核处理器 降低复杂性：某些场景下线程可能比协程更简单易用 向后兼容：现有异步代码仍然有效 PEP 734: 多解释器支持核心特性：每个解释器拥有独立的全局状态，提供更安全的并行执行环境。 # 伪代码示例（PEP 734 实现）import interpreters# 创建独立的解释器实例interp1 = interpreters.create()interp2 = interpreters.create()# 在不同解释器中执行代码，完全隔离code1 = def cpu_task(): return sum(i*i for i in range(1000000))result = cpu_task()code2 = async def io_task(): await asyncio.sleep(1) return IO task completedresult = asyncio.run(io_task())# 并行执行，每个解释器独立运行interp1.exec(code1)interp2.exec(code2) 对异步编程的影响竞争与互补的新格局 # 场景分析：不同工作负载的最优选择# I/O 密集型 - 异步仍然是最佳选择async def io_heavy_work(): async with aiohttp.ClientSession() as session: tasks = [session.get(url) for url in urls] return await asyncio.gather(*tasks)# CPU 密集型 - Free-Threading 提供新选择 def cpu_heavy_work(): with ThreadPoolExecutor() as executor: futures = [executor.submit(compute, data) for data in datasets] return [f.result() for f in futures]# 混合工作负载 - 多解释器方案def hybrid_work(): io_interp = interpreters.create() cpu_interp = interpreters.create() # 不同类型的任务在不同解释器中执行 项目选型的一些建议立即可行的方案： IO 密集型项目：继续使用 FastAPI + aiohttp 异步方案 CPU 密集型项目：采用多进程方案或等待 Free-Threading 稳定 混合负载项目：考虑 asyncio + 执行器的混合方案 Web 框架推荐： 新项目：FastAPI（异步优先） Django 4.1+（异步支持） 现有项目：渐进式引入异步，在 IO 边界使用 HTTP 客户端推荐： 异步场景：aiohttp httpx（异步模式） 同步场景：httpx（同步模式） requests 异步编程的未来可期异步编程就是一把双刃剑——用对了场景能大幅提升性能，用错了反而增加复杂度，开篇引入的那边文章就是很好的揭示了这一点。 当下：在适合的场景（Web API、爬虫、实时应用）继续使用异步编程 未来：关注 Python 3.14+ 的发展，准备拥抱多样化的并发解决方案 技术选型没有银弹，只有最适合的方案。随着 Python 并发能力的不断增强，我相信，未来的异步编程会更加简单、强大和普及。 参考资料： Python has had async for 10 years – why isn’t it more popular? - Anthony Shaw PEP 703: Making the Global Interpreter Lock Optional PEP 734: Multiple Interpreters in the Stdlib asyncio 官方文档","tags":["python工程实践","异步编程"],"categories":["Python"]},{"title":"数据结构的基本知识","path":"/2025/09/02/dsa/250902-data-structure-basic/","content":"数据结构的分类常见的数据结构包括数组、链表、栈、队列、哈希表、树、堆、图，它们可以从逻辑结构和物理结构两个维度进行分类。 逻辑结构：线性与非线性逻辑结构主要描述的是数据元素之间的逻辑关系，它可以分为线性和非线性两大类。简言之，线性结构指数据在逻辑关系上呈线性排列，非线性结构则呈非线性排列。 线性数据结构：数组、链表、栈、队列、哈希表，元素之间是一对一的顺序关系。 非线性数据结构：树、堆、图、哈希表。非线性结构又可以进一步分为树形结构和网状结构： 树形结构：树、堆、哈希表、元素之间一对多的关系 网状结构：图，元素之间是多对多的关系 物理结构：连续和分散物理结构反映了数据在计算机内存中的存储方式。可分为连续空间存储（数组）和分散空间存储（链表）。物理结构从底层决定了数据的访问、更新、增删等操作方法，两种物理结构在时间效率和空间效率方面呈现出互补的特点。 需要注意的是，所有的数据结构都是基于数组、链表或者二者的组合实现的： 基于数组可实现：栈、队列、哈希表、树、堆、图、矩阵、张量（维度 3的数组）等。 基于链表可实现：栈、队列、哈希表、树、堆、图等。 基本数据类型我们日常了解到的文本、图片、视频、语音、3D模型等格式的数据，基本都是由各种基本数据类型组构成。 什么是基本数据类型通常而言，基本数据类型是CPU可以直接进行运算的类型，或者在算法中直接被使用的类型，主要包括以下几种： 整数类型byte、short、int、long 浮点数类型float、double ，用于表示小数。 字符类型 char ，用于表示各种语言的字母、标点符号甚至表情符号等。 布尔类型bool ，用于表示“是”与“否”判断。基本数据类型以二进制的形式存储在计算机中，一个二进制位即为1bit,在绝大多操作系统中， 1字节（byte）由 8比特（bit）组成。 基本数据类型与数据结构的关系基本数据类型提供了数据的“内容类型”，而数据结构提供了数据的“组织方式”。 # 使用多种基本数据类型来初始化数组numbers: list[int] = [0] * 5decimals: list[float] = [0.0] * 5# Python 的字符实际上是长度为 1 的字符串characters: list[str] = [0] * 5bools: list[bool] = [False] * 5# Python 的列表可以自由存储各种基本数据类型和对象引用data = [0, 0.0, a, False, ListNode(0)] 数字编码原码、反码和补码 数字是以“补码”的形式存储在计算机中。 原码：将数字的二进制表示的最高位视为符号位，其中0表示正数，1表示负数，其余位表示数字的值。 反码：正数的反码与其原码相同，负数的反码是对其原码除符号位外的所有位取反。 补码：正数的补码与其原码相同，负数的补码是在其反码的基础上加1。 需要注意的是： 引入反码是为了解决负数不能直接用于用算。 引入补码是为了解决原码和反码都有正负零歧义问题。 浮点数编码为什么int和float长度都是4字节，但float取值范围远大于int? 因为float采用了不同的表示方式，根据 IEEE 754 标准，32-bit 长度的 float 由以下三个部分构成: 符号位S：占1位 指数位E：占8位 分数位N：占23位 float的表示方式包含指数位，导致其取值范围远大于int,但需要明白的是，尽管浮点数float扩展了取值范围，但其副作用是牺牲了精度。同理，双精度double也采用了类似的表示方法。 字符编码在计算机中，所有的数据都是以二进制的形式存储的，字符char也不例外，为了表示字符，需要建立一套字符集，用于规定每个字符和二进制数之间一一对应关系，有了字符集之后，计算机就可以通过查表完成二进制数到字符的转换了。 ASCII 字符集ASCII码，其全称为American Standard Code for Information Interchange（美国标准信息交换代码）：它使用7位2进制数（一个字节的低7位）表示一个字符，最多能够表示128个不同的字符。最开始，ASCII码仅能表示英文，随着计算机的全球化，诞生了一种能够表示更多语言的 EASCII 字符集。它在 ASCII 的 7 位基础上扩展到 8 位，能够表示 256 个不同的字符。 GBK字符集后来，EASCII 仍不够用。为处理数千日常汉字，我国于1980年推出了GB2312，收录6763字。后为兼容生僻字和繁体字，又扩展出GBK，收录21886字，并采用ASCII单字节、汉字双字节的编码方式。 Unicode字符集Unicode的中文名称为“统一码”，是一种通用字符集，理论上能容纳100多万个字符，本质上是给每个字符分配一个编码，称为“码点”，它没有规定在计算机中如何存储这些字符码点，而是直接将所有的字符存储为等长的编码，由于ASCII已经证明了编码英文只需要1字符，这就使得英文在Unicode下编码非常浪费内存空间，为了解决这个问题，产生了下列的UTF-8编码。 UTF-8编码UTF-8是当前使用最广泛的Unicode编码方法，它是一种可变长度的编码，使用1到4字节来表示一个字符，根据字符的复杂性而变，ASCII 字符只需 1 字节，拉丁字母和希腊字母需要 2 字节，常用的中文字符需要 3 字节，其他的一些生僻字符需要 4 字节。 UTF-8 的编码规则并不复杂，分为以下两种情况。 对于长度为 1 字节的字符，将最高位设置为0 ，其余 7 位设置为 Unicode 码点。值得注意的是，ASCII 字符在 Unicode 字符集中占据了前 128 个码点。也就是说，UTF-8 编码可以向下兼容 ASCII 码。这意味着我们可以使用 UTF-8 来解析年代久远的 ASCII 码文本。 对于长度为 n 字节的字符（其中 n 1），将首个字节的高 n 位都设置为 1 ，第 n+1 位设置为 0 ；从第二个字节开始，将每个字节的高 2 位都设置为 10 ；其余所有位用于填充字符的 Unicode 码点。 UTF-8 之外，常见的编码还有 UTF-16 和 UTF-32。 UTF-16：使用 2 或 4 字节表示字符。多数常用字符为 2 字节，且其编码值与 Unicode 码点相同。 UTF-32：每个字符固定 4 字节，空间占用通常更大。 总结对比： 存储空间：UTF-8 对英文高效（1字节）；UTF-16 对部分中文高效（2字节）。 兼容性：UTF-8 通用性最佳，是优先支持的标准。 编程语言的字符编码早期编程语言常采用 UTF-16 或 UTF-32 等长编码存储字符串，便于像数组一样处理，具有以下优势： 随机访问：UTF-16 可直接访问第 nn 个字符，时间复杂度为 O(1)O(1)；UTF-8 为变长编码，需要 O(n)O(n) 时间。 字符计数：UTF-16 获取字符串长度为 O(1)O(1) 操作，UTF-8 需遍历整个字符串。 字符串操作：在 UTF-16 上进行分割、连接、插入等操作更简单；UTF-8 需额外计算以避免编码错误。 现代语言的改进方案 Python：str 类型根据最大 Unicode 码点动态选择存储：全 ASCII 用 1 字节，全在基本多语言平面（BMP）用 2 字节，有超出 BMP 的字符则用 4 字节。 Go：string 内部使用 UTF-8，并提供 rune 表示单个 Unicode 码点。","tags":["DSA"],"categories":["数据结构与算法"]},{"title":"复杂度分析与大O表示法","path":"/2025/09/01/dsa/250901-complexity-analysis/","content":"在工程领域中，大量问题是难以达到最优解的，许多问题只是被“差不多”的解决了。问题的难以程度一方面取决于问题本身的性质，另一方面也取决于观测问题的人的只是储备。人的知识越完备，经验越多，分析问题就会越深入，问题就能被解决的更优雅。 ——摘录 在前面一篇《DSA重学日志:[一]浅析数据结构与算法的核心概念》中简单的用个人理解的方式介绍了下复杂度分析和大O复杂度表示法，接下来正式的引入科学的概念及定义。 在引入之前，先想一个问题：如果我们想准确的预估一段代码的运行时间，应该如何操作呢？ 确定运行平台，包括硬件配置、编程语言、系统环境等，因为这些因素都会影响代码的运行效率。 评估各种计算操作所需要的运行时间，例如加法操作需要1ns, 乘法操作需要10ns,打印操作需要5ns等。 统计代码滑总所有的计算操作，并将所有的操作和执行时间求和，从而得到运行时间。 但实际上，上述的思路仅适合理论情况下，在实际场景中不合理也不现实，因为算法执行效率本就应该与平台无关，另一方面，我们没法准确的获取每种操作的运行时间。 由于实际测试具有较大的局限性，通常都是通过一些计算来评估算法的效率，也就是统计时间增长趋势，这种估算法的方法被称为：渐进复杂度分析（asymptotic complexity analysis）,简称复杂度分析。 复杂度分析的定义复杂度分析描述的是随着输入数据大小的增加，算法执行所需时间和空间的增长趋势。可以从三个方面来理解： 时间和空间资源：分别对应时间复杂度（time complexity）和空间复杂度(space complexity) 随着输入数据大小的增加：反映了算法执行效率与输入数据体量之间的关系。 时间和空间的增长趋势：表示复杂度分析关注不是运行时间和占用空间的具体值，而是时间或空间增长的“快慢” 针对事后统计法存在的局限性提出的解决方案, 不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方案，这种方案叫做复杂度分析法，也叫大O复杂度表示法。复杂度分析法又分为：时间复杂度分析、空间复杂度分析 大O复杂度表示法 分类 大O时间复杂度表示法（又称：渐进时间复杂度，简称：时间复杂度） 大O空间复杂度表示法（又称：渐进空间复杂度，简称：空间复杂度） 公式：T(n)=O(f(n)) T(n) : 表示代码执行的时间，n表示数据规模的大小 fn: 表示每行代码执行次数的总和，这是一个公式，所以用fn表示 O: 表示代码执行时间T(n)与f(n)表达式成正比 如：T(n) 2n * unit_time 、 T(n) (2n2+2n+3)*unit_time。fn: 2n fn: (2n2+2n+3)T(n) O(2n) T(n) O(2n2+2n+3) 大O时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随着数据规模增长的变化趋势，故也称为渐进时间复杂度。 如何进行时间复杂度分析？专业（数学）方法：求出函数渐近上界 首先统计操作数量，然后判断渐近上界。 def algorithm(n: int): a = 1 # +1 a = a + 1 # +1 a = a * 2 # +1 # 循环 n 次 for i in range(n): # +1 print(0) # +1 如上述代码，给定一个输入大小为n的函数，设算法的操作数量是一个关于数据大小n的函数，记为T(n),则以上函数的操作数量为：T(n) 3 + 2n，T(n)是一次函数，说明其运行时间的增长趋势是线性的，因此它的时间复杂度是线性阶。 通常将线性阶的时间复杂度记为O(n),这个数学符号称为大O记号（big-O natation）,表示函数T(n)的渐近上界（asymptotic upper bound）。 由此可见时间复杂度分析本质上是计算“操作数量T(n)”的渐近上界。 公式：T(n)=O(f(n)) T(n) : 表示代码执行的时间，n表示数据规模的大小 fn: 表示每行代码执行次数的总和，这是一个公式，所以用fn表示 O: 表示代码执行时间T(n)与f(n)表达式成正比 由此可见：确定f(n)之后，就可以得到时间复杂度O(f(n)). 实用（估算）方法：大O复杂度表示法方法一：只关注循环执行次数最多的一段代码 通常会忽略T(n)的常量、低阶、系统、只需要记录一个最大阶的量级就好，这段代码执行次数的n的量级，就是整段要分析代码的时间复杂度。 方法二：加法法则：总复杂度等于量级最大的那段代码的复杂度 抽象成公式则为：如果T1(n)O(f(n))，T2(n)O(g(n))；那么T(n)T1(n)+T2(n)max(O(f(n)), O(g(n))) O(max(f(n), g(n))). 方法三：乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积。 抽象为公式则为：T1(n)O(f(n))，T2(n)O(g(n))；那么T(n)T1(n)*T2(n)O(f(n))*O(g(n))O(f(n)*g(n)). 示例： def algorithm(n: int): a = 1 # +0（技巧 1） a = a + n # +0（技巧 1） # +n（技巧 2） for i in range(5 * n + 1): print(0) # +n*n（技巧 3） for i in range(2 * n): for j in range(n + 1): print(0) 完整统计：T(n) 2n(n+1) + (5n+1) + 2 $2n^2 + 7n +3$ 偷懒统计： $n^2 + n$ 上述代码的时间复杂度为：$O(n^2)$ 常见时间复杂度类型 多项式量级： 常量阶O(1)、对数阶O(logn)、线性阶O(n)、线性对数阶O(nlogn)、k次方阶O(n^k) 非多项式量级： 指数阶O(2n) 、 阶乘阶O(n!) 当数据规模n越来越大是，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。 按执行效率从高到低排列常见的有： $$O(1) O(log n) O(n) O(n log n) O(n²) O(2ⁿ) O(n!)$$ 常数阶 对数阶 线性阶 线性对数阶 平方阶 指数阶 阶乘阶 常数阶O(1)常数阶的操作数量与输入数据大小n无关，即不随着n的变化而变化。 常量级时间复杂度，并不是指只执行了一行代码； 只要代码的执行时间不随n的增大而增大，这样的代码的时间复杂度就记作O(1)， 只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，起时间复杂度依然是O(1) def constant(n: int) - int: 常数阶 count = 0 size = 100000 for _ in range(size): count += 1 return count 线性阶O(n)线性阶的操作数量相对于输入数据的大小n以线性级别增长，通常出现在单层循环中。 def linear(n: int) - int: 线性阶 count = 0 for _ in range(n): count += 1 return countdef array_traversal(nums: list[int]) - int: 线性阶（遍历数组） count = 0 # 循环次数与数组长度成正比 for num in nums: count += 1 return count 需要注意的是：输入数据大小n需根据输入数据的类型来具体确定。比如在第一个示例中，变量 为输入数据大小；在第二个示例中，数组长度 为数据大小。 平方阶O($n^2$)平方阶的操作数量相对于输入数据大小n以平方级别增长。通常出现在嵌套循环中，外层循环和内存循环的时间复杂度都为$O(n)$，因此总体的时间复杂度为$O(n^2)$ def quadratic(n: int) - int: 平方阶 count = 0 # 循环次数与数据大小 n 成平方关系 for i in range(n): for j in range(n): count += 1 return count 指数阶 $O(2^n)$在算法中，指数阶常出现于递归函数中。在生物学中的“细胞分裂”也是指数阶增长的典型代表。 def exponential(n: int) - int: 指数阶（循环实现） count = 0 base = 1 # 细胞每轮一分为二，形成数列 1, 2, 4, 8, ..., 2^(n-1) for _ in range(n): for _ in range(base): count += 1 base *= 2 # count = 1 + 2 + 4 + 8 + .. + 2^(n-1) = 2^n - 1 return countdef exp_recur(n: int) - int: 指数阶（递归实现） if n == 1: return 1 return exp_recur(n - 1) + exp_recur(n - 1) + 1 指数阶增长速度非常迅速，在穷举法（暴力搜索、回溯等）中比较常见。对于数据规模较大的问题，指数阶是不可接受的，通常需要使用动态规划或贪心算法等来解决。 对数阶$O(log n)$与指数阶相反，对数阶放映了“每轮缩减到一半”的情况。设输入数据大小为n, 由于每轮缩减到一半，因此循环次数是$log2n$，即$2^n$的反函数。 对数阶也常出现于递归函数和分治策略（一分为多和化繁为简）的算法中。它增长缓慢，是仅次于常数阶的理想的时间复杂度。 def logarithmic(n: int) - int: 对数阶（循环实现） count = 0 while n 1: n = n / 2 count += 1 return countdef log_recur(n: int) - int: 对数阶（递归实现） if n = 1: return 0 return log_recur(n / 2) + 1 线性对数阶$O(nlogn)$线性对数阶常出现于嵌套循环中，两层循环的时间复杂度分别为$O(nlogn)$ 和 $O(n)$ 主流的排序算法的时间复杂度通常为$O(nlogn)$，如：快速排序、归并排序、堆排序等。 def linear_log_recur(n: int) - int: 线性对数阶 if n = 1: return 1 # 一分为二，子问题的规模减小一半 count = linear_log_recur(n // 2) + linear_log_recur(n // 2) # 当前子问题包含 n 个操作 for _ in range(n): count += 1 return count 阶乘$O(n!)$阶乘在算法场景中，通常使用递归实现，在数学场景中，对应“全排列”问题。 def factorial_recur(n: int) - int: 阶乘阶（递归实现） if n == 0: return 1 count = 0 # 从 1 个分裂出 n 个 for _ in range(n): count += factorial_recur(n - 1) return count 在n较大时是不可接受的。 如何进行空间复杂度分析？空间复杂度（space complexity）用于衡量算法占用内存空间随着数据量变大时的增长趋势。其实和时间复杂度很类似，只需将“运行时间”替换成“占用内存空间”就好。 算法相关空间算法在运行过程中使用的内存空间主要包括以下几种。 输入空间：用于存储算法的输入数据。 暂存空间：用于存储算法在运行过程中的变量、对象、函数上下文等数据。 输出空间：用于存储算法的输出数据。 一般情况下，空间复杂度的统计范围是“暂存空间”加上“输出空间”。 暂存空间可以进一步划分为三个部分。 暂存数据：用于保存算法运行过程中的各种常量、变量、对象等。 栈帧空间：用于保存调用函数的上下文数据。系统在每次调用函数时都会在栈顶部创建一个栈帧，函数返回后，栈帧空间会被释放。 指令空间：用于保存编译后的程序指令，在实际统计中通常忽略不计。 在分析一段程序的空间复杂度时，我们通常统计暂存数据、栈帧空间和输出数据三部分 class Node: 类 def __init__(self, x: int): self.val: int = x # 节点值 self.next: Node | None = None # 指向下一节点的引用def function() - int: 函数 # 执行某些操作... return 0def algorithm(n) - int: # 输入数据 A = 0 # 暂存数据（常量，一般用大写字母表示） b = 0 # 暂存数据（变量） node = Node(0) # 暂存数据（对象） c = function() # 栈帧空间（调用函数） return A + b + c # 输出数据 推算方法空间复杂度的推算方法与时间复杂度大致相同，只需将统计对象从“操作数量”转为“使用空间大小”即可。 而与时间复杂度不同的是，我们通常只关注最差空间复杂度，这是因为内存空间是一项硬性要求，我们必须要确保在所有输入数据下都有足够的内存空间预留。 一般的最差有两层含义： 以最差输入数据为准 以算法运行中的峰值内存为准 另外需要注意，在递归函数中，需要统计栈帧空间。 def function() - int: # 执行某些操作 return 0def loop(n: int): 循环的空间复杂度为 O(1) for _ in range(n): function()def recur(n: int): 递归的空间复杂度为 O(n) if n == 1: return return recur(n - 1) 函数 loop() 和 recur() 的时间复杂度都为 ，但空间复杂度不同。 函数 loop() 在循环中调用了 次 function() ，每轮中的 function() 都返回并释放了栈帧空间，因此空间复杂度仍为 。 递归函数 recur() 在运行过程中会同时存在 个未返回的 recur() ，从而占用 的栈帧空间。 常见空间复杂度类型$O(1) O(log n) O(n) O(n²) O(2ⁿ)$ 常数阶 对数阶 线性阶 平方阶 指数阶 常数阶O(1)常数阶常见于数量与输入数据大小 无关的常量、变量、对象。 需要注意的是，在循环中初始化变量或调用函数而占用的内存，在进入下一循环后就会被释放，因此不会累积占用空间，空间复杂度仍为 ： def function() - int: 函数 # 执行某些操作 return 0def constant(n: int): 常数阶 # 常量、变量、对象占用 O(1) 空间 a = 0 nums = [0] * 10000 node = ListNode(0) # 循环中的变量占用 O(1) 空间 for _ in range(n): c = 0 # 循环中的函数占用 O(1) 空间 for _ in range(n): function() 线性阶O(n)线性阶常见于元素数量与n成正比的数组、链表、栈、队列等。 def linear(n: int): 线性阶 # 长度为 n 的列表占用 O(n) 空间 nums = [0] * n # 长度为 n 的哈希表占用 O(n) 空间 hmap = dict[int, str]() for i in range(n): hmap[i] = str(i) def linear_recur(n: int): 线性阶（递归实现） print(递归 n =, n) if n == 1: return linear_recur(n - 1) 平方阶O($n^2$)平方阶常见于矩阵和图，元素数量与n成平方关系： def quadratic(n: int): 平方阶 # 二维列表占用 O(n^2) 空间 num_matrix = [[0] * n for _ in range(n)] def quadratic_recur(n: int) - int: 平方阶（递归实现） if n = 0: return 0 # 数组 nums 长度为 n, n-1, ..., 2, 1 nums = [0] * n return quadratic_recur(n - 1) 指数阶 $O(2^n)$指数阶常见于二叉树。 def build_tree(n: int) - TreeNode | None: 指数阶（建立满二叉树） if n == 0: return None root = TreeNode(0) root.left = build_tree(n - 1) root.right = build_tree(n - 1) return root 对数阶$O(log n)$对数阶常见于分治算法。例如归并排序（输入长度为n 的数组，每轮递归将数组从中点处划分为两半，形成高度为 $(log n)$的递归树，使用$O(log n)$ 栈帧空间。）、数字转化为字符串。 权衡利弊降低时间复杂度通常需要以提升空间复杂度为代价，反之亦然。我们将牺牲内存空间来提升算法运行速度的思路称为“以空间换时间”，反之，则称为“以时间换空间”。 选择那种思路取决于我们更看中哪个方面。在大多数情况下，时间比空间更宝贵，因此“以空间换时间”通常是更常用的策略。","tags":["DSA"],"categories":["数据结构与算法"]},{"title":"浅析数据结构与算法的核心概念","path":"/2025/09/01/dsa/250831-dsa-basic/","content":"写在前面数据结构与算法常被视为一个整体，但它其实是由「数据结构」和「算法」两部分构成。我们很少见到有人将他们完全分开讲解，正是因为二者相辅相成、密不可分：数据结构是为算法的服务的，而算法总要作用在某种特定的数据结构之上。 此外值得注意的是，算法工程师所从事的“算法”工作，与数据结构与算法中的算法中所有的“算法”是两回事。前者的侧重于在数据建模、特征工程和参数调优，计算机更多是作为实现计算的工具；而后者强调的是一种计算机思维——要求我们能够站在计算机的底层视角，对现实问题进行抽象与简化，并运用合理的数据结构设计出高效的解放方案。 数据结构：数据该怎么高效的存数据结构（data structure）是指数据组织和存储的方式，其核心解决的是数据该怎么存的问题。 常见的数据结构可以分为两大类： 线性结构 数组（Array）、链表（Linked List）、栈（Stack）、队列（Queue） 非线性结构 树（Tree）、堆（Heap）、散列表（hash Table）、图（Graph） 设计一个好的数据结构，我们通常需要考虑以下几点： 如何节省内存？- 尽可能减少内存的占用； 如何操作更快？- 优化增删改查等基本操作的效率； 如何清晰表达？- 通过简洁的数据表示支持算法高效运行。 算法：数据该怎么高效的用算法（algorithm）是指在有限时间内解决特定问题的一组指令或者操作步骤。就像一个靠谱的食谱，只要跟着步骤做，就能在有限时间里做出想要的菜。 一个好的算法，要有这些特质： 明确的问题定义：包含清晰的输入和输出定义； 可行性：能够在有限步骤、时间、和内存空间下完成； 确定性：在相同的输入和运行条件下，输出始终相同； 可重复性：任何人依照步骤执行都应该得到相同输出。 我们在设计算法的时候，一般会先后追求两个层面的目标：解决问题和尽可能高效的解决问题。也就是说，我们的目标是设计出“既快又省”的算法，具体点就是： 找到问题解法：算法需要在规定的输入范围内可靠地求得问题的正确解。 寻找最优解法：同一个问题可能存在多种解法，我们希望找到尽可能高效的算法。 常见的算法思想有不少，比如： 递归（Recursion）- 自己调用自己 分治（Divide and Conquer）- 拆解小问题逐个击破 动态规划（Dynamic Programming）- 记住已经做过的，避免重复劳动 贪心算法（Greedy Algorithm）- 每一步都选当前最好的 回溯（Backtracking）- 试错了就回头 枚举（Enumeration）- 把所有可能都试一遍 复杂度分析：高效判断算法的性价比复杂度分析，它描述了随着输入数据大小的增加，算法执行所需时间和空间的增长趋势。 说白了就是不看实际运行，就能预估算法随着数据量变大，对执行速度和占用内存有多大影响。 最常用的方法就是大O复杂度表示法，靠它，就能在不用真跑测试的情况下，大致判断一个算法的效率。 复杂度分析的常见概念： 最好情况时间复杂度（best case time complexity） 在运气最好的时候执行这段代码 最坏情况时间复杂度（wrost case time complexity） 在运气最背的时候执行这段代码 平均情况时间复杂度（average case time complexity） 大多数日常情况下，执行这段代码 均摊时间复杂度（amortized complexity） 又称：摊还分析、平摊分析 通过摊还分析法得到的时间复杂度 均摊时间复杂度是一种特殊的平均时间复杂度 通常情况下，并不都是需要区分最好、最坏、平均情况时间复杂度的，使用一种复杂度就能满足需求了。 大O复杂度表示法主要看两点： 时间复杂度：运行时间随数据规模增长的趋势 空间复杂度：占用内存随数据规模增长的趋势 它的核心公式是：$T(n)O(f(n))$ , 表示执行时间T(n)和某个函数f(n)成正比。 大O时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随着数据规模增长的变化趋势，故也称为“渐进时间复杂度”。 问题与思考数据结构与算法到底是什么关系？数据结构是算法的基石，算法必须依赖数据结构才能发挥作用。反过来，算法又是数据结构的灵魂，没有算法，数据结构也仅是静态的存储单元。它们共同的目标是解决如何更省、更快地存储和处理数据。 怎么判断一个算法好不好？算法效率的评估通常采用以下两种方法： 事后统计法（又称实际测试法）： 一般采取的做法是把算法（代码）运行一遍，根据统计、监控就能得到算法执行的是时间和占用的内存大小。但这种做法有很大的局限性，主要在于，测试结果非常依赖于测试环境（如：运行机器的处理器性能），测试结果受到数据规模大小的影响很大如：对一组数据进行排序，有序的数据一定比无序数据执行效率高，再比如数据量太小的数据，测试结果也无法真实的反应算法的性能）。 复杂度分析法（又称理论估算法）： 采用大O复杂度表示法。此方法主要从时间复杂度和空间复杂度两个方面对算法进行理论上的估算，该方法不受环境与具体数据干扰，是更为通用和推荐的评估方式。 常见的复杂度类型有哪些？按执行效率从高到低排列常见的有： $$O(1) O(log n) O(n) O(n log n) O(n²) O(2ⁿ) O(n!)$$ 常数阶 对数阶 线性阶 线性对数阶 平方阶 指数阶 阶乘阶 总结一下 所有数据结构，本质上都是数组（顺序存储）和链表（链式存储）的组合和扩展。其核心在于如何实现高效的遍历与访问（即增、删、改、查）。 所有算法思想，本质上都是对穷举的优化。关键在于如何做到无遗漏、无冗余——也就是在尽可能少的步骤中，找到问题的解。","tags":["DSA"],"categories":["数据结构与算法"]},{"title":"重回原点再出发","path":"/2025/08/31/dsa/250831-dsa-intro/","content":"在AI编程助手高速演进时代，我们每天都在坚持技术的飞跃。然而，越是在浪潮之巅，我越发意识到—真正可持续的成长，依然离不开坚实的技术根基。 根基无声，却能支撑我们走向更远的未来…… 因此，我从今天（2025.08.31）起，启动一项为期31天的专项计划：重新系统学习数据结构与算法，并将这个系列命名为《DSA重学日志》。 “DSA”代表Data Structure and Algorithms.这不仅是一次温故知新的回顾，更是一场刻意选择的“返璞归真”—在智能工具日益强大的今天，我选择回到底层，夯实每一块知识基石，重建对数据结构与算法的理解。 在接下来一个月中，我会坚持每日输出一篇学习笔记，内容不局限与概念复述，更会融入代码实践，算法思想拆解和个人总结。我通过这种写作的方式，倒逼自己能够有目标的坚持走完这段里程。","tags":["DSA"],"categories":["数据结构与算法"]},{"title":"Supervisor：三年后的更新，把我三年前的笔记唤醒","path":"/2025/08/30/tech/python/250830-supervisor/","content":"失踪人口回归：Supervisor时隔三年再更新时隔近三年，这款经典的进程管理工具终于迎来了更新。 最近偶然注意到Supervisor项目发布了新版本（4.3.0—2025.08.23)，印象中这个项目已经很久没有动静了。于是特意去翻看了更新日志，才发现距离上个版本发布已经过去了两年零八个月。仔细阅读版本说明后，发现这次更新主要集中在性能优化和稳定性提升，主要包括了： 两项重要Bug修复: CPU 使用率优化：修复了在某些情况下轮询器未注销已关闭的文件描述符，导致过度轮询、CPU 占用异常升高的问题。 重启稳定性提升：修复了在重启期间若有 HTTP 请求可能导致“端口已被占用”错误的问题。 兼容性更新 支持 Python 3.13：修复了仅在 Python 3.13 环境下会失败的单元测试。 清理依赖关系：Python 3.8 及以上版本不再需要 setuptools 作为运行时依赖；Python 3.7 及以下版本仍需手动安装 setuptools。 时间飞逝，Supervisor已有近三年未发布版本了，而我也差不多有三年多没有使用这个组件了。正好借此机会，我将之前在使用过程中记录的笔记重新整理了一遍，分享给大家。 初识Supervisor：你的进程管理Supervisor是用Python开发的一款用于Linux的进程管理工具，采用典型的CS架构，够高效监控和管理进程状态。官方资源：Github仓库、官方文档 Supervisor主要包含两个组件： supervisord：服务端，负责启动、监控和管理子进程。 supervisorctl：客户端，提供命令行界面管理子进程。 主要特点： 进程状态监控和管理 进程崩溃自动重启 统一的进程管理接口 Web管理界面支持 进程组管理功能 注意事项： Supervisor要求要离的程序必须非守护进程方式运行（例如：管理nginx，必须在 nginx 的配置文件里添加一行设置 daemon off 让 nginx 以非 daemon 方式启动） 修改配置文件（配置文件通常位于/etc/supervisord.conf）后需要执行reload才能更改生效。 快速上手：你只需要几分钟安装pip install supervisor # 或 uv add supervisor# 生成配置文件到指定路径（此处路径指定为了/etc/supervisord.conf）echo_supervisord_conf /etc/supervisord.conf 常用命令 supervisord: 服务端命令： # 启动服务supervisord -c /etc/supervisord.conf# 查看是否在运行ps aux | grep supervisord supervisorctl: 客户端命令： supervisorctl status # 查看进程状态supervisorctl start program_name # 启动program_name进程supervisorctl stop program_name # 终止program_name进程supervisorctl restart program_name # 重启program_name进程supervisorctl reread # 更新配置，根据最新的配置启动所有程序supervisorctl update # 更新配置，重启配置有变化的进程 supervisorctl: shell命令： supervisorctl # 进入shell命令行 status # 查看进程状态 start program_name # 启动program_name进程 stop program_name # 终止program_name进程 restart program_name # 重启program_name进程 reread # 更新配置，根据最新的配置启动所有程序 update # 更新配置，重启配置有变化的进程 start fastapi:* # 启动 fastapi组 程序 stop fastapi:* # 停止 fastapi组 程序 基础使用： step1：编辑supervisor主配置文件，在配置文件的末尾加上[include]内容 ...[include]files=/etc/supervisor.d/*.conf #若你本地无/etc/supervisor.d目录，请自建 step2：创建应用配置文件：/etc/supervisor.d/demo.conf 配置文件 [program:自定义的服务名称]command=python3 /xxxx/main.py #服务的启动命令，在这里用的python环境为supervisor安装的环境，若想指定其他python环境则可指定python包的位置如command=xxxx/xxxx/bin/python3 /xxxx/main.py便可directory=/xxxxprocess_name=%(program_name)sautorestart=truestartsecs=3stdout_logfile=/home/task.log #输出日志文件路径stderr_logfile=/home/task.log #报错日志文件路径user=your_username step3:启动 supervisord -c /etc/supervisor.conf 配置详解：磨刀不误砍柴工 打开配置文件： etcsupervisord.conf 基础配置段[unix_http_server]file=/var/run/supervisor.sock ; socket文件路径;chmod=0700 ;socket文件的mode，默认是0700;chown=nobody:nogroup ;socket文件的owner，格式：uid:gid[inet_http_server] ;HTTP服务器，提供web管理界面 port=0.0.0.0:9001 ;Web管理后台运行的IP和端口，如果开放到公网，需要注意安全性username=user ;登录管理后台的用户名password=pwd ;登录管理后台的密码[supervisord]logfile=/tmp/supervisord.log ;日志文件，默认是 $CWD/supervisord.loglogfile_maxbytes=50MB ;日志文件大小，超出会rotate，默认 50MB，如果设成0，表示不限制大小logfile_backups=10 ;日志文件保留备份数量默认10，设为0表示不备份loglevel=info ;日志级别，默认info，其它: debug,warn,tracepidfile=/tmp/supervisord.pid ;pid 文件nodaemon=false ;是否在前台启动，默认是false，即以 daemon 的方式启动minfds=1024 ;可以打开的文件描述符的最小值，默认 1024minprocs=200 ;可以打开的进程数的最小值，默认 200[supervisorctl]serverurl=unix:///tmp/supervisor.sock ;通过UNIX socket连接supervisord，路径与unix_http_server部分的file一致;serverurl=http://127.0.0.1:9001 ; 通过HTTP的方式连接supervisord[include]files=/etc/supervisor.d/*.conf #若你本地无/etc/supervisor.d目录，请自建 程序配置段[program:your_program_name]command=/path/to/your/command ; 必须：要执行的命令process_name=%(program_name)s ; 进程名numprocs=1 ; 进程数量directory=/path/to/run/in ; 执行命令前切换的目录autostart=true ; 是否随supervisor启动autorestart=unexpected ; 退出时是否自动重启startsecs=1 ; 启动后持续运行1秒才认为是成功startretries=3 ; 启动失败时的重试次数user=username ; 运行用户redirect_stderr=true ; 重定向stderr到stdoutstdout_logfile=/path/to/logfile ; stdout日志路径stdout_logfile_maxbytes=1MB ; 日志文件最大大小stdout_logfile_backups=10 ; 日志备份数量environment=KEY=value ; 环境变量 入门到进阶：让效率翻倍Web管理界面启动Web管理界面可以直观地监控和管理进程, 在etcsupervisord.conf中修改[inet_http_server]的参数下列参数，修改后记得重启supervisor进程，在浏览器访问 http://host-ip:9001 [inet_http_server]port=0.0.0.0:9001username=your_usernamepassword=your_password 进程分组管理对于多个相关进程，可以使用分组功能进行统一管理： [group:app_group]programs=app1,app2,app3[program:app1]command=/path/to/app1[program:app2]command=/path/to/app2[program:app3]command=/path/to/app3 分组后可以使用下列命令进行管理整个组： supervisorctl start app_group:supervisorctl stop app_group: 需要注意的是： 当添加了上述配置后，app1、app2和app3 的进程名就会变成 app_group:app1 、 app_group:app2和app_group:app1 ，以后就要用这个名字来管理进程了，而不是之前的 app[x]了。 Supervisor配置开机启动方式一： centos-7 进入 libsystemdsystem 目录，并创建 supervisord.service 文件。 [Unit]Description=supervisordAfter=network.target [Service]Type=forkingExecStart=/usr/bin/supervisord -c /etc/supervisor/supervisord.confExecStop=/usr/bin/supervisorctl $OPTIONS shutdownExecReload=/usr/bin/supervisorctl $OPTIONS reloadKillMode=processRestart=on-failureRestartSec=42s [Install]WantedBy=multi-user.target 加入开机启动 systemctl enable supervisordsystemctl start supervisord 方式二 Linux环境中：以systemd的方式管理 编写启动脚本：vim etcrc.dinit.dsupervisord #!/bin/sh## /etc/rc.d/init.d/supervisord## Supervisor is a client/server system that# allows its users to monitor and control a# number of processes on UNIX-like operating# systems.## chkconfig: - 64 36# description: Supervisor Server# processname: supervisord# Source init functions. /etc/rc.d/init.d/functionsprog=supervisordprefix=/usrexec_prefix=$prefixprog_bin=$exec_prefix/bin/supervisordPIDFILE=/var/run/$prog.pidstart()echo -n $Starting $prog: daemon $prog_bin --pidfile $PIDFILE -c /etc/supervisord.conf[ -f $PIDFILE ] success $$prog startup || failure $$prog startupechostop()echo -n $Shutting down $prog: [ -f $PIDFILE ] killproc $prog || success $$prog shutdownechocase $1 instart)start;;stop)stop;;status)status $prog;;restart)stopstart;;*)echo Usage: $0 start|stop|restart|status;;esac 设置开机启动及systemd方式启动。 sudo chmod +x /etc/rc.d/init.d/supervisordsudo chkconfig --add supervisordsudo chkconfig supervisord onsudo service supervisord start 实践出真知Django应用配置#项目名[program:django]#脚本目录directory=/root/social_engineering/#脚本执行命令command=/root/anaconda3/bin/python manage.py runserver 0.0.0.0:8000 # 或用gunicorn等服务启动#supervisor启动的时候是否随着同时启动，默认Trueautostart=true#当程序exit的时候，这个program不会自动重启,默认unexpected，设置子进程挂掉后自动重启的情况，有三个选项，false,unexpected和true。如果为false的时候，无论什么情况下，都不会被重新启动，如果为unexpected，只有当进程的退出码不在下面的exitcodes里面定义的autorestart=true#这个选项是子进程启动多少秒之后，此时状态如果是running，则我们认为启动成功了。默认值为1startsecs=1#脚本运行的用户身份 #日志输出 stderr_logfile=/tmp/django_stderr.logstdout_logfile=/tmp/django_stdout.log#把stderr重定向到stdout，默认 falseredirect_stderr = true#stdout日志文件大小，默认 50MBstdout_logfile_maxbytes = 20#stdout日志文件备份数stdout_logfile_backups = 20; 可以通过 environment 来添加需要的环境变量，一种常见的用法是修改 PYTHONPATH; environment=PYTHONPATH=$PYTHONPATH:/path/to/somewhere Celery应用配置#项目名[program:celery]#脚本目录directory=/root/social_engineering/#脚本执行命令command=/root/anaconda3/bin/celery -A tasks worker --loglevel=info#supervisor启动的时候是否随着同时启动，默认Trueautostart=true#程序崩溃时自动重启autorestart=true#这个选项是子进程启动多少秒之后，此时状态如果是running，则我们认为启动成功了。默认值为1startsecs=1#脚本运行的用户身份 #日志输出 stderr_logfile=/tmp/celery_stderr.logstdout_logfile=/tmp/celery_stdout.log#把stderr重定向到stdout，默认 falseredirect_stderr = true#stdout日志文件大小，默认 50MBstdout_logfile_maxbytes = 20#stdout日志文件备份数stdout_logfile_backups = 20 Redis服务配置[program:zedis]command=/opt/zedis/redis-server /opt/zedis/redis.confuser=redisuserautostart=true ;启动supervisord的时候会将该配置项设置为true的所有进程自动启动stopasgroup=true ;使用supervisorctl停止zedis时，子进程也会一起停止killasgroup=true ;向进程组发送kill信号，包括子进程startsecs=10 ;进程从STARING状态转换到RUNNING状态所需要保持运行10s时间startretries=3 ;启动失败自动重试次数，默认是3autorestart=true ;进程停止后自动启动stdout_logfile=/var/log/zedis-outstdout_logfile_maxbytes=1MBstdout_logfile_backups=10stderr_logfile=/var/log/zedis-errstderr_logfile_maxbytes=1MBstderr_logfile_backups=10 Ø 因为无法监视后台进程，需要把Zedis前台运行（daemonize no），否则会识别不到Zedis启动，重试启动3次,日志提示6379已占用。 Ø 因为redis.conf指定了logfile，stdout_logfile并无内容输出；如果不指定redis.conf只是启动redis-server，日志会输出到stdout_logfile。 组应用配置 新建文件，fastapi_app.conf [group:fastapi]programs=fastapi-app[program:fastapi-1]command=/home/python/scripts/fastapi1_app.shdirectory=/home/python/fastapi-1-backenduser=pythonautorestart=trueredirect_stderr=falseloglevel=infostopsignal=KILLstopasgroup=truekillasgroup=true[program:fastapi-2]command=/home/python/scripts/fastapi-2.shdirectory=/home/python/fastapi-2-backenduser=pythonautorestart=trueredirect_stderr=falseloglevel=infostopsignal=KILLstopasgroup=truekillasgroup=true 避坑指南问题1：sock文件不存在问题描述：unix:///tmp/supervisor.sock no such file **根源分析：**在supervisor默认配置中，其启动的sock等都会放到tmp目录，而tmp目录会自动清理导致无法使用supervisorctl 解决方案： # 修改主配置文件中的tmp路径sed -i s|/tmp/supervisor|/var/run/supervisor|g /etc/supervisord.conf# 创建所需目录并设置权限mkdir -p /var/run/supervisorchmod 755 /var/run/supervisor# 重启supervisorsupervisorctl reload 问题2：进程不断重启问题描述：进程启动后Supervisor不断尝试重启 解决方案： 确保管理的进程以前台方式运行 检查进程启动命令是否正确 查看日志文件定位具体问题 问题3：启动了多个supervisord服务，导致无法正常关闭服务问题描述：在运行supervisord -c /etc/supervisord.conf之前，直接运行过supervisord -c etcsupervisord.dxx.conf导致有些进程被多个superviord管理，无法正常关闭进程 解决方案： 使用ps -fe | grep supervisord查看所有启动过的supervisord服务，kill相关的进程。 总结一下Supervisor是一款强大且灵活的进程管理工具，合理配置显著提升服务的稳定性和可维护性。本文从安装配置、基础使用到进阶使用全面介绍了Supervisor的使用方法， 希望能帮助你在实际工作中更好地运用这一工具。 最佳实践建议： 为每个服务创建独立的配置文件 合理配置日志轮转，避免磁盘空间不足 使用Web界面进行日常监控和管理 设置进程分组，简化管理操作 配置开机启动，确保服务可靠性","tags":["进程管理"],"categories":["Python"]},{"title":"我用过的Python工具链：从virtualenv到uv的演进之路","path":"/2025/08/28/tech/python/250827-py-env-package/","content":"高效开发，从选择合适的工具开始 在Python开发的生涯中，我相信很多人也许都曾经历过这样的困境：昨天还能正常运行的项目，今天突然报错；在不同项目间切换时，依赖冲突让人头痛不已；团队协作时，环境配置差异导致各种问题……，这其实就是典型的“依赖地狱”。 好在Python社区提供了丰富的工具帮我们来解决这些问题，近年来，Python开源社区涌现了大量优秀的版本管理和包管理工具，可谓是“长江后浪推前浪”，形成了老中青三代并存的态势，从老牌的virtualenv、conda，到现代的poetry、pdm，再到新兴的hatch、uv，每个工具都有其独特的哲学和适用场景。 下面结合我个人的实践经历，带你深入探索Python开发工具生态，希望能够帮助你在日常开发中做出更明智的选择。 工具生态总览 🔧可以将Python工具分为三个时代： 经典时代：virtualenv + pip + requirements.txt、venv、conda 过渡时代：pythonz、pyenv、pipenv、Rye 现代时代：poetry、uv、pdm、hatch 每个工具都在尝试解决相同的问题：如何高效的管理Python版本、隔离环境、管理依赖以及保证可重现性。 新生代明星：uv 🚀uv是一个用Rust开发的极速Python包和项目管理器，主打高性能和一体化设计（既能管理Python的多版本，又能高效的管理依赖包）。它旨在替代pip, pip-tools, pipx, poetry, pyenv, twine和 virtualenv等多种工具，目前Github Star数已超过66k。详细的介绍与使用：官方文档， Github地址； 其主要的特点： 极致的性能：比pip快8-10倍（无缓存）至80-115倍（有缓存）； 一体化设计：替代多个包管理工具(pip, pip-tools, pipx, poetry, pyenv, twine, virtualenv等)； 现代标准：支持最新的PEP标准； 开发者友好： 智能缓存、跨平台（支持macOS, Linux, and Windows） 随时随地的安装需要的Python解释器版本（uv python install） 自动化的虚拟环境管理，无需手动激活环境； 支持扩展工具管理（uv tool install、uv tool run） 支持基于PEP723标准的单文件脚本执行。 科学计算首选：condaconda是一款功能强大的跨平台包和环境管理器，通过创建不同Python版本的虚拟环境进行多版本管理。分析了完整版（Anaconda）和精简版（Miniconda）。详细的介绍与使用：官方文档， Github地址 其主要的特点： Anaconda（大而全）：除了安装 Conda 内核，还安装了完整的 Anaconda 的发行包，即内置集成了很多工具包，包括python的多个版本，常用机器学习等科学工具包；官方下载，清华源下载； Miniconda（小而美）：只安装了 Conda 内核和一些必要的工具包；官方下载、清华源下载； 二进制兼容：提供丰富的科学计算、数据分析的预编译包； 跨语言支持：能够管理C、C++、R等其他语言的包； 生态丰富：特别是数据科学和机器学习领域； 环境隔离：强大的环境管理功能； 其适用的场景： 数据科学和机器学习项目； 需要复杂的二进制依赖的项目； 跨语言场景、集成开发场景。 国产之光：PDMPDM是一个现代Python包和依赖项管理器，支持最新的PEP标准，他不仅是一个包管理器，还能通过灵活的插件系统提升开发工作流程。详细介绍与使用：官方文档、Github地址。 其主要的特点： 革命性支持PEP标准-PEP582（本地包目录（__pypackages__）替代虚拟环境） 支持PEP标准-PEP517（build backend） 支持PEP标准-PEP621（project metadata）; 采用去中心化安装缓存（类似pnpm)，节省磁盘空间； 灵活而强大的插件系统、多功能用户脚本管理； 便捷的Python解释器版本的安装方式。 其适用的场景： 开源库和框架开发； 微服务于云原生应用开发； 开发工具链和CLI应用开发； 项目管理专家：PoetryPoetry是一个跨平台开源依赖管理和打包工具，需要Python3.8+。它简化了Python的依赖管理以及包发布流程。目前Github star 数接近34k ,被许多知名项目采用。 其主要的特点： 全生命周期管理：从依赖管理到打包发布的一体化解决方案，简化Pypi发布流程，且支持私有仓库； 统一的配置管理：取代了 setup.py、requirements.txt、setup.cfg等，仅通过pyproject.toml文件就能轻松机进行包的版本版本和迁移工作，且支持PEP621标准； 分组依赖管理：可以对包的安装场景进行分类管理如开发（静态检查相关包）、测试（pytest）、生产环境包的分组； 强大的依赖解析：使用先进的依赖解析算法，能够有效的处理复杂的依赖关系； 环境隔离于管理：自动创建和管理虚拟环境，支持特定项目的环境配置； 在配置文件中支持自定义Scripts,如：poetry run python xxx; 其适用的场景： Web应用开发（Django、FastAPI等项目） 库和框架开发（需要严格版本管理的开源项目） 企业级应用（需要稳定的依赖管理和发布流程的项目） 现代化的管理工具：HatchHatch 是一个现代化的Python项目管理和打包工具，专注于提供一致且可重复的开发体验。可以管理环境（它允许每个项目有多个环境，但不允许把它们放在项目目录中），可以管理包（但不支持 lockfile）。Hatch 也可以用来打包一个项目（用符合 PEP 621 标准的 pyproject.toml 文件）并上传到 PyPI。 其主要的特点： 统一的项目管理：通过pyproject.toml统一配置 多环境管理：支持为每个项目创建多个独立环境 强大的模板系统：快速生成项目结构和配置文件 完整的打包支持：从开发到发布的完整工作流 插件架构：可通过插件扩展功能 其他工具 pyenv pyenv 可让您轻松地在多个 Python 版本之间切换。它简单、不引人注目，并且遵循 UNIX 单一用途工具只做好一件事的传统。该项目是从rbenv和ruby-build分叉出来的，并针对 Python 进行了修改。详细安装与使用： Github地址 pythonz 支持 CPython、Stackless、PyPy 和 Jython 的 Python 安装管理器。详细介绍与使用：官方文档、Github地址 Pipenv 是一个跨平台开源的Python虚拟环境管理工具，依赖于原生Python安装（支持Python3.7+），其主要特点是使用pipfile和pipfile.lock文件来维护环境中的包版本；以及支持pipenv CLI操作 venv（Python内置虚拟环境） Python原生安装包中内置的虚拟环境模块，每个虚拟环境拥有自己独立的site目录。 Virtualenv（经典虚拟环境工具） 依赖原生Python，创建相同Python版本的不同虚拟环境，其场景主要应用于同一个Python版本的不同项目中，做到依赖包的隔离，避免依赖冲突。详细使用请阅读官方文档。 另外，该工具还有个脚本工具叫virtualenvwrapper，可以方便开发者方便管理虚拟环境。在个人工作早期，基本是使用virtualenv+virtualenvwrapper工具来进行相关开发工作。 Rye（已被uv替代） Rye是一款全面的Python项目和包管理解决方案，提供统一的安装体验，无缝安装和管理Python组件，基于pyproject.toml项目、依赖项以及虚拟环境。 目前官方声称已停止维护，并且建议使用uv,同时也提供了uv迁移指南. 总结与建议 工具 适用场景 特点 uv 高性能、一体化开发 极速、现代、All-in-One conda 科学计算、数据分析 跨语言、预编译包丰富 PDM 现代 PEP 标准项目 插件化、去中心化缓存 Poetry 依赖管理 + 打包发布 统一配置、分组管理 pyenv 纯版本管理 轻量、专注 venv 轻量虚拟环境（Python 3.3+） 内置、无需安装 Pipenv 传统虚拟环境 + 依赖管理 兼容旧项目 如果你追求性能和现代化体验，uv 是不二之选； 如果你是数据科学家或机器学习工程师，conda 仍是首选； 如果你希望遵循最新 PEP 标准且喜欢插件生态，可以尝试 PDM； 如果你只需要简单的版本切换，pyenv 足够好用； 如果你需要依赖管理 + 打包一体化，Poetry 成熟且强大。 工具没有绝对的好坏，关键在于是否适合你的项目和工作流。希望本文能帮助你在众多工具中找到最适合的那一个。","tags":["依赖包管理"],"categories":["Python"]},{"title":"从计数到分代一步步窥探Python的垃圾回收机制","path":"/2025/08/26/tech/python/250826-pygc/","content":"在编程世界中，高效的内存管理是保证程序稳定运行的关键之一。然而Python作为一门高级编程语言，其自动垃圾回收（Garbage Collection, GC）常常被Pythoner们当做理所当然的黑盒，可一旦线上出现内存飙升，GC就成了最熟悉的陌生人。今天就来拆开这个黑盒，一窥究竟，看看Python是如何优化的帮我们进行垃圾回收的。 Python的垃圾回收主要依赖于引用计数，并在此基础上引入标记-清除和分代回收两种机制，以解决循环引用和提升回收效率的问题。 引用计数：最朴素的记账法每个Python对象中都包含一个名为PyObject的基础结构体，其中，ob_refcnt字段即为该对象的引用计数，其基本规则如下： 当对象被新的引用指向时 - ob_refcnt++； 当某个引用被删除或离开作用域时，del - ob_refcnt–； ob_refcnt 0 - 一旦引用计数归零，对象会被立即回收。 这样做的可以让实现变得简单，逻辑很直观，于此同时，实时性很强，内存可立即被释放；当然，这样做也会有一些局限性，比如维护计数需要额外开销、无法处理循环引用（即：两个或多个对象相互引用，却无外部引用指向它们 ）的情况等问题。 import sysa = [1, 2, 3] # 引用计数为 1b = a # 引用计数增加为 2print(sys.getrefcount(a)) # 输出 3（getrefcount本身也会增加一个临时引用）del b # 引用计数减为 1# 函数结束时，a 离开作用域，引用计数归零，对象被回收 标记-清除：解决循环引用的利器针对引用计数无法处理的循环引用问题，Python引入了标记-清除（Mark Sweep）机制。其基本思路是这样的： 先按需分配内存，直到空闲内存不足时触发垃圾回收； 从根对象（如寄存器和程序栈中的引用）出发，遍历所有可达对象，并对其进行标记； 遍历结束后，清除所有未被标记的对象，释放其占用的内存空间。 通过这样的处理方式，即使对象之间存在循环引用，只要它们已经不再被实际使用，就会被正确回收。 import gcimport psutilimport osdef rss_mb(): 获取当前进程占用的物理内存 (MB) return psutil.Process(os.getpid()).memory_info().rss // 1024 // 1024class Node: 一个会互相引用的小节点 def __init__(self, value): self.value = value self.partner = None def __repr__(self): return fNode(self.value)# -------------------------------------------------# 1) 制造 100 万个循环引用print(Step 1: 制造循环引用)print(RSS before:, rss_mb(), MB)nodes = []for i in range(1_000_000): a, b = Node(i), Node(-i) a.partner = b b.partner = a nodes.extend([a, b])print(RSS after create:, rss_mb(), MB)# 2) 删除外部引用，观察内存del nodesprint(RSS after del nodes:, rss_mb(), MB) # 大概率不会降多少# 3) 手动触发一次标记-清除gc.collect()print(RSS after gc.collect():, rss_mb(), MB) 分代回收： 以空间换时间的优化策略为了进一步提升回收的效率，Python使用了分代回收的策略。该策略将内存中的对象按其存活时间划分为不同的集合（称为“代”，Generational GC），Python默认定义了三代： 第0代：新创建的对象； 第1代：经历过一次垃圾回收后仍然存活的对象； 第2代：存活时间更长的对象。 回收频率随代号的增加而降低，即代数越高，GC越“懒得管”。比如说： 新分配的对象会被放在第0代（GC的常客，回收最勤快）； 如果它们经历过一次GC后仍然存活，则被移至第1代（偶尔光顾）； 若再经历一次GC仍未被回收，则进入第二代（几乎佛系，除非内存告急）。 这样做的好处是：大部分新对象往往很快就不再使用，而对“年轻代”进行频繁回收，可以在更短时间内清理掉大部分垃圾，而检查老一代对象的频率较低，从而减少了整体的开销。通俗的说，这种“年纪越大越免检”的策略，让GC把80%的精力花在最容易产生垃圾的第0代，整体吞吐量大幅提升。 import gc# 查看分代回收的阈值print(gc.get_threshold()) # 输出 (700, 10, 10)# 手动执行垃圾回收gc.collect() # 回收所有代gc.collect(0) # 只回收第0代gc.collect(1) # 回收第0代和第1代 总结：Python的垃圾回收机制是一个结合了引用计数、标记-清除和分代回收的复合系统。它既保证了内存回收的实时性，也有效解决了循环引用问题，并通过分代策略实现了高性能的自动内存管理。 了解这一机制后，我们在后续的编码中可以考虑： 尽量使用局部变量，缩短对象生命周期 手动断开长引用链：a.b None 调优GC：get_threshold(), 或在低延迟场景下，gc.disable()后手动触发。 另外，在日常开发中如果遇到了OOM相关问题，可以尝试分析的思路有： 实时监控内存使用 psutil工具 定位内存增长点 先确定内存中一直保持增长的是什么类型的对象，看看内存分配最频繁的地方是哪里； 分析大对象 将重点怀疑的对象通过工具输出引用关系（如：objgraph）,从而进一步分析出内存的问题源自哪里。 有了上述的思路后，可以尝试借助下列工具进行更进一步的问题分析和定位： 下列工具的具体使用方法，可点击链接前往官方文档进行学习使用。 官方工具：tracemalloc——跟踪内存分配 第三方库：objgraph——对象引用关系可视化 第三方库：pympler——详细对象大小分析 第三方库：memory_profiler——逐行内存分析","tags":["gc"],"categories":["Python"]},{"title":"异步消息处理的利器之FastStream","path":"/2025/08/24/tech/python/250824-pylib-faststream/","content":"为什么有FastStream在处理消息队列（Kafka、RabbitMQ、Redis）时，你是否厌倦了重复编写底层连接、订阅、序列化和消息异常处理的代码？比如说之前可能用过的kafka-python、pika、redis-py之类的库，虽然为业务的开发免去了重复造轮子，也具备了一定的灵活性，但我还是觉得缺乏现代开发所追求的开发效率和优雅性。恰巧最近在Github中无意看到了这个与消息处理相关的库-FastStream，在阅读完README的内容后，一时兴趣被调动了，于是花了几个小时深入文档并上手体验感受了一下，下面就来详细介绍一下。 FastStream是什么？FastStream是一个用于构建与消息代理（如 Apache Kafka、RabbitMQ、NATS 和 Redis）交互的服务框架，让构建事件驱动的微服务变得轻而易举。它提供了统一的API，让开发者可以在不重写应用程序逻辑的情况切换消息组件，实现了开发者只需聚焦于业务逻辑的开发。它的设计哲学是：通过简单的装饰器，将函数变成强大的消息处理器。 其主要特点有： 统一直观的API: 一次编写，随处运行，使用装饰器@broker.subscriber 和@broker.publisher 声明消息处理器。 自动文档：自动生成AsyncAPI文档，使团队之间的集成无缝衔接。 类型安全与自动解析：依托Pydantic，自动验证、序列化和解析消息体。 强大的异步支持：原生支持async/await ,轻松处理高并发IO场景。 开箱即用：直观的装饰器、内置依赖注入、测试工具和CLI工具等，使开发变得简单且快速。 核心特性多消息代理支持 代理 说明 安装 Kafka 高吞吐量事件流 pip install faststream[kafka] RabbitMQ 可靠消息队列 pip install faststream[rabbit] NATS 轻量级消息传递 pip install faststream[nats] Redis 简单发布订阅 pip install faststream[redis] # 只需更改导入和代理初始化from faststream.kafka import KafkaBroker# from faststream.rabbit import RabbitBroker# from faststream.nats import NatsBroker# from faststream.redis import RedisBroker broker = KafkaBroker(localhost:9092)# broker = RabbitBroker(amqp://guest:guest@localhost:5672/)# broker = NatsBroker(nats://localhost:4222/)# broker = RedisBroker(redis://localhost:6379/) 强大的装饰器 @broker.subscriber(): 标记函数从队列主题消费消息 @broker.publisher(): 标记函数以发布消息（通常与订阅者一起使用） 上述两个装饰器将普通的Python函数转换为消息处理器，自动处理消息的序列化、反序列化和路由。 @broker.subscriber(input-queue)@broker.publisher(output-queue)async def process_message(data: dict) - dict: # 您的业务逻辑在这里 return processed: True, data: data Pydantic集成了解FastAPI项目的人一定都知道Pydantic库，FastStream利用Pydantic进行消息验证和序列化。由于Pydantic是个独立且很热门的库，因此在此就不展开对Pydantic的使用，如需详细了解，请：点击前往。 内置测试测试分布式系统可能具有一定的挑战性，但使用FastStream通过内置测试工具使其变得简单。内置TestBroker类：将消息处理器重定向到内存处理，让你无需运行实际代理即可进行测试逻辑。 import pytestfrom faststream.rabbit import TestRabbitBroker @pytest.mark.asyncioasync def test_my_handler(): async with TestRabbitBroker(broker) as test_broker: await test_broker.publish(test: data, input-queue) # 您的断言在这里 依赖注入（DI）FastStream包含了一个强大的依赖注入系统，用法和FastAPI中的Depends类似。 可以轻松的管理数据库连接、配置等资源，使得代码更易测试和维护。 from faststream import Dependsdef get_db_connection(): # 模拟获取数据库连接 return DatabaseConnection@broker.subscriber(order_topic)async def process_order(event: dict, db: str = Depends(get_db_connection)): print(f使用 db 处理订单: event) 代码实践10行代码实现一个Consumer# 安装: pip install faststream-kafkafrom faststream import FastStreamfrom faststream.kafka import KafkaBrokerfrom pydantic import BaseModel# 定义数据模型class UserCreated(BaseModel): user_id: int email: str# 创建应用与Brokerbroker = KafkaBroker(localhost:9092)app = FastStream(broker)# 核心：用装饰器定义处理器@broker.subscriber(user_created_topic)async def handle_user_created_event(event: UserCreated): # 类型自动推断！ print(f收到新用户注册事件！ID: event.user_id, 邮箱: event.email) # 此处可写入你的业务逻辑，如发邮件、写数据库等 return status: successif __name__ == __main__: app.run() 实现用户注册与发送邮件需求：构建一个用户注册服务，处理新用户注册并发送欢迎电子邮件。 from faststream import FastStream, Loggerfrom faststream.rabbit import RabbitBrokerfrom pydantic import BaseModel, Field, EmailStr # 设置代理和应用程序broker = RabbitBroker(amqp://guest:guest@localhost:5672/)app = FastStream(broker) # 定义我们的消息结构class UserRegistration(BaseModel): username: str = Field(..., min_length=3) email: EmailStr age: int = Field(..., ge=18) # 处理新用户注册@broker.subscriber(user.registrations)@broker.publisher(user.welcome_emails)async def handle_registration( user: UserRegistration, logger: Logger) - dict: logger.info(fNew user registered: user.username) # 业务逻辑在这里 welcome_message = fWelcome user.username! Thanks for joining us. return email: user.email, message: welcome_message # 发送欢迎电子邮件@broker.subscriber(user.welcome_emails)async def send_welcome_email(email_data: dict, logger: Logger): logger.info(fSending welcome email to email_data[email]) # 电子邮件发送逻辑将在这里 总结FastStream非常适合应用于：事件驱动架构（EDA）中的微服务、实时数据ETL管道、需要处理Kafka、RabbitMQ消息等场景中，它以简洁的语法（装饰器），严谨的类型系统（Pydantic)，自动化的工具链以及广泛的生态融合（FastAPI），为Python在流式处理、异步消息处理等场景中注入了新活力。","tags":["python库","消息处理"],"categories":["Python"]},{"title":"软件工程中的13条“潜规则”定律","path":"/2025/05/12/thinking/02.software-lows/","content":"一、帕金森定律(Parkinson’s law)定律：工作会不断扩展，填满所有可用的时间（任务总能拖到最后期限前完成？） 如果你给一个任务设定了1周的期限，它很可能就会花掉1周；如果设定了2周，它就可能花掉2周。这常常被用来解释为什么设定“伪造”的（有时甚至不合理的）截止日期似乎能提高效率—它迫使人们在有限的时间内集中精力。但这一定律也容易被滥用，导致不切实际的期望和压力。所以合理的设定Deadlines是必要的，但要警惕其副作用，并结合对工作量的实际评估。它提醒我们时间管理的重要性，以及在没有明确时间约束时，任务可能无限膨胀的风险。 二、霍夫施塔特定律(Hofstadter’s law)定律：事情总是比你预期的要花费更长的时间，即使你已经考虑了霍夫施塔特定律（估时永远不准？） 这是对软件项目估时最困难最精准的自嘲。几乎所有的软件项目都会延期，即使你已经预留了缓冲时间。这一定律完美地平衡了帕金森定律：如果你因为帕金森定律而设置过短的Deadline，结果很可能是团队Burnout 或者和项目持续延期。软件工时评估及其困难，充满了不确定性。简单的缓冲时间往往不够。有效的项目管理需要在时间、资源和可协商的范围之间找到平衡，并依赖持续沟通和实践经验。 三、布鲁克斯定律(Brooks’ law)定律：向一个已延期的软件项目中增加人力，将使其更加延迟（人月神话？） 这就是著名的“9个孕妇不能在一个月内生出一个婴儿”的道理。当项目延期，高层管理者常常会说：“这个项目很紧急，你可以充其他团队调配任何你需要的人！”但项目经理的内心可能是：“请别再打扰我们，让我们专心工作就好”。增加新人手需要时间成本：新人需要学习项目背景、熟悉代码库、建立沟通渠道。这些都会消耗现有团队成员的时间和精力，增加沟通开销，短期内甚至可能降低整体生产力。在项目后期，尤其是面临延期时，要极其谨慎地考虑增加人手。更好的策略可能是缩减范围，优化流程或给于现在团队更多不受干扰的时间。 四、康威定律(Conway’s law)定律：组织输出的设计是这些组织沟通结构的副本。（你的架构是不是反映了你的团队结构？） 简单的说，你的系统架构往往是你团队组织结构的镜像。如果你的公司有独立的“前端团队”和“后端团队”，他们之间的沟通壁垒和协作模式，会直接反映在前后端接口的设计、数据格式的匹配度以及可能出下你的额外“胶水代码”上。这一定律告诉我们组织结构对技术决策的深远影响。反过来，我们可可以利用逆康威定律(Inverse Conway Maneuver)：为了达成期望的系统架构，主动调整团队的组织结构和沟通方式。例如，想要微服务化？那就组建更小、更自治、拥有端到端职责的团队。 五、坎宁安定律(Cunningham’s law)定律：在互联网上获得正确答案的最佳方式不是提问，而是发布一个错误的答案。（想得到反馈？先大胆“错”一个？） 这条定律巧妙地利用了人性—人们往往更乐于纠正错误，而不是回答问题。在工作中遇到阻碍时，可以巧妙运用这一定律。例如：与其提交一个请求单等待DevOps团队处理，不如自己尝试写一个（哪怕不完美的）解决方案，提交一个Pull Request.即使写的不对，通常也能更快地获得相关人员的注意和具体的修改建议。同时也促进了知识的传递和流程的改进。主动迈出第一步，哪怕是“错误”的一步，也比原地等待更有效。 六、斯特金定律(Sturgeon’s law)定律：任何事情（特别是人类创造出来的）90%都是垃圾。（你做的功能多少是真正有价值的？） 这条定律是对现实的残酷揭示，有点像加强版的“二八定律”。无论是代码、想法、功能特性，大部分都可能是平庸甚至无用的。你发布的大部分功能可能对用户价值寥寥，只有那一小部分核心功能支撑着你的产品。这要求我们具有批判性思维，用于质疑。作为开发者，不能仅仅被动接受产品经理给出的需求列表，而要思考功能的真正价值，避免将精力浪费在那“90%的垃圾”上。这也解释了为什么“10倍工程师”并非写10倍代码的人，而是能创造10倍价值的人——他们更懂得识别和聚焦那些重要的10%。 七、扎文斯基定律(Zawinski’s law)定律：每一个程序都试图扩展直到它能阅读邮件为止。那些不能如此扩展的程序会被可以如此扩展的程序替代掉。（警惕功能蔓延！） 这条定律形象的描述了“功能蔓延”（feature creep)的现象。程序（或产品）总有一种内在的趋势去添加越来越多的功能，最终变得臃肿不堪。尤其在AI时代，给任何应用加上一个聊天机器人似乎都轻而易举。我们要警惕无休止的功能添加。保持产品的核心价值和简洁性至关重要。过多的功能不仅会增加复杂度和维护成本。还可能让用户（尤其是新用户）感到困惑，找不到真正需要的功能。需要有意识地做减法，抵制“什么都想要”的诱惑。 八、海勒姆定律(Hyrum’s law The Law of Implicit Interfaces)定律：当你有足够多的API用户时，你在合同（文档）中承诺什么都无关紧要：你系统中所有可观察的行为都会被某些人所依赖。（接口行为不能轻易改动！） 这条定律对API的设计和维护者至关重要，它揭示了接口（API）维护的残酷现实。即使某个行为没有写在你的官方文档里，只要它是可以观察到的（比如某个特定的错误返回格式、某个未公开的内部端点、某个副作用），一旦有足够多的用户，就一定会有人依赖上这个行为。因此，API的设计和变更需要极其谨慎。任何微小的改动，即使是修复Bug或改动未承诺的行为，都有可能破坏依赖者的系统。这也解释了为什么移除那些依据斯特金定律属于“垃圾”的特性如此困难——总有用户在依赖它们。进行接口设计时，要尽可能减少可观察的副作用，明确接口契约，并为变更做好版本管理和兼容性策略。 九、普莱斯定律(Price’s law Price’s Square Root Law)定律：在一个组织中，一半的工作是由占总人数平方根的人完成的。（团队里核心贡献者？） 这一定律量化了贡献度的不平均分布。例如：在一个10人的团队里，大约3个人（√10 ≈3.16）完成了50%的工作；在一个100人的公司里，大约10个人（√100 10）的产出相当于剩余90人的总和。这也可以在一定程度上解释了为什么Twitter在大规模裁员后没有立即崩溃。团队规模的扩大并不会带来线性的产出增长。如果你想让产出翻倍，可能需要4倍的人员规模。这警示管理者在扩张团队时需要关注人效和组织结构，识别并赋能那些核心贡献者。 十、瑞格曼效应(The Ringelmann effect)定律：当团队的规模增加时，个体成员的生产力趋于下降（人多不一定力量大？） 这个效应早在1913年就被发现（通过拔河实验）。团队越大，个体平均贡献的力量越小。原因主要有两个：一是动机丧失（即：“社会惰性化”，觉得自己的贡献不重要或者难以衡量）；二是协调成本增加（沟通、同步、冲突解决等开销变大）。这也是对布鲁克斯定律和普莱斯定律的有力补充。保持小而精干的团队往往效率更高，尤其是在需要高度协作和创新的领域。明确的职责划分，有效的沟通机制和对个体贡献者的认可，有助于缓解瑞格曼效应。 十一、古德哈特定律(Goodhart’s law)定律：当一个度量本身成为目标时，它就不再是一个好的度量。（警惕KPI陷阱！） 这是关于KPI和度量最著名的警告。一旦某个指标（如代码行数、PR数量、Bug数量、用户增长数、客户满意度）被设定为考核目标，人们就会想方设法“优化”这个指标本身，而不是优化它所代表的真实价值，最终导致该指标失去意义。例如，为了提高代码行数二写冗余代码，为了快速关闭工单而不是定位根因并从根因上解决问题。对任何单一的量化指标都要保持警惕。度量是必要的，但不能迷信指标。需要结合多个指标、定性分析以及对最终业务价值的判断，来全面评估绩效和进展。 十二、吉尔布定律(Gilb’s law)定律：任何你需要量化的东西，都可用用某种方式来衡量，这种衡量方式优于完全不衡量。（与上一条辩证看，还是要量化!） 这一定律是古德哈特定律的必要平衡。它告诉我们，尽管度量可能不完美、可能被“攻击”，但完全放弃量化是不可取的。“没有度量、就没有改进”。找到一个（哪怕是粗糙的）量化方法，总比凭感觉行事要好。因此，不要害怕古德哈特定律而彻底放弃量化。关键在于选择合适的度量维度（比如DORA指标、开发者体验DevEx等），持续迭代和优化度量方法，并结合业务背景进行解读。 十三、墨菲定律(Murphy’s law)定律：任何可能会出错的事情，最终都会出错。（Anything that can go wrong will go wrong.） 这条定律大家再熟悉不过了。它提醒我们，那些看起来概率极小、懒得处理的边缘情况、那个被你忽略的潜在Bug.那一次“应该没问题”的侥幸操作，往往会在关键的时候给你带来麻烦。在软件工程中，要有敬畏之心。进行充分的测试（尤其是边缘情况的测试）建立健壮的错误处理和容错机制、实施灰度发布和监控告警。都是应对墨菲定律的必要手段。不要低估任何可能出错的环节。 定律是启发，而非束缚，这13条定律，更像是前辈们用经验和教训为我们绘制的“认知地图”。它们并非严格的科学定理，但在理解软件开发这个复杂系统时，能为我们提供宝贵的视角和警示。将这些定律记在心中，不是为了给自己设定或找借口，而是为了让我们在日常的编码、设计、沟通和决策中，多一份清醒，多一份审慎，少踩一些坑，从而更从容地驾驭软件工程这门充满挑战与乐趣的艺术。","categories":["时笺"]},{"title":"常用的一些Docker使用技巧","path":"/2025/04/28/tech/docker/02-docker-usage-skills/","content":"一、Docker镜像直接推送到远程服务器unregistry 是一个轻量级容器镜像注册表，可直接从Docker守护进程的存储中存储或提取镜像。 支持LinuxMacOS使用，目前在Windows中使用需要借助wsl2 # 核心命令 docker pussh myapp:latest user@server docker pussh myapp:latest user@server:2222 docker pussh myapp:latest user@server.example.com 二、一键load文件夹里的所有镜像 方式一 ls -F /data/package_3th/*.tar | awk cmd=docker load -i $0;print(cmd);system(cmd) # /data/package_3th 为镜像的文件夹 方式二： import osimport sysif __name__ == __main__: workdir = /data/package_3th os.chdir(workdir) files = os.listdir(workdir) for filename in files: print(filename) os.system(docker load -i %s%filename) 三、如何找出容器启动命令及参数方式一：runlike 在runlike传递一个容器名称，就会直接打印出该容器的运行命令。runlike使用起来非常方便，可以直接通过pip安装，也可以通过容器方式免安装使用: # pip$ pip install runlike# by docker$ alias runlike=docker run --rm -v /var/run/docker.sock:/var/run/docker.sock assaflavie/runlike 方式二：get_command_4_run_container 通过容器获取容器的docker run命令get the docker run by a container # 拉取镜像docker pull cucker/get_command_4_run_container# 对于启动命令封装成一个bash命令，后续可以直接使用 get_docker_command [容器名称]/[容器ID]echo alias get_docker_command=docker run --rm -v /var/run/docker.sock:/var/run/docker.sock cucker/get_command_4_run_container ~/.bashrc \\ \\. ~/.bashrc# 启动docker run --rm -v /var/run/docker.sock:/var/run/docker.sock cucker/get_command_4_run_container [容器名称]/[容器ID]# demo示例[root@automlgpu4 bluewhale_dev]# docker pull cucker/get_command_4_run_containerUsing default tag: latestlatest: Pulling from cucker/get_command_4_run_container8a49fdb3b6a5: Pull complete 0357922e53aa: Pull complete 9676b6d4b964: Pull complete ddbd03ee1059: Pull complete 877a053836a3: Pull complete a68de691a1fb: Pull complete ac3fd788d9e3: Pull complete 51fb85f07a5a: Pull complete Digest: sha256:d3fba2642b3bd30dc8bd7379b18e889c6ad9d76c138ffe870b47b7fb6b9c9e94Status: Downloaded newer image for cucker/get_command_4_run_container:latestdocker.io/cucker/get_command_4_run_container:latest[root@automlgpu4 bluewhale_dev]# docker run --rm -v /var/run/docker.sock:/var/run/docker.sock cucker/get_command_4_run_container bluewhale_dev-pg-1docker run -d \\ --name bluewhale_dev-pg-1 \\ --cgroupns host \\ --env POSTGRES_PASSWORD=xxx \\ --env TZ=Asia/Shanghai \\ --env POSTGRES_USER=xxx \\ --hostname pg \\ --label com.docker.compose.config-hash=5a31f16dd78cccc6759eba8ae3183d729013898d96a7b364124fec1efcabf280 \\ --label com.docker.compose.container-number=1 \\ --label com.docker.compose.depends_on= \\ --label com.docker.compose.image=sha256:1149d285a5f5c4340cefa2211869c3a6b1128ac78974545e0b4fe62d3d0e66a8 \\ --label com.docker.compose.oneoff=False \\ --label com.docker.compose.project=bluewhale_dev \\ --label com.docker.compose.project.config_files=/data/bluewhale/bluewhale_dev/docker-compose.yml \\ --label com.docker.compose.project.working_dir=/data/bluewhale/bluewhale_dev \\ --label com.docker.compose.service=pg \\ --label com.docker.compose.version=2.13.0 \\ --log-opt max-size=100m \\ --network=bluewhale_dev_looklook_net \\ -p 5532:5432/tcp \\ --stop-signal SIGINT \\ -v /data/bluewhale/bluewhale_dev/data/pg:/var/lib/postgresql/data:rw \\ postgres:14-alpine 方式三：docker inspect命令docker inspect container name 查看 四、whaler-通过镜像导出dockerfileWhaler Whaler 是一个 Go 程序，旨在将 Docker 镜像逆向到创建它的 Dockerfile 中 # alias export docker image to dockerfile$ alias whaler=docker run -t --rm -v /var/run/docker.sock:/var/run/docker.sock:ro pegleg/whaler","tags":["Docker"],"categories":["云原生"]},{"title":"通过SMTP实现邮件推送服务","path":"/2025/04/23/tech/others/250423-email-push/","content":"最近有一个需要在 W平台开发一个邮件群发推送的功能需求，经过调研，大概有如下几种选型方案: 开源项目 message-pusher: 一款基于Go开发的消息推送服务，开箱即用，支持多种消息推送方式（包括：邮件消息、QQ、企企微应用号、企微群机器人、飞书机器人以及群组消息等），支持 Markdown,可通过 Docker 进行本地部署，也提供了官方部署站。 Austin: 一款基于 Java开发的消息推送平台，支持推送下发邮件、短信、微信服务号、微信小程序、企微、钉钉等消息类型，其核心特点就是统一的接口发送各种类型的消息，对消息生命周期全链路追踪，很适合私有化部署的场景。 第三方服务： 阿里云的邮件推送服务（其他云厂商类似） 免费额度：每个用户有2000封免费额度（每天200封） 购买资源包：1w封半年包19.98元、5w 封半年包90元，这里购买资源包 SendGrid 官方宣称：开始免费（有60天的免费使用权，且每天支持100封），然后按需付费。看价格表收费还是挺贵的，并没有进行使用体验过。 经过综合评估，本着价格合适、开发便捷、服务稳定可靠的选型原则，最终决定采用阿里云的邮件推送服务，下文将介绍基于阿里云的邮件推送服务实现群发功能。 主要流程1. 准备（购买）域名因为在创建发信域名时需要一个域名（该域名可无需备案），若当前没有域名，可点击域名注册进行域名购买，购买完成后，需要进行认证，认证通过后方可使用，在阿里云的域名控制台中可查看已经购买的域名，同时需要确认域名状态为正常。 2. 创建发信域名进入邮件推送控制台，在邮件设置中找到发信域名并新建域名，在新建域名可填写主域名，也可以填写二级域名（推荐），创建完成后，点击配置进入配置页面，根据发信配置中内容，需要去域名控制台域名解析添加记录，如下图操作。 参照上图配置完成后，回到邮件推送控制台的域名管理页中，点击验证，验证通过后列表页面的状态更新为验证通过。 3. 创建发信地址接着点击发信地址页面，新建发信地址，需要填写发信域名（PS:填写第二步创建的域名）、账号（PS:此处我填写的是域名名称）、回信地址非必填、发信类型有两种批量邮件和触发邮件，根据我的需求我选择批量邮件类型，点击确认即可。创建完成后，发信地址列表会有一条记录，接着设置 STMP密码，记住此密码，在第四步中需要使用到。 再点击邮件标签，新建邮件标签，填写一个标签名称即可（PS：此名称回展示在邮件的发件人栏）。 至此，配置工作基本完成。 4. 使用 STMP 服务进行邮件发送在阿里云的邮件推送服务中，支持以 API、SDK以及 STMP 三种方式进行调用。下文代码实现中基于 STMP+Python3进行实现，官网文档 Demo 链接参考：SMTP 之 Python3.6 及以上调用示例 代码实现 pip install dotenv #!/usr/bin/env python# -*- coding: utf-8 -*-import osimport smtplibimport emailimport loggingfrom email.mime.multipart import MIMEMultipartfrom email.mime.text import MIMETextfrom email.mime.application import MIMEApplicationfrom email.header import Headerfrom email.utils import formataddr, make_msgid, formatdatefrom typing import Union, List, Dict, Any, Tuplefrom pathlib import Pathfrom dotenv import load_dotenv# 配置日志logging.basicConfig( level=logging.INFO, format=%(asctime)s - %(name)s - %(levelname)s - %(message)s)logger = logging.getLogger(AliyunEmailSender)# 加载环境变量load_dotenv()class EmailConfig: 邮件配置类，用于存储SMTP服务器相关配置 def __init__( self, username: str | None = None, password: str | None = None, smtp_host: str = smtpdm.aliyun.com, smtp_port: int = 80, from_alias: str | None = None, reply_to: str | None = None, use_ssl: bool | None = None, debug_level: int = 0 ) - None: 初始化邮件配置 Args: username: SMTP用户名 password: SMTP密码 smtp_host: SMTP服务器地址 smtp_port: SMTP服务器端口 from_alias: 发件人显示名称 reply_to: 回复地址 use_ssl: 是否使用SSL连接，若为None则根据端口自动判断 debug_level: SMTP调试级别 (0-无调试, 1-调试) self.username = username or os.getenv(MAIL_USERNAME) self.password = password or os.getenv(MAIL_PASSWORD) self.smtp_host = smtp_host or os.getenv(MAIL_SMTP_HOST, smtpdm.aliyun.com) # 处理端口号 if smtp_port is not None: self.smtp_port = smtp_port else: env_port = os.getenv(MAIL_SMTP_PORT) self.smtp_port = int(env_port) if env_port else 80 self.from_alias = from_alias or os.getenv(MAIL_FROM_ALIAS) self.reply_to = reply_to or os.getenv(MAIL_REPLY_TO) # 自动判断是否使用SSL if use_ssl is None: self.use_ssl = self.smtp_port == 465 else: self.use_ssl = use_ssl self.debug_level = debug_level # 验证配置 self.validate() def validate(self) - None: 验证配置的有效性 if not all([self.username, self.password]): raise ValueError(SMTP配置错误: 用户名和密码不能为空) if not self.smtp_host: raise ValueError(SMTP配置错误: SMTP服务器地址不能为空)class EmailContent: 邮件内容类，用于构建邮件内容 def __init__( self, subject: str, html_body: str | None = None, text_body: str | None = None, attachments: list[Union[str, Path, Tuple[str, bytes, str]]] | None = None ) - None: 初始化邮件内容 Args: subject: 邮件主题 html_body: HTML格式的邮件内容 text_body: 纯文本格式的邮件内容 attachments: 附件列表，可以是文件路径、Path对象或(文件名,数据,MIME类型)元组 self.subject = subject self.html_body = html_body self.text_body = text_body self.attachments = attachments or [] # 验证内容 self.validate() def validate(self) - None: 验证邮件内容的有效性 if not self.html_body and not self.text_body: raise ValueError(邮件内容错误: HTML和纯文本内容不能同时为空)class SMTPEmailSender: 阿里云SMTP邮件服务封装类 def __init__(self, config: EmailConfig | None = None) - None: 初始化邮件发送器 Args: config: 邮件配置，如果为None则使用环境变量创建默认配置 self.config = config or EmailConfig() def create_message( self, content: EmailContent, to_addresses: Union[str, List[str]], cc_addresses: Union[str, List[str]] | None = None, bcc_addresses: Union[str, List[str]] | None = None, extra_headers: Dict[str, str] | None = None ) - MIMEMultipart: 创建邮件消息对象 Args: content: 邮件内容 to_addresses: 收件人地址 cc_addresses: 抄送地址 bcc_addresses: 密送地址 extra_headers: 额外的邮件头部 Returns: MIMEMultipart: 创建的邮件对象 # 处理收件人列表 to_list = to_addresses.split(,) if isinstance(to_addresses, str) else to_addresses cc_list = cc_addresses.split(,) if isinstance(cc_addresses, str) and cc_addresses else [] bcc_list = bcc_addresses.split(,) if isinstance(bcc_addresses, str) and bcc_addresses else [] # 创建复合邮件 msg = MIMEMultipart(alternative) msg[Subject] = Header(content.subject) msg[From] = formataddr([self.config.from_alias, self.config.username]) if self.config.from_alias else self.config.username msg[To] = ,.join(to_list) if cc_list: msg[Cc] = ,.join(cc_list) if self.config.reply_to: msg[Reply-to] = self.config.reply_to # 添加消息ID和日期 msg[Message-id] = make_msgid() msg[Date] = formatdate() # 添加额外头部 if extra_headers: for key, value in extra_headers.items(): msg[key] = value # 添加邮件内容 if content.text_body: text_part = MIMEText(content.text_body, _subtype=plain, _charset=UTF-8) msg.attach(text_part) if content.html_body: html_part = MIMEText(content.html_body, _subtype=html, _charset=UTF-8) msg.attach(html_part) # 添加附件 for attachment in content.attachments: self._add_attachment(msg, attachment) return msg def _add_attachment( self, msg: MIMEMultipart, attachment: Union[str, Path, Tuple[str, bytes, str]] ) - None: 添加附件到邮件 Args: msg: 邮件对象 attachment: 附件，可以是文件路径、Path对象或(文件名,数据,MIME类型)元组 try: if isinstance(attachment, (str, Path)): # 文件路径 path = Path(attachment) filename = path.name with open(path, rb) as f: data = f.read() mimetype = application/octet-stream else: # (文件名,数据,MIME类型)元组 filename, data, mimetype = attachment part = MIMEApplication(data, Name=filename) part[Content-Disposition] = fattachment; filename=filename part[Content-Type] = fmimetype; name=filename msg.attach(part) except Exception as e: logger.error(f添加附件失败: str(e)) def send( self, content: EmailContent, to_addresses: Union[str, List[str]], cc_addresses: Union[str, List[str]] | None = None, bcc_addresses: Union[str, List[str]] | None = None, extra_headers: Dict[str, str] | None = None ) - bool: 发送邮件 Args: content: 邮件内容 to_addresses: 收件人地址，可以是字符串(逗号分隔)或列表 cc_addresses: 抄送地址，可以是字符串(逗号分隔)或列表(可选) bcc_addresses: 密送地址，可以是字符串(逗号分隔)或列表(可选) extra_headers: 额外的邮件头部(可选) Returns: bool: 发送是否成功 # 处理收件人列表 to_list = to_addresses.split(,) if isinstance(to_addresses, str) else to_addresses cc_list = cc_addresses.split(,) if isinstance(cc_addresses, str) and cc_addresses else [] bcc_list = bcc_addresses.split(,) if isinstance(bcc_addresses, str) and bcc_addresses else [] # 所有接收方 receivers = to_list + cc_list + bcc_list # 创建邮件 msg = self.create_message(content, to_addresses, cc_addresses, bcc_addresses, extra_headers) # 发送邮件 try: # 创建SMTP客户端 if self.config.use_ssl: client = smtplib.SMTP_SSL(self.config.smtp_host, self.config.smtp_port) else: client = smtplib.SMTP(self.config.smtp_host, self.config.smtp_port) # 设置调试级别 if self.config.debug_level 0: client.set_debuglevel(self.config.debug_level) try: # 登录 client.login(self.config.username, self.config.password) # 发送邮件 client.sendmail(self.config.username, receivers, msg.as_string()) logger.info(f邮件已成功发送至 len(receivers) 个收件人) return True except smtplib.SMTPException as e: logger.error(fSMTP错误: str(e)) return False finally: # 确保关闭连接 client.quit() except Exception as e: logger.error(f邮件发送失败: str(e)) return False def send_email( self, to_addresses: Union[str, List[str]], subject: str, html_body: str | None = None, text_body: str | None = None, cc_addresses: Union[str, List[str]] | None = None, bcc_addresses: Union[str, List[str]] | None = None, attachments: list[Union[str, Path, Tuple[str, bytes, str]]] | None = None, extra_headers: Dict[str, str] | None = None ) - bool: 兼容旧版API的发送邮件方法 Args: to_addresses: 收件人地址，可以是字符串(逗号分隔)或列表 subject: 邮件主题 html_body: HTML格式的邮件内容(可选，但html_body和text_body至少需要一个) text_body: 纯文本格式的邮件内容(可选，但html_body和text_body至少需要一个) cc_addresses: 抄送地址，可以是字符串(逗号分隔)或列表(可选) bcc_addresses: 密送地址，可以是字符串(逗号分隔)或列表(可选) attachments: 附件列表(可选) extra_headers: 额外的邮件头部(可选) Returns: bool: 发送是否成功 try: # 构建邮件内容 content = EmailContent( subject=subject, html_body=html_body, text_body=text_body, attachments=attachments ) # 发送邮件 return self.send( content=content, to_addresses=to_addresses, cc_addresses=cc_addresses, bcc_addresses=bcc_addresses, extra_headers=extra_headers ) except ValueError as e: logger.error(f邮件参数错误: str(e)) return False except Exception as e: logger.error(f邮件发送失败: str(e)) return Falsedef main() - None: 示例用法 try: # 初始化邮件发送器 - 使用环境变量 email_sender = SMTPEmailSender() # 收件人列表 recipients = 129xxx1@qq.com,sxxx@163.com # 邮件内容 subject = 测试邮件 - 阿里云SMTP邮件服务 html_content = html head style body font-family: Arial, sans-serif; .container padding: 20px; max-width: 600px; margin: 0 auto; .header color: #333; .content line-height: 1.6; .footer margin-top: 30px; font-size: 12px; color: #999; /style /head body div class=container h1 class=header这是一封测试邮件/h1 div class=content p这是通过阿里云SMTP邮件服务发送的测试邮件。/p p如果您收到这封邮件，说明邮件发送功能正常工作。/p /div div class=footer 此邮件为系统自动发送，请勿回复。 /div /div /body /html text_content = 这是一封测试邮件。这是通过阿里云SMTP邮件服务发送的测试邮件。如果您收到这封邮件，说明邮件发送功能正常工作。 此邮件为系统自动发送，请勿回复。 # 1. 使用兼容旧版API发送 logger.info(正在发送测试邮件(方法1)...) success = email_sender.send_email( to_addresses=recipients, subject=subject, html_body=html_content, text_body=text_content ) if success: logger.info(邮件发送成功!) else: logger.error(邮件发送失败!) # 2. 使用新API发送（更灵活） logger.info(正在发送测试邮件(方法2)...) email_content = EmailContent( subject=subject, html_body=html_content, text_body=text_content ) success = email_sender.send( content=email_content, to_addresses=recipients ) if success: logger.info(邮件发送成功!) else: logger.error(邮件发送失败!) except Exception as e: logger.exception(f发生异常: str(e)) # 示例：使用自定义配置 try: # 自定义配置 custom_config = EmailConfig( username=custom@example.com, password=password123, smtp_host=smtp.example.com, smtp_port=587, from_alias=自定义发件人, use_ssl=False ) # 仅作示例，不实际发送 logger.info(使用自定义配置创建发送器示例) _ = SMTPEmailSender(config=custom_config) except Exception as e: logger.exception(f自定义配置示例异常: str(e))if __name__ == __main__: main() .env # 阿里云SMTP邮件服务配置MAIL_USERNAME=your_mail_account@example.com # 发信地址MAIL_PASSWORD=your_smtp_password # SMTP密码MAIL_FROM_ALIAS=Your Name # 发信人名称MAIL_REPLY_TO=your_reply_address@example.com # 回信地址(可选)MAIL_SMTP_HOST=smtpdm.aliyun.com # SMTP服务器MAIL_SMTP_PORT=80 # SMTP端口(80或25为非SSL端口，465为SSL端口)","tags":["邮件推送","python工程实践"],"categories":["技术实践"]},{"title":"每天学点AI","path":"/2025/04/14/tech/modelops/00-daily-study-ai/","content":"20250421: 什么是向量嵌入 ⌛️：2025-04-21 01:37:45 向量嵌入（Vector Embeddings）是讲复杂数据（如文本、图像、音频等）转换为密集数据向量的过程和结果。这些向量通常是高维度的数字数组，使机器能够“理解”数据间的语义关系。 其核心思想是通过数据表示捕捉原始数据的语义信息，讲抽象概念映射到多维空间，这样语义空间的相似性，就可以转化为向量空间中的接近性（数学问题） 向量嵌入工作流程 特征提取：从原始数据（文本、图像等）中识别和提取关键特征 向量化转换：讲提取的特征通过神经网络映射到高维向量空间 维度处理：根据需要进行降维或标准化，优化向量表示 这种机制使计算机能够以数学方式处理和理解复杂的非结构化数据。 向量嵌入的优点（针对数据库场景） 稠密表示：相比传统稀疏向量（如 TF-IDF）更节省存储空间 相似性保持：原始数据相似性在向量空间得以保留（余弦相似度~语义相似度） 跨模态统一：允许文本、图像、视频在同一空间进行联合检索 索引友好：适合 HNSW、IVF-PQ 等近似最近邻算法加速 增量更新：支持新数据库嵌入无需重建整个向量空间 向量嵌入可能存在的问题（数据库视角） 维度膨胀：维度特别多的向量会显著增加存储和内存消耗 距离失真：降维处理可能破坏原始空间关系 版本漂移：不同模型版本难以建立有效索引结构 冷启动：空数据库阶段难以建立有效索引结构 精度衰减：量化压缩（如：int8）导致的检索精度损失 核心应用场景 混合搜索：结合元数据过滤与向量相似性检索（如语义搜索） 内容去重：通过向量距离识别重复、相似内容 智能推荐：基于用户行为向量的实体物品匹配（兴趣相似度计算） 时序分析：追踪向量漂移模式（用户兴趣、内容热点的演化分析） 知识管理：RAG 系统中高效知识检索与上下文关联 聚类分析：自动发现数据中的潜在模式和分组结构 缓存优化：高频查询结果的向量空间缓存加速 20250417: 什么是向量数据库 2025-04-17 23:50:15 向量数据库（Vector Database）是一种专门设计用于存储、管理和搜索向量嵌入（vector embedding）的数据库系统。 其核心价值在于能够高效执行相似性搜索（similarity search），支持 AI应用中常见的“寻找最相似内容”需求，成为现代人工智能基础设施的重要组成部分。 向量数据库的工作原理 向量嵌入存储：将文本、图像等内容通过嵌入模型转换为高维数字向量并存储 相似性搜索：使用近似最近邻（ANN）算法实现高效搜索，支持余弦相似度、内积、欧式距离等多种度量方式 索引机制：通过 HNSW（分层可导航小世界图）实现快速导航，或 IVF（反向文件索引）进行聚类加速 元数据过滤：支持在相似性搜索中结合传统数据库的过滤条件（如时间范围、类别标签） 这种设计使向量数据库能在毫秒级内从百万级甚至数十亿向量中找最相似项。 向量数据库的优势 高效相似性搜索：通过 ANN 算法实现 O(log n)时间复杂度，比传统数据库的精确搜索 O(n)快很多 混合查询能力：可同时处理“找到与这张图片相似且价格低于100元的产品”这类复合查询 AI应用集成：特别适合实现 RAG架构中长期记忆模块 规模可扩展性：支持海量向量数据的存储和检索 多模态支持：同时处理文本、图像、音频等不同类型的嵌入向量 实时性能：支持高并发、低延迟的查询操作 向量数据库可能存在的挑战 精度与速度权衡：更高精度通常意味着更慢的查询速度 资源消耗：高维向量索引可能需要大量内存 嵌入质量依赖：搜索结果质量很大程度上取决于输入嵌入的质量 维度诅咒：随着向量维度增加，搜索效率可能下降 复杂调优需求：最佳性能可能需要专业知识进行参数调优 常见向量数据库 Chroma: 为 AI 应用设计的嵌入式向量数据库 Pinecone: 全托管式向量数据库服务 Weaviate: 开源向量搜索引擎 Milvus: 开源分布式向量数据库 Qdrant: 高性能向量相似度搜索引擎 FAISS(Facebook AI Similarity Search)： 这是算法库而非完整数据库（通常需要配合其他存储系统使用） PGVector: PostgreSQL 的向量扩展，由于 Postgre 性能很强，因此支持的维度特别高。 向量数据库 VS 传统数据库 维度 向量数据库 传统关系型数据库 数据类型 高维向量（非结构化数据特征） 标量（数值、字符串等结构化数据） 查询方式 近似最近邻（ANN）、语义相似性匹配 精确匹配（SQL条件查询） 应用场景 推荐系统、图像检索、RAG、生物信息学 事务处理、报表生成 扩展性 水平扩展支持十亿级向量 垂直扩展为主，单表容量有限 扩展：什么是向量 向量（Vector）是数学和物理学中的基本概念，指同时具有**大小（模长）**和**方向**的量，可形象化为带箭头的线段。例如，位移、速度、力等物理量均为向量 数学表示：在坐标系中，向量可用坐标形式表示（如二维平面中的(2,3)），或通过起点和终点（如向量AB）描述 运算规则：包括加法（平行四边形法则）、减法、数乘、点积（内积）和叉积（外积），适用于物理和工程中的合力分析、投影计算等场景 抽象化扩展：在线性代数中，向量被抽象为向量空间的元素，可能不局限于几何意义上的方向，而是满足特定运算规则的数学对象 简言之：向量是描述多维特征的基础数学工具，而向量数据库则是AI时代处理非结构化数据的核心技术 20250416: 什么是多模态模型 2025-04-16 23:14:36 **多模态模型（Mutlimodel LLM）是能够同时处理和关联多种数据模态（如：文本、图像、视频等）的大语言模型。这类模型通过统一表示空间，实现跨模态的语义理解和内容生成。** 核心工作原理（多模态模型的工作机制包含三个关键阶段）： 模态编码：使用专用编码器（CNNViT 处理图像、BERT 处理文本等）提取各模态特征 特征对齐：通过交叉注意力机制（cross-attention）建立细粒度跨模态关联（如图像区域与文本描述的对应关系） 联合推理：在共享表示空间中进行跨模态信息融合与语义推理 技术优势（系统实现角度） 统一接口：支持自然语言作为跨模态交互的统一接口 知识迁移：视觉-语言等跨模态知识的相互增强 上下文扩展：能同时利用多模态上下文信息（如文本描述+示意图） 数据效率：通过多任务学习提升小样本场景表现 灵活部署：架构灵活性，支持级联式（冻结编码器+可训练适配器）或端到端联合训练架构（不同模态流程整合到单一神经网络中的架构） 实现挑战（工程化角度） 计算复杂度：多模态并行处理带来的显存算力压力 对齐噪声：跨模态数据标注的噪声会影响注意力机制 模态鸿沟：不同模态特征分布的差异导致融合困难 延迟积累：级联架构中各组件（如图像编码器+LLM）的推理延迟叠加问题 评估困境：现有基准（如 MMLU、MMBENCH）难以全面评估跨模态推理能力 20250415: 什么是大模型量化 2025-04-15 01:43:53 量化（Quantization）是一种通过降低模型参数的数值精度来压缩模型大小的技术。在深度学习中，模型的参数通过以32位浮点数（FP32）来存储，通过量化可以将其转换成更低精度的表示形式，从而减少模型的内存占用和计算开销。 FP32的大小是4字节（每个字节8bit, 4字节* 8bit 32 bit）而 FP16的大小是2字节，则为16bit. 这也是为什么大家喜欢用 Q4量化模型的原因，跟 FP16（16bit）的模型相比，Q4（4bit）的模型只有14的大小，运行起来需要的内存也是14. 现在大多数模型训练都采用 FP16的精度，而当下流行的Deepseek-V3采用了 FP8精度训练，能显著提升训练速度和降低硬件成本。 Q4_K_M 是什么意思 这种命名方式一般是 GGUFGGML 格式的模型，它们通常采用 K量化模型，格式类似 Q4_K_M，这里的 Q 的后面数字代表量化精度，K代表 K量化方法，M代表名在尺寸和 PPL(Perplexity,困惑度)之间的平衡度，有0, 1，XS, S, M, L 等 PPL 是评估语言模型性能的重要指标，它用来衡量模型对下一个词的预测准确程度。常见 Kl量化版本的 PPL 对比（这是一个7B 模型） bf16、4bit、int4、fp8是什么意思 bf16是16bit 的精度，4bit 是4bit 精度，同样也建议至少使用4bit 量化的模型，除非模型特别大200B+ 20250414: 什么是 LLM蒸馏技术 2025-04-14 00:25:15 LLM 蒸馏（Distillation）是一种技术，用于将大语言模型(LLM)的知识转移到较小的模型中。其主要目的是在保持模型性能的同时，减少模型的大小和计算资源需求。通过蒸馏技术，较小的模型可以在推理时更高效地运行，适用于资源受限的环境。 蒸馏过程（步骤） 训练教师模型：首先训练一个大型且性能优越的教师模型。 生成软标签：使用教师模型对训练数据进行预测，生成软目标（soft targets），这些目标包含了教师模型的概率分布信息。 训练学生模型：使用软目标和原始训练数据（hard targets）来训练较小的学生模型，使其能够模仿教师模型的行为。这种方法不仅可以提高模型的效率，还可以在某些情况下提高模型的泛化能力。 蒸馏的优点 减少模型大小和计算资源需求 增加推理速度、易于访问和部署 蒸馏可能存在的问题 信息丢失：由于学生模型比教师模型小，可能无法完全捕捉教师模型的所有知识和细节，导致信息丢失。 依赖教师模型：学生模型的性能高度依赖于教师模型的质量，如果教师模型本身存在偏差或错误，学生模型可能会继承这些问题。 适用性限制：蒸馏技术可能不适用于所有类型的模型或任务，尤其是那些需要高精度和复杂推理的任务。 典型例子 GPT-4o(教师模型)中提炼出 GPT-4o-minio(学生模型) Deepseek-R1(教师模型)中提炼出 Deepseek-R1-Distill-Qwen-32B(学生模型)：这个不是传统意义上的蒸馏，是蒸馏+数据增强+微调 其他蒸馏技术 数据增强：使用教师模型生成额外的训练数据。通过创建更大、更具包容性的数据集，学生模型可以接触到更广泛的场景和示例，从而提高其泛化性能。 中间层蒸馏：将知识从教师模型的中间层转移到学生模型。通过学习这些中间表示，学生模型可以捕获更详细和结构化的信息，从而获得更好的整体表现。 多教师蒸馏：通过汇总不同教师模型的知识，学生模型可以实现更全面的理解并提高稳健性，因为它整合了不同的观点和见解。","tags":["LLM","AI"],"categories":["ModelOps"]},{"title":"关于MlFlow的实践总结","path":"/2025/04/09/tech/modelops/03-mlflow-deploy/","content":"基于 Mlflow 进行业务开发已有几个月了，一直忙于开发，没有时间去总结，近期找了个时间好好梳理总结了，故记录下来了。 MLflow中的一些概念功能特性MlFlow（项目地址、官方文档）是一个开源的机器学习模型管理平台，由 Databricks团队开发，可用于解决机器学习领域中的实验管理、模型管理、模型部署以及协作中的关键问题，当前项目 star数已超过20k，社区还是比较活跃的。 目前有六大核心组件，可按需应用到不同的场景中： Experiment Tracking：一组 API，用于记录 ML 实验中的模型、参数和结果，并使用交互式 UI 进行比较。 Model Packaging ：打包模型及其元数据（如依赖项版本）的标准格式，确保可靠的部署和强大的可重复性。 Model Registry ：一个集中的模型存储、一组 API 和 UI，用于协作管理 MLflow 模型的整个生命周期。 Serving ：用于在 Docker、Kubernetes、Azure ML 和 AWS SageMaker 等平台上进行无缝模型部署以进行批量和实时评分的工具。 Evaluation ：一套自动化模型评估工具，与实验跟踪无缝集成，以记录模型性能并直观地比较多个模型的结果。 Observability：跟踪与各种 GenAI 库和 Python SDK 的集成以进行手动检测，提供更流畅的调试体验并支持在线监控。 模型格式规范 基于生产可用的部署方式本部署采用 Mlflow+PostgreSQL+MinIO 的部署架构, 说明： PostgreSQL和 MinIO的部署不在此文章赘述，默认这两服务是正常可用状态。 虚拟环境方式 默认 conda 命令是可用的 创建工作目录# 创建MLflow部署目录mkdir -p /opt/mlflow-deploycd /opt/mlflow-deploy# 创建日志目录mkdir -p /var/log/mlflow 创建并配置Conda环境# 创建Python 3.11的Conda环境conda create -n mlflowpy311envpro python=3.11 -y# 激活环境conda activate mlflowpy311envpro# 安装MLflow 2.18和相关依赖pip install mlflow psycopg2-binary boto3 -i https://pypi.tuna.tsinghua.edu.cn/simple/ 配置环境变量cat /opt/mlflow-production/mlflow.env EOL# MinIO配置export MLFLOW_S3_ENDPOINT_URL=http://192.168.10.117:9966export AWS_ACCESS_KEY_ID=minio@2025export AWS_SECRET_ACCESS_KEY=minio@adminexport MLFLOW_S3_IGNORE_TLS=true# 分片上传优化配置export MLFLOW_ENABLE_MULTIPART_UPLOAD=trueexport MLFLOW_MULTIPART_UPLOAD_CHUNK_SIZE=26214400 # 25MBexport MLFLOW_MAX_PARALLELISM=16 # 并行上传线程数export MLFLOW_S3_MAX_THREADS=16 # S3客户端线程池大小export MLFLOW_S3_MAX_CONNECTION_POOL_SIZE=100 # 连接池大小export MLFLOW_S3_CONNECT_TIMEOUT=60 # 连接超时(秒)export MLFLOW_S3_READ_TIMEOUT=300 # 读取超时(秒)export MLFLOW_ARTIFACT_UPLOAD_TIMEOUT=7200 # 制品上传超时时间(秒)# 数据库配置export MLFLOW_BACKEND_STORE_URI=postgresql://root:postgre2025@192.168.10.117:30438/mlflow # 此数据库需要手动先创建好，待启动 mlflow 时会自动初始化数据库表。# Artifact存储配置export MLFLOW_ARTIFACT_ROOT=s3://mlflowhub #此 Bucket需要手动在MinIO的 Dashboard中创建# 服务器配置export MLFLOW_HOST=0.0.0.0export MLFLOW_PORT=8968EOL# 设置权限chmod 600 /opt/mlflow-deploy/mlflow.env 创建启动脚本cat /opt/mlflow-deploy/start_mlflow.sh EOL#!/bin/bash# 加载环境变量source /opt/mlflow-deploy/mlflow.env# 激活Conda环境source /root/miniconda3/bin/activate mlflowpy311envpro# 设置Python缓冲区export PYTHONUNBUFFERED=1# 为boto3设置配置export S3_USE_SIGV4=Trueexport AWS_S3_ADDRESSING_STYLE=virtual# 启动MLflow服务mlflow server \\ --backend-store-uri $MLFLOW_BACKEND_STORE_URI \\ --default-artifact-root $MLFLOW_ARTIFACT_ROOT \\ --host $MLFLOW_HOST \\ --port $MLFLOW_PORT \\ --artifacts-destination $MLFLOW_ARTIFACT_ROOT \\ --serve-artifacts \\ --workers 8 \\ /var/log/mlflow/mlflow.log 21EOL# 设置权限chmod +x /opt/mlflow-deploy/start_mlflow.sh 启动服务 下列两种启动方式，选择一种即可，生产部署建议采用systemd的方式启动 使用Systemd服务 创建systemd服务 cat /etc/systemd/system/mlflow.service EOL[Unit]Description=MLflow Tracking ServerAfter=network.target[Service]Type=simpleUser=rootWorkingDirectory=/opt/mlflow-productionExecStart=/bin/bash -c source /root/miniconda3/bin/activate mlflowpy311envpro mlflow server --backend-store-uri postgresql://root:postgre2025@192.168.10.117:30438/mlflow --default-artifact-root s3://mlflowhub --host 0.0.0.0 --port 8968 --artifacts-destination s3://mlflowhub --serve-artifactsEnvironment=MLFLOW_S3_ENDPOINT_URL=http://192.168.10.117:9966Environment=AWS_ACCESS_KEY_ID=minio@2025Environment=AWS_SECRET_ACCESS_KEY=minio@adminEnvironment=MLFLOW_S3_IGNORE_TLS=trueEnvironment=MLFLOW_ENABLE_MULTIPART_UPLOAD=trueEnvironment=MLFLOW_MULTIPART_UPLOAD_CHUNK_SIZE=26214400Restart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOL# 重新加载systemd配置systemctl daemon-reload 启动MLflow服务 # 启动服务systemctl start mlflow# 设置开机自启systemctl enable mlflow# 检查服务状态systemctl status mlflow mlflow日志查看 # 查看最新日志tail -f /var/log/mlflow/mlflow.log# 查看错误信息grep ERROR /var/log/mlflow/mlflow.log# 重启服务systemctl restart mlflow 使用nohup服务改写启动脚本，在启动命令前后加上nohup 即可 cat /opt/mlflow-deploy/start_mlflow.sh EOL#!/bin/bash# 加载环境变量source /opt/mlflow-deploy/mlflow.env# 激活Conda环境source /root/miniconda3/bin/activate mlflowpy311envpro# 设置Python缓冲区export PYTHONUNBUFFERED=1# 为boto3设置配置export S3_USE_SIGV4=Trueexport AWS_S3_ADDRESSING_STYLE=virtual# 启动MLflow服务 # ###############修改此处###########################nohup mlflow server \\ --backend-store-uri $MLFLOW_BACKEND_STORE_URI \\ --default-artifact-root $MLFLOW_ARTIFACT_ROOT \\ --host $MLFLOW_HOST \\ --port $MLFLOW_PORT \\ --artifacts-destination $MLFLOW_ARTIFACT_ROOT \\ --serve-artifacts \\ --workers 8 \\ /var/log/mlflow/mlflow.log 21 EOL# 设置权限chmod +x /opt/mlflow-deploy/start_mlflow.sh Docker-compose方式环境要求 Docker 和 Docker Compose 确保 8968 端口未被占用 确保 MinIO 服务已启动并可访问 确保 PostgreSQL 数据库已创建 创建docker-compose.ymlversion: 3.3services: mlflow: build: . container_name: mlflow-server restart: always ports: - $MLFLOW_HOST_PORT:-8968:$MLFLOW_PORT:-5000 volumes: - $MLFLOW_DATA_DIR:-./mlruns:/mlflow/mlruns environment: - TZ=$TZ:-Asia/Shanghai - PYTHONUNBUFFERED=$PYTHONUNBUFFERED:-1 - GUNICORN_CMD_ARGS=$GUNICORN_CMD_ARGS # MLflow 基础配置 - MLFLOW_TRACKING_URI=$MLFLOW_TRACKING_URI # PostgreSQL 配置 - MLFLOW_BACKEND_STORE_URI=$MLFLOW_BACKEND_STORE_URI # MinIO 配置 - AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID - AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY - MLFLOW_S3_ENDPOINT_URL=$MLFLOW_S3_ENDPOINT_URL - MLFLOW_DEFAULT_ARTIFACT_ROOT=$MLFLOW_DEFAULT_ARTIFACT_ROOT # SSL 配置 - MLFLOW_S3_IGNORE_TLS=$MLFLOW_S3_IGNORE_TLS # 大文件上传配置 - MLFLOW_ENABLE_PROXY_MULTIPART_UPLOAD=$MLFLOW_ENABLE_PROXY_MULTIPART_UPLOAD - MLFLOW_MULTIPART_UPLOAD_MINIMUM_FILE_SIZE=$MLFLOW_MULTIPART_UPLOAD_MINIMUM_FILE_SIZE - MLFLOW_MULTIPART_UPLOAD_CHUNK_SIZE=$MLFLOW_MULTIPART_UPLOAD_CHUNK_SIZE command: mlflow server --host $MLFLOW_HOST:-0.0.0.0 --port $MLFLOW_PORT:-5000 networks: - mlflow-networknetworks: mlflow-network: driver: bridge 创建DockerfileFROM ghcr.io/mlflow/mlflow:v2.20.3# 设置工作目录WORKDIR /mlflow# 复制依赖文件COPY requirements.txt /mlflow/requirements.txt# 设置阿里云镜像源并安装依赖RUN pip install --no-cache-dir -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com -r requirements.txt# 复制配置文件COPY .env /mlflow/.env# 设置环境变量ENV PYTHONUNBUFFERED=1ENV GUNICORN_CMD_ARGS=--timeout 120 --workers 4 --threads 2# 暴露端口EXPOSE 5000# 启动命令CMD [mlflow, server, --host, 0.0.0.0, --port, 5000] 创建requirements.txtpsycopg2-binary==2.9.9boto3==1.34.34 创建.env# 容器配置MLFLOW_HOST_PORT=8968MLFLOW_DATA_DIR=./mlrunsTZ=Asia/Shanghai# MLflow 基础配置PYTHONUNBUFFERED=1GUNICORN_CMD_ARGS=--timeout 120 --workers 4 --threads 2MLFLOW_TRACKING_URI=http://192.168.3.117:8968# PostgreSQL 配置MLFLOW_BACKEND_STORE_URI=postgresql://root:postgre2025@192.168.3.117:30438/mlflow# MinIO 配置 - 使用 MLflow 需要的环境变量名AWS_ACCESS_KEY_ID=H0B1O3rSjic9uvEt AWS_SECRET_ACCESS_KEY=sKY9teJFLByLDjs3Ux4jw65koq3b7ihlMLFLOW_S3_ENDPOINT_URL=http://192.168.2.117:9966MLFLOW_DEFAULT_ARTIFACT_ROOT=s3://mlflowhub/# 禁用 SSL 验证MLFLOW_S3_IGNORE_TLS=true# 大文件上传配置MLFLOW_ENABLE_PROXY_MULTIPART_UPLOAD=trueMLFLOW_MULTIPART_UPLOAD_MINIMUM_FILE_SIZE=104857600MLFLOW_MULTIPART_UPLOAD_CHUNK_SIZE=26214400# 服务器配置MLFLOW_HOST=0.0.0.0MLFLOW_PORT=5000MLFLOW_WORKERS=4 启动服务docker-compose up -d 验证服务# 检查容器状态docker-compose ps# 检查日志docker-compose logs -f 访问 MLflow UI: 打开浏览器访问: http://192.16.3.117:9868 FAQ 初始化数据库: 正常情况下，只要在PostgreSQL中创建好mlflow数据库后，在启动服务时会自动初始化数据库表，但在实践中确实也遇到了 没有自动初始化的情况，所以提供下列方法 # 使用环境变量连接数据库docker-compose run --rm mlflow mlflow db upgrade -U $MLFLOW_BACKEND_STORE_URI# 或者在本地安装 mlflow 后执行pip install mlflowmlflow db upgrade -U postgresql://root:postgre2025@192.168.3.117:30438/mlflow 5.","tags":["Mlflow"],"categories":["ModelOps"]},{"title":"新的生产力","path":"/2025/04/01/tools/250401-new-tools/","content":"自进入2025年后，用了五年多的16寸MacBookPo（2019款 32G+512）就 很突然的开始出现各种自动重启、频繁死机状态，很影响心态。期间打Apple售后线上排查过、也拿去市里最大的 Apple 店线下排查过，售后人员给出的结论是硬件有问题需要更换，考虑是 Inter老芯片加上已有些年限，再花几千维修费，着实有些不值，权衡利弊之后， 最终在3月12日线上买一台新的14寸的MacBookPro（M4 Pro + 48G+512, 14+20）,在国补-2000，教育补贴-1650的加持下，性价比自认为还是相当 OK 的。 一直想写下来记录一下，奈何三月工作太忙，一直拖到今天才有时间，接下来介绍一下新设备到手后的一些常用设置和常用软件的安装。 装机设置默认配置修改# 禁止 “Are you sure you want to open this application?” 提示defaults write com.apple.LaunchServices LSQuarantine -bool false# 禁止磁盘映像验证defaults write com.apple.frameworks.diskimages skip-verify -bool truedefaults write com.apple.frameworks.diskimages skip-verify-locked -bool truedefaults write com.apple.frameworks.diskimages skip-verify-remote -bool true# 桌面隐藏外部磁盘和可移动介质defaults write com.apple.finder ShowExternalHardDrivesOnDesktop -bool falsedefaults write com.apple.finder ShowRemovableMediaOnDesktop -bool false# 显示所有扩展名和隐藏文件defaults write -g AppleShowAllExtensions -bool truedefaults write com.apple.finder AppleShowAllFiles -bool true# 禁用修改扩展名时的警告defaults write com.apple.finder FXEnableExtensionChangeWarning -bool false# 显示底部地址栏defaults write com.apple.finder ShowPathbar -bool true# 禁止创建 .DS_Store 文件defaults write com.apple.desktopservices DSDontWriteNetworkStores -bool true 允许打开任何来源的应用sudo spctl --master-disable 然后去『系统设置 安全性与隐私 安全性 任何来源』打开 提示 已损坏无法确认开发者身份开启【任何来源】后，还有部分软件在安装后会提示【已损坏】或【无法确认开发者身份】，需要使用xattr命令 sudo xattr -dr com.apple.quarantine /Applications/[应用名称].app 当不知道如何找到应用名称时，打开一个终端和访达中的应用程序，将需要执行的软件拖到终端就可以知道软件的全路径，接着拷贝最后的名称即可。 终端Git中文乱码git config --global core.quotepath false 装机必备软件 做为一个开发者必备的装机软件 Xcode Command Line Tools# 安装 Xcode Command Line Toolsxcode-select --install# 删除 Xcode Command Line Toolssudo rm -rf /Library/Developer/CommandLineTools Homebrew Mac的软件包管理工具，用于安装卸载管理各种软件，包括命令行工具、库和应用程序等 Gitihub地址 # 官方源安装/bin/bash -c $(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)# 国内源完整版安装/bin/zsh -c $(curl -fsSL https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)# 极速安装（update 功能需要命令修复）/bin/zsh -c $(curl -fsSL https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh) speed 安装参考链接： Homebrew 国内安装脚本 Homebrew 安装教程 brew 常用命令 # 更新 Homebrewbrew update# 安装指定的软件包brew install package# 卸载指定的软件包brew uninstall package# 搜索可用的软件包，并显示匹配的结果brew search query# 列出已安装的软件包brew list# 查看可以升级的软件包brew outdated# 升级所有软件包到最新版本brew upgrade# 升级指定的软件包到最新版本brew upgrade package# 查看指定软件包的信息，包括版本号、安装路径、依赖关系等brew info package# 清理 Homebrew 临时文件和旧版本的软件包brew cleanup# 查看可以清理的 Homebrew 临时文件和旧版本的软件包brew cleanup -n iTerm2 系统内置终端替代品 安装说明 brew install --cask iterm2 配色方案参考链接 iTerm2-Color-Schemes iTerm2-Material-Design 修改默认shell为zsh # 查看所有的 shellcat /etc/shells# 查看当前窗口使用的 shellecho $SHELL# 查看系统用户默认的 shellcat /etc/passwd | grep sh# 切换系统默认 shellchsh -s /bin/zsh 常用快捷键 操作 含义 ⌘ + N 新建窗口 ⌘ + T 新建 Tab ⌘ + W 关闭 Tab 窗口 ⌘ + ← 切换到上一个 Tab ⌘ + → 切换到下一个 Tab ⌘ + 数字 快速切换 Tab ⌘ + D 垂直分屏 ⌘ + Shift + D 水平分屏 ⌘ + Enter 切换全屏 ⌘ + F 查找 ⌘ + 查看光标的位置 Ctrl + P 上一条命令 Ctrl + R 搜索命令历史 Ctrl + L 清屏 Ctrl + U 清除当前行 Ctrl + W 删除光标前面的一个单词 Ctrl + K 删除光标后面的所有字符 Ctrl + A 移动光标到行首 Ctrl + E 移动光标到行尾 mas-cli Mac App Store命令行工具 brew 安装 brew install mas 常用命令 # 搜索应用程序mas search [query]# 列出已经安装的应用程序mas list# 通过应用程序的 ID 进行安装，可以从 search 命令或者应用程序网页中获取mas install [app-id]# 升级已经安装的应用程序mas upgrade# 列出有更新可用的应用程序mas outdated duti 设置默认应用程序的命令行工具 brew 安装 brew install duti 常用命令 # 查看指定文件类型的默认应用程序duti -x txt# 更改文件类型的默认应用程序duti -s com.apple.TextEdit .txt all 科学上网 wmsxwd 星辰大海 一元机场","tags":["Mac"],"categories":["ToolBox"]},{"title":"Git的约定式提交规范","path":"/2025/03/20/tech/devops/01-git-commit-lint/","content":"概述约定式提交规范是一种基于提交信息的轻量级约定。它提供了一组简单规则来创建清晰的提交历史；这更有利于编写自动化工具。通过在提交信息中描述功能、修复和破坏性变更，使这种惯例与 SemVer 相互对应。 提交说明的结构如下所示： 原文： type[optional scope]: description[optional body][optional footer(s)] 译文： 类型[可选 范围]: 描述[可选 正文][可选 脚注] 提交说明包含了下面的结构化元素，以向类库使用者表明其意图： fix: 类型 为 fix 的提交表示在代码库中修复了一个 bug（这和语义化版本中的 PATCH 相对应）。 feat: 类型 为 feat 的提交表示在代码库中新增了一个功能（这和语义化版本中的 MINOR 相对应）。 BREAKING CHANGE: 在脚注中包含 BREAKING CHANGE: 或 类型(范围) 后面有一个 ! 的提交，表示引入了破坏性 API 变更（这和语义化版本中的 MAJOR 相对应）。破坏性变更可以是任意 类型 提交的一部分。 除 fix: 和 feat: 之外，也可以使用其它提交 类型 ，例如 @commitlintconfig-conventional（基于 Angular 约定）中推荐的 build:、chore:、 ci:、docs:、style:、refactor:、perf:、test:，等等。 build: 用于修改项目构建系统，例如修改依赖库、外部接口或者升级 Node 版本等； chore: 用于对非业务性代码进行修改，例如修改构建流程或者工具配置等； ci: 用于修改持续集成流程，例如修改 Travis、Jenkins 等工作流配置； docs: 用于修改文档，例如修改 README 文件、API 文档等； style: 用于修改代码的样式，例如调整缩进、空格、空行等； refactor: 用于重构代码，例如修改代码结构、变量名、函数名等但不修改功能逻辑； perf: 用于优化性能，例如提升代码的性能、减少内存占用等； test: 用于修改测试用例，例如添加、删除、修改代码的测试用例等。 脚注中除了 BREAKING CHANGE: description ，其它条目应该采用类似 git trailer format 这样的惯例。 其它提交类型在约定式提交规范中并没有强制限制，并且在语义化版本中没有隐式影响（除非它们包含 BREAKING CHANGE）。 可以为提交类型添加一个围在圆括号内的范围，以为其提供额外的上下文信息。例如 feat(parser): adds ability to parse arrays.。 示例包含了描述并且脚注中有破坏性变更的提交说明feat: allow provided config object to extend other configsBREAKING CHANGE: `extends` key in config file is now used for extending other config files 包含了 ! 字符以提醒注意破坏性变更的提交说明feat!: send an email to the customer when a product is shipped 包含了范围和破坏性变更 ! 的提交说明feat(api)!: send an email to the customer when a product is shipped 包含了 ! 和 BREAKING CHANGE 脚注的提交说明chore!: drop support for Node 6BREAKING CHANGE: use JavaScript features not available in Node 6. 不包含正文的提交说明docs: correct spelling of CHANGELOG 包含范围的提交说明feat(lang): add polish language 包含多行正文和多行脚注的提交说明fix: prevent racing of requestsIntroduce a request id and a reference to latest request. Dismissincoming responses other than from latest request.Remove timeouts which were used to mitigate the racing issue but areobsolete now.Reviewed-by: ZRefs: #123 约定式提交规范本文中的关键词 “必须（MUST）”、“禁止（MUST NOT）”、“必要（REQUIRED）”、“应当（SHALL）”、“不应当（SHALL NOT）”、“应该（SHOULD）”、“不应该（SHOULD NOT）”、“推荐（RECOMMENDED）”、“可以（MAY）” 和 “可选（OPTIONAL）” ，其相关解释参考 RFC 2119 。 每个提交都必须使用类型字段前缀，它由一个名词构成，诸如 feat 或 fix ， 其后接可选的范围字段，可选的 !，以及必要的冒号（英文半角）和空格。 当一个提交为应用或类库实现了新功能时，必须使用 feat 类型。 当一个提交为应用修复了 bug 时，必须使用 fix 类型。 范围字段可以跟随在类型字段后面。范围必须是一个描述某部分代码的名词，并用圆括号包围，例如： fix(parser): 描述字段必须直接跟在 类型(范围) 前缀的冒号和空格之后。 描述指的是对代码变更的简短总结，例如： fix: array parsing issue when multiple spaces were contained in string 。 在简短描述之后，可以编写较长的提交正文，为代码变更提供额外的上下文信息。正文必须起始于描述字段结束的一个空行后。 提交的正文内容自由编写，并可以使用空行分隔不同段落。 在正文结束的一个空行之后，可以编写一行或多行脚注。每行脚注都必须包含 一个令牌（token），后面紧跟 :space 或 space# 作为分隔符，后面再紧跟令牌的值（受 git trailer convention 启发）。 脚注的令牌必须使用 - 作为连字符，比如 Acked-by (这样有助于 区分脚注和多行正文)。有一种例外情况就是 BREAKING CHANGE，它可以被认为是一个令牌。 脚注的值可以包含空格和换行，值的解析过程必须直到下一个脚注的令牌分隔符出现为止。 破坏性变更必须在提交信息中标记出来，要么在 类型(范围) 前缀中标记，要么作为脚注的一项。 包含在脚注中时，破坏性变更必须包含大写的文本 BREAKING CHANGE，后面紧跟着冒号、空格，然后是描述，例如： BREAKING CHANGE: environment variables now take precedence over config files 。 包含在 类型(范围) 前缀时，破坏性变更必须通过把 ! 直接放在 : 前面标记出来。 如果使用了 !，那么脚注中可以不写 BREAKING CHANGE:， 同时提交信息的描述中应该用来描述破坏性变更。 在提交说明中，可以使用 feat 和 fix 之外的类型，比如：docs: updated ref docs. 。 工具的实现必须不区分大小写地解析构成约定式提交的信息单元，只有 BREAKING CHANGE 必须是大写的。 BREAKING-CHANGE 作为脚注的令牌时必须是 BREAKING CHANGE 的同义词。 为什么使用约定式提交 自动化生成 CHANGELOG。 基于提交的类型，自动决定语义化的版本变更。 向同事、公众与其他利益关系者传达变化的性质。 触发构建和部署流程。 让人们探索一个更加结构化的提交历史，以便降低对你的项目做出贡献的难度。 FAQ在初始开发阶段我该如何处理提交说明？我们建议你按照假设你已发布了产品那样来处理。因为通常总 有人 使用你的软件，即便那是你软件开发的同事们。他们会希望知道诸如修复了什么、哪里不兼容等信息。 提交标题中的类型是大写还是小写?大小写都可以，但最好是一致的。 如果提交符合多种类型我该如何操作？回退并尽可能创建多次提交。约定式提交的好处之一是能够促使我们做出更有组织的提交和 PR。 这不会阻碍快速开发和迭代吗？它阻碍的是以杂乱无章的方式快速前进。它助你能在横跨多个项目以及和多个贡献者协作时长期地快速演进。 约定式提交会让开发者受限于提交的类型吗（因为他们会想着已提供的类型）？约定式提交鼓励我们更多地使用某些类型的提交，比如 fixes。除此之外，约定式提交的灵活性也允许你的团队使用自己的类型，并随着时间的推移更改这些类型。 这和 SemVer 有什么关联呢？fix 类型提交应当对应到 PATCH 版本。feat 类型提交应该对应到 MINOR 版本。带有 BREAKING CHANGE 的提交不管类型如何，都应该对应到 MAJOR 版本。 我对约定式提交做了形如 @jameswomack/conventional-commit-spec 的扩展，该如何版本化管理这些扩展呢？我们推荐使用 SemVer 来发布你对于这个规范的扩展（并鼓励你创建这些扩展！） 如果我不小心使用了错误的提交类型，该怎么办呢？当你使用了在规范中但错误的类型时，例如将 feat 写成了 fix在合并或发布这个错误之前，我们建议使用 git rebase -i 来编辑提交历史。而在发布之后，根据你使用的工具和流程不同，会有不同的清理方案。 当使用了 不在 规范中的类型时，例如将 feat 写成了 feet在最坏的场景下，即便提交没有满足约定式提交的规范，也不会是世界末日。这只意味着这个提交会被基于规范的工具错过而已。 所有的贡献者都需要使用约定式提交规范吗？并不！如果你使用基于 squash 的 Git 工作流，主管维护者可以在合并时清理提交信息——这不会对普通提交者产生额外的负担。有种常见的工作流是让 git 系统自动从 pull request 中 squash 出提交，并向主管维护者提供一份表单，用以在合并时输入合适的 git 提交信息。 约定式提交规范中如何处理还原（revert）提交?还原提交（Reverting）会比较复杂：你还原的是多个提交吗？如果你还原了一个功能模块，下次发布的应该是补丁吗？ 约定式提交不能明确的定义还原行为。所以我们把这个问题留给工具开发者，基于 类型 和 脚注 的灵活性来开发他们自己的还原处理逻辑。 一种建议是使用 revert 类型，和一个指向被还原提交摘要的脚注： revert: let us never again speak of the noodle incidentRefs: 676104e, a215868","tags":["Git"],"categories":["DevOps"]},{"title":"Kserve之RawDeployment模式部署","path":"/2025/03/15/tech/modelops/02-kserve-deploy/","content":"前置说明Kserve 有多种安装部署模式，包括 Serverless、ModelMesh 以及 Kubernetes Deployment，下文的安装模型采用的 Kubernetes Deployment 中的 RawDeployment 模式，其他安装详见官方文档， 官方文档提到安装 Kserve 需要依赖 Istio 和 Cert Manager、由于 Isito 组件比较重且不太了解，考虑到我的目标是为了快速完成 kserve 的部署，所以就使用 Ingress-nginx 来替代了 Istio，效果是一样。另外安装时需要注意版本对应关系 K8s v1.29.7 KServe v0.13.0 Cert-manager v1.15.3 Ingress-nginx（与k8s版本对应关系） v4.11.2 依赖组件ingress-nginx 部署 该组件采用 Helm 安装 官方文档 安装包链接 如果在外网状态良好时，可以采用在线安装，由于当前是公司内网，所以采用将 helm 安装包下载下来，进行离线安装 # 添加repohelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx# 下载ingress-nginx.tgzwget https://github.com/kubernetes/ingress-nginx/releases/download/helm-chart-4.11.2/ingress-nginx-4.11.2.tgz# 解压tar -zxf ingress-nginx-4.11.2.tgz# 安装helm install ingress-nginx ./ingress-nginx --namespace ingress-nginx --create-namespace# 由于ingress-nginx依赖的镜像被墙无法下载，所以需要曲线救国，主要删除value.yaml中的镜像hash,需要在执行节点下载docker pull k8s.kubesre.xyz/ingress-nginx/kube-webhook-certgen:v1.4.3docker tag k8s.kubesre.xyz/ingress-nginx/kube-webhook-certgen:v1.4.3 registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3docker pull k8s.kubesre.xyz/ingress-nginx/controller:v1.11.2docker tag k8s.kubesre.xyz/ingress-nginx/controller:v1.11.2 registry.k8s.io/ingress-nginx/controller:v1.11.2 安装成功后满刻看到如下信息 (base) [root@node77 ingress-nginx]# helm install ingress-nginx ./ingress-nginx --namespace ingress-nginxNAME: ingress-nginxLAST DEPLOYED: Fri Aug 30 19:34:39 2024NAMESPACE: ingress-nginxSTATUS: deployedREVISION: 1TEST SUITE: NoneNOTES:The ingress-nginx controller has been installed.It may take a few minutes for the load balancer IP to be available.You can watch the status by running kubectl get service --namespace ingress-nginx ingress-nginx-controller --output wide --watchAn example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example namespace: foo spec: ingressClassName: nginx rules: - host: www.example.com http: paths: - pathType: Prefix backend: service: name: exampleService port: number: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tlsIf TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: base64 encoded cert tls.key: base64 encoded key type: kubernetes.io/tls(base) [root@node77 ingress-nginx]# kubectl get service --namespace ingress-nginx ingress-nginx-controller --output wide --watchNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORingress-nginx-controller LoadBalancer 10.96.1.190 pending 80:32702/TCP,443:32064/TCP 37s app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx CertManager 部署本组件采用 YAML 方式安装，若使用 Helm 安装，请参考：Link 文档地址 下载地址 kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.15.3/cert-manager.yaml(base) [root@node77 cert-manager]# kubectl get pods --namespace cert-managerNAME READY STATUS RESTARTS AGEcert-manager-cainjector-5fd6444f95-rbczx 1/1 Running 0 80scert-manager-d894bbbd4-zd8ns 1/1 Running 0 80scert-manager-webhook-869674f96f-cgp92 1/1 Running 0 80s kserve 部署安装 文档地址： 当前最新版本 v0.13.0 安装指南 kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.13.0/kserve.yamlkubectl apply -f https://github.com/kserve/kserve/releases/download/v0.13.0/kserve-cluster-resources.yaml# 修改 inferenceservice-config 的部署模型为 RawDeploymentkubectl patch configmap/inferenceservice-config -n kserve --type=strategic -p data: deploy: \\defaultDeploymentMode\\: \\RawDeployment\\# 将kserve.yaml中ingressClassName的值修改为nginx，默认值为istioingress: |- ingressClassName : nginx, 测试 新建一个 Namespace 不要放在默认 namespace 下，很重要很关键， kubectl create namespace test 创建一个 inferenceservice 官方的案例的 yaml 中，并没有指定 namespace，如果执行下列命令就会创建到默认 namespace 下，后续部署模型就会遇到问题，该问题排查了好久。 官网示例 kubectl apply -n test -f - EOFapiVersion: serving.kserve.io/v1beta1kind: InferenceServicemetadata: name: sklearn-irisspec: predictor: model: modelFormat: name: sklearn storageUri: gs://kfserving-examples/models/sklearn/1.0/modelEOF 对上述内容进行修改。 新建 yaml 文件(sklearn-iris-model.yaml) apiVersion: serving.kserve.io/v1beta1kind: InferenceServicemetadata: name: sklearn-iris-1 namespace: test # 这个很关键很重要spec: predictor: model: modelFormat: name: sklearn storageUri: http://172.16.x.xx:9999/model.joblib 由于官方给的示例的模型存储在 gs 上，一般情况下无法直接下载，我个人的解决方法是复制storageUri 的地址（modelssklearn1.0model），点开平台下载链接的后，将路径拼在后面进行下载到本地（需要注意，需要下载该模型前缀下的所有资源，并放到一个文件夹下），然后在上传到部署的环境中。 在模型文件夹下，启动一个 httpserver 该步骤非必须，如果可以直接从 gs 下，则忽略这一步。启动外部服务的目的是能够让 kserve 能够去下载模型 (base) [root@node77 test-kserve-deploy]# python -m http.server 9999Serving HTTP on 0.0.0.0 port 9999 (http://0.0.0.0:9999/) ...127.0.0.1 - - [03/Sep/2024 12:19:13] GET / HTTP/1.1 200 -(base) [root@node77 kserve]# curl 127.0.0.1:9999!DOCTYPE HTMLhtml lang=enheadmeta charset=utf-8titleDirectory listing for //title/headbodyh1Directory listing for //h1hrullia href=mlflow-iris.ymlmlflow-iris.yml/a/lilia href=mlflow-wine-model.yamlmlflow-wine-model.yaml/a/lilia href=mlflow_wine/mlflow_wine//a/lilia href=mlflow_wine.zipmlflow_wine.zip/a/lilia href=my_model/my_model//a/lilia href=my_model.zipmy_model.zip/a/lilia href=python_env.yamlpython_env.yaml/a/lilia href=python_model.pklpython_model.pkl/a/lilia href=sklearn_iris/sklearn_iris//a/lilia href=tmp/tmp//a/li/ulhr/body/html(base) [root@node77 kserve]# 执行 yaml 文件 # 在此目录下(base) [root@node77 sklearn_iris]# lsmodel.joblib sklearn-iris-model.yaml# 执行命令kubectl apply -f sklearn-iris-model.yaml 查看模型部署状态 (base) [root@node77 sklearn_iris]# kubectl get all -n testNAME READY STATUS RESTARTS AGEpod/sklearn-iris-1-predictor-84d48876bb-wvlm7 1/1 Running 0 40sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/sklearn-iris-1-predictor ClusterIP 10.96.0.243 none 80/TCP 40sNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/sklearn-iris-1-predictor 1/1 1 1 40sNAME DESIRED CURRENT READY AGEreplicaset.apps/sklearn-iris-1-predictor-84d48876bb 1 1 1 40sNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEhorizontalpodautoscaler.autoscaling/sklearn-iris-1-predictor Deployment/sklearn-iris-1-predictor unknown/80% 1 1 1 40s 查看 inferenceservice 状态 (base) [root@node77 sklearn_iris]# kubectl get inferenceservice -n testNAME URL READY PREV LATEST PREVROLLEDOUTREVISION LATESTREADYREVISION AGEsklearn-iris-1 http://sklearn-iris-1-test.example.com True 5m48s 使用 ingress 测试 查看 ingress-nginx 访问地址 (base) [root@node77 sklearn_iris]# kubectl get svc -n testNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEsklearn-iris-1-predictor ClusterIP 10.96.0.243 none 80/TCP 7m2s 编写输入参数 文件，input.json instances: [ [6.8, 2.8, 4.8, 1.4], [6.0, 3.4, 4.5, 1.6] ] 编写测试脚本 #!/bin/bash# nginx ingress 的访问地址INGRESS_HOST=10.96.0.243INGRESS_PORT=80MODEL_NAME=sklearn-iris-1INPUT_PATH=@./input.jsonSERVICE_HOSTNAME=$(kubectl get inferenceservice $MODEL_NAME -o jsonpath=.status.url | cut -d / -f 3)# 使用 curl 发起请求curl -H Content-Type: application/json -H Host: $SERVICE_HOSTNAME http://$INGRESS_HOST:$INGRESS_PORT/v1/models/$MODEL_NAME:predict -d $INPUT_PATH 执行测试 sh -x infer_test.sh 测试结果如下图","tags":["K8S","Kserve"],"categories":["ModelOps"]},{"title":"关于ModelOps的相关技术","path":"/2025/03/15/tech/modelops/01-modelops-tech/","content":"框架级模型开发Scikit-learn（SKlearn） 官网 简介：一个基于 SciPy 构建的用于机器学习的 Python 模块，并根据 3-Clause BSD 许可证分发 PyTorch 官网 简介：Python 中的张量和动态神经网络，具有强大的 GPU 加速功能 模型解释Alibi Explain 官网 Github 简介： 模型检测Alibi Detect 官网 Github 简介：是一个 Python 开源库，专注于异常值、对抗性、和漂移检测。该库涵盖表格数据、文本、图像和时间序列的在线和离线检测器，TensorFlow 和 PyTorch 后端都支持漂移检测。 模型管理MLflow 官网 Github 简介：MLflow 是一个开源平台，专门用于帮助机器学习从业者和团队处理机器学习过程的复杂性，专注于机器学习项目的整个生命周期，确保每个阶段都是可管理、可追溯和可再现的。 模型部署Seldon-core 官网 Github 简介：一款用于打包、部署、监控和管理数千个生产机器学习模型的 MLOps 框架 Kserve 官网 Github 简介：一款基于 Kubernetes 高度可扩展且基于标准的模型部署与推理平台 Ray 官网 Github 简介：一款 AI 计算引擎，管理、执行和优化 AI 工作负载的计算需求。它通过单一、灵活的框架统一基础设施，支持从数据处理到模型训练再到模型服务等任何人工智能工作负载。 Polyaxon 官网 Github 简介：一个用于构建、训练和监控大规模深度学习应用程序的平台,用于管理和编排机器学习生命周期的 MLOps 工具 模型服务MLServer 官网 Github 简介：一款 Python 开发的、适用于机器学习模型的推理服务器，旨在提供一种简单的方法来通过 REST 和 gRPC 接口开始为您的机器学习模型提供服务，完全符合KFServing 的 V2 Dataplane规范。 平台级SkyPilot 简介：是一个用于在任何基础设施上运行 AI 和批处理工作负载的框架，提供统一执行、高成本节省和高 GPU 可用性。 文档链接, Github地址 相关博客介绍 Cloudpods 简介：一个使用Golang开发的云原生开源统一多云混合云平台，即Cloudpods是云上的云。 Cloudpods 不仅能够管理本地 KVM裸机，还能够管理来自许多云提供商的许多云帐户的资源。它隐藏了底层云提供商的差异，并公开了一组 API，允许以编程方式与众多云进行交互。 文档链接 BentoML 简介:是一个 Python 库，用于构建针对 AI 应用程序和模型推理进行优化的在线服务系统, 服务 AI 应用程序和模型的最简单方法 - 构建模型推理 API、作业队列、 LLM应用程序、多模型管道等 文档地址、Github地址 其他Cog 简介：Cog 是将机器学习模型打包到容器的工具。可通过配置将机器学习模型所需的环境和依赖，自动打包到容器里方便部署，让你不再为编写 Docker 文件和 CUDA 而痛苦，还能自动启动 HTTP 接口服务方便调用 文档地址、Github地址","tags":["ModelOps"],"categories":["ModelOps"]},{"title":"开发工具之KtConnect的使用","path":"/2025/03/12/tech/k8s/03-ktconnect/","content":"概述 官网链接 KtConnect 是阿里开源的一款云原生协同开发测试解决方案,旨在提升开发者在Kubernetes 场景下的本地开发测试效率,它通过建立本地到K8S集群的双向通道,允许开发者在本地直接访问K8S集群服务,或将K8S集群的流量转发到本地.简言之,KtConnect是面向K8S的本地开发者辅助工具. Win环境 下载链接 1、下载kubectl client 命令行工具 链接 2、配置环境变量并验证kubectl version --client############PS##################### JHC142: C:\\ [ base 3.12.4]❯ kubectl version --clientClient Version: v1.29.2Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3 3、配置K8S集群的config 将k8s集群的config配置文件拷贝到C:\\Users\\xxxx\\.kube 文件夹中 4、验证是否能连接到k8s集群kubectl cluster-info############PS##################### JHC142: C:\\ [ base 3.12.4] ❯ kubectl cluster-infoKubernetes control plane is running at https://172.x.x.x:6443CoreDNS is running at https://172.16.2.129:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxyTo further debug and diagnose cluster problems, use kubectl cluster-info dump. 5、下载ktconnect并配置 下载 kt connect 并将其放到和kubectl client 的同一个 bin目录下 6、验证是否安装成功ktctl -v############PS##################### JHC142: C:\\ [ base 3.12.4] ❯ ktctl -vktctl version 0.3.7 7、启动 使用管理员启动Powershell,并启动kt-connect ktctl connect -n namespace name --excludeIps 172.16.2.0/24 # 网关地址需要更换成为自己的############PS##################### JHC142: C:\\ [ base 3.12.4] ❯ ktctl connect -n bluewhale-platform --excludeIps 172.16.2.0/244:15PM INF Using cluster context kubernetes-admin@kubernetes (kubernetes)4:15PM INF KtConnect 0.3.7 start at 22548 (windows amd64)4:15PM INF Fetching cluster time ...4:15PM INF Using tun2socks mode4:15PM INF Successful create config map kt-connect-shadow-odtnt4:15PM INF Deploying shadow pod kt-connect-shadow-odtnt in namespace bluewhale-platform4:15PM INF Waiting for pod kt-connect-shadow-odtnt ...4:15PM INF Pod kt-connect-shadow-odtnt is ready4:15PM INF Port forward local:10441 - pod kt-connect-shadow-odtnt:22 established4:15PM INF Socks proxy established2024/08/23 16:15:17 Using existing driver 0.142024/08/23 16:15:17 Creating adapter4:15PM INF Tun device KtConnectTunnel is ready2024/08/23 16:15:17 Removed orphaned adapter KtConnectTunnel 14:15PM INF Adding route to 10.96.0.0/164:15PM INF Adding route to 100.90.254.0/244:15PM INF Adding route to 100.82.112.0/244:15PM INF Adding route to 100.85.170.0/244:15PM INF Adding route to 100.103.44.0/244:15PM INF Route to tun device completed4:15PM INF Setting up dns in local mode4:15PM INF Port forward local:45198 - pod kt-connect-shadow-odtnt:53 established4:15PM INF Setup local DNS with upstream [tcp:127.0.0.1:45198 udp:172.16.1.117:53]4:15PM INF Creating udp dns on port 534:15PM INF ---------------------------------------------------------------4:15PM INF All looks good, now you can access to resources in the kubernetes cluster4:15PM INF --------------------------------------------------------------- Mac环境 下载 ktctl MacOS x86 64位 MacOS ARM 64位 安装 ktctl tar zxf ktctl_0.3.7_MacOS_x86_64.tar.gzmv ktctl /usr/local/bin/ktctlktctl --version 安装 kubectl 安装说明 brew install kubectl 将 K8S 集群的 config 拷贝到~/.kube 目录下 启动 sudo ktctl connect -n namespace --excludeIps 172.16.2.0/24","tags":["K8S"],"categories":["云原生"]},{"title":"储存组件Longhorn部署使用","path":"/2025/03/12/tech/k8s/02-longhorn/","content":"概述Longhorn是基于Kubernetes和Container的构建的分布式块存储系统，轻量、可靠且功能强大， Longhorn为每个块设备卷创建一个专门的存储控制器，并且对多个节点上存储副本中的卷进行实时复制。以下是它的主要特点： Longhorn的卷可以作为Kubernetes集群中分布式有状态应用的存储。 可以将块存储划分为Longhorn卷，以便在有或没有云提供商的情况下使用Kubernetes卷。 可以跨节点和数据中心复制块存储，从而提高可用性。 可以在外部存储上存储备份数据，例如：NFSv4或AWS S3。 可以创建跨群集灾难恢复卷，以便从备用的Kubernetes集群的备份中快速恢复来自主Kubernetes群集的数据。 可以为卷设置计划，对卷定期生成快照，并把快照备份到NFS或者兼容S3的备用存储中。 支持从备份中恢复卷。 支持不中断卷服务的情况下进行升级。 项目信息： 官方地址 Github: 当前项目版本V1.7, 比较活跃，Star数：5.9k、Fork数：585 安装部署前提条件 一个正常运行的Kubernetes集群 集群每个节点已经安装下列软件包，并正常启动 open-iscsi（iscsid服务） 安装： yum install iscsi-initiator-utils 环境检测 各存储节点创建目录及挂载 # 将存储盘挂载到/data1所在目录，并创建longhorn目录mkdir -p /home/data1/longhorn 存储节点使用命令打标签：kubectl label nodes node77 node78 node79 node.longhorn.iocreate-default-disktrue 安装longhorn所需系统依赖 yum install -y nfs-utilsyum --setopt=tsflags=noscripts install iscsi-initiator-utilsecho InitiatorName=$(/sbin/iscsi-iname) /etc/iscsi/initiatorname.iscsisystemctl enable iscsidsystemctl start iscsidwget https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 -O /usr/local/bin/jqchmod +x /usr/local/bin/jq 新建longhorn_check_env.sh文件，将下列内容拷贝进入，保存并执行。 #!/bin/bashNVME_CLI_VERSION=1.12####################################################### Log######################################################export RED=\\x1b[0;31mexport GREEN=\\x1b[38;5;22mexport CYAN=\\x1b[36mexport YELLOW=\\x1b[33mexport NO_COLOR=\\x1b[0mif [ -z $LOG_TITLE ]; then LOG_TITLE=fiif [ -z $LOG_LEVEL ]; then LOG_LEVEL=INFOfidebug() if [[ $LOG_LEVEL == DEBUG ]]; then local log_title if [ -n $LOG_TITLE ]; then log_title=($LOG_TITLE) else log_title= fi echo -e $GREEN[DEBUG]$log_title $NO_COLOR$1 fiinfo() if [[ $LOG_LEVEL == DEBUG ]] ||\\ [[ $LOG_LEVEL == INFO ]]; then local log_title if [ -n $LOG_TITLE ]; then log_title=($LOG_TITLE) else log_title= fi echo -e $CYAN[INFO] $log_title $NO_COLOR$1 fiwarn() if [[ $LOG_LEVEL == DEBUG ]] ||\\ [[ $LOG_LEVEL == INFO ]] ||\\ [[ $LOG_LEVEL == WARN ]]; then local log_title if [ -n $LOG_TITLE ]; then log_title=($LOG_TITLE) else log_title= fi echo -e $YELLOW[WARN] $log_title $NO_COLOR$1 fierror() if [[ $LOG_LEVEL == DEBUG ]] ||\\ [[ $LOG_LEVEL == INFO ]] ||\\ [[ $LOG_LEVEL == WARN ]] ||\\ [[ $LOG_LEVEL == ERROR ]]; then local log_title if [ -n $LOG_TITLE ]; then log_title=($LOG_TITLE) else log_title= fi echo -e $RED[ERROR]$log_title $NO_COLOR$1 fi####################################################### Check logics######################################################set_packages_and_check_cmd() case $OS in *debian* | *ubuntu* ) CHECK_CMD=dpkg -l | grep -w PACKAGES=(nfs-common open-iscsi cryptsetup dmsetup) ;; *centos* | *fedora* | *rocky* | *ol* ) CHECK_CMD=rpm -q PACKAGES=(nfs-utils iscsi-initiator-utils cryptsetup device-mapper) ;; *suse* ) CHECK_CMD=rpm -q PACKAGES=(nfs-client open-iscsi cryptsetup device-mapper) ;; *arch* ) CHECK_CMD=pacman -Q PACKAGES=(nfs-utils open-iscsi cryptsetup device-mapper) ;; *gentoo* ) CHECK_CMD=qlist -I PACKAGES=(net-fs/nfs-utils sys-block/open-iscsi sys-fs/cryptsetup sys-fs/lvm2) ;; *) CHECK_CMD= PACKAGES=() warn Stop the environment check because $OS is not supported in the environment check script. exit 1 ;; esacdetect_node_kernel_release() local pod=$1 KERNEL_RELEASE=$(kubectl exec $pod -- nsenter --mount=/proc/1/ns/mnt -- bash -c uname -r) echo $KERNEL_RELEASEdetect_node_os() local pod=$1 OS=$(kubectl exec $pod -- nsenter --mount=/proc/1/ns/mnt -- bash -c grep -E ^ID_LIKE= /etc/os-release | cut -d= -f2) if [[ -z $OS ]]; then OS=$(kubectl exec $pod -- nsenter --mount=/proc/1/ns/mnt -- bash -c grep -E ^ID= /etc/os-release | cut -d= -f2) fi echo $OScheck_local_dependencies() local targets=($@) local all_found=true for ((i=0; i$#targets[@]; i++)); do local target=$targets[$i] if [ $(which $target) = ]; then all_found=false error Not found: $target fi done if [ $all_found = false ]; then msg=Please install missing dependencies: $targets[@]. info $msg exit 2 fi msg=Required dependencies $targets[@] are installed. info $msgcreate_ds() cat EOF $TEMP_DIR/environment_check.yamlapiVersion: apps/v1kind: DaemonSetmetadata: labels: app: longhorn-environment-check name: longhorn-environment-checkspec: selector: matchLabels: app: longhorn-environment-check template: metadata: labels: app: longhorn-environment-check spec: hostPID: true containers: - name: longhorn-environment-check image: alpine:3.12 args: [/bin/sh, -c, sleep 1000000000] volumeMounts: - name: mountpoint mountPath: /tmp/longhorn-environment-check mountPropagation: Bidirectional securityContext: privileged: true volumes: - name: mountpoint hostPath: path: /tmp/longhorn-environment-checkEOF kubectl create -f $TEMP_DIR/environment_check.yaml /dev/nullcleanup() info Cleaning up longhorn-environment-check pods... kubectl delete -f $TEMP_DIR/environment_check.yaml /dev/null rm -rf $TEMP_DIR info Cleanup completed.wait_ds_ready() while true; do local ds=$(kubectl get ds/longhorn-environment-check -o json) local numberReady=$(echo $ds | jq .status.numberReady) local desiredNumberScheduled=$(echo $ds | jq .status.desiredNumberScheduled) if [ $desiredNumberScheduled = $numberReady ] [ $desiredNumberScheduled != 0 ]; then info All longhorn-environment-check pods are ready ($numberReady/$desiredNumberScheduled). return fi info Waiting for longhorn-environment-check pods to become ready ($numberReady/$desiredNumberScheduled)... sleep 3 donecheck_mount_propagation() local allSupported=true local pods=$(kubectl -l app=longhorn-environment-check get po -o json) local ds=$(kubectl get ds/longhorn-environment-check -o json) local desiredNumberScheduled=$(echo $ds | jq .status.desiredNumberScheduled) for ((i=0; idesiredNumberScheduled; i++)); do local pod=$(echo $pods | jq .items[$i]) local nodeName=$(echo $pod | jq -r .spec.nodeName) local mountPropagation=$(echo $pod | jq -r .spec.containers[0].volumeMounts[] | select(.name==mountpoint) | .mountPropagation) if [ $mountPropagation != Bidirectional ]; then allSupported=false error node $nodeName: MountPropagation is disabled fi done if [ $allSupported != true ]; then error MountPropagation is disabled on at least one node. As a result, CSI driver and Base image cannot be supported exit 1 else info MountPropagation is enabled ficheck_hostname_uniqueness() hostnames=$(kubectl get nodes -o jsonpath=.items[*].status.addresses[?(@.type==Hostname)].address) if [ $? -ne 0 ]; then error kubectl get nodes failed - check KUBECONFIG setup exit 1 fi if [[ ! $hostnames[@] ]]; then error kubectl get nodes returned empty list - check KUBECONFIG setup exit 1 fi deduplicate_hostnames=() num_nodes=0 for hostname in $hostnames; do num_nodes=$((num_nodes+1)) if ! echo $deduplicate_hostnames[@] | grep -q \\$hostname\\; then deduplicate_hostnames+=($hostname) fi done if [ $#deduplicate_hostnames[@] != $num_nodes ]; then error Nodes do not have unique hostnames. exit 2 fi info All nodes have unique hostnames.check_nodes() local name=$1 local callback=$2 shift shift info Checking $name... local all_passed=true local pods=$(kubectl get pods -o name -l app=longhorn-environment-check) for pod in $pods; do eval $callback $pod $@ if [ $? -ne 0 ]; then all_passed=false fi done if [ $all_passed = false ]; then return 1 fiverlte() printf %s $1 $2 | sort -C -Vverlt() ! verlte $2 $1kernel_in_range() verlte $2 $1 verlt $1 $3check_kernel_release() local pod=$1 local node=$(kubectl get $pod --no-headers -o=custom-columns=:.spec.nodeName) recommended_kernel_release=5.8 local kernel=$(detect_node_kernel_release $pod) if verlt $kernel $recommended_kernel_release ; then warn Node $node has outdated kernel release: $kernel. Recommending kernel release = $recommended_kernel_release return 1 fi local broken_kernel=(5.15.0-94 6.5.6) local fixed_kernel=(5.15.0-100 6.5.7) for i in $!broken_kernel[@]; do if kernel_in_range $kernel $broken_kernel[$i] $fixed_kernel[$i] ; then warn Node $node has a kernel version $kernel known to have a breakage that affects Longhorn. See description and solution at https://longhorn.io/kb/troubleshooting-rwx-volume-fails-to-attached-caused-by-protocol-not-supported return 1 fi donecheck_iscsid() local pod=$1 kubectl exec $pod -- nsenter --mount=/proc/1/ns/mnt -- bash -c systemctl status --no-pager iscsid.service /dev/null 21 if [ $? -ne 0 ]; then kubectl exec $pod -- nsenter --mount=/proc/1/ns/mnt -- bash -c systemctl status --no-pager iscsid.socket /dev/null 21 if [ $? -ne 0 ]; then node=$(kubectl get $pod --no-headers -o=custom-columns=:.spec.nodeName) error Neither iscsid.service nor iscsid.socket is running on $node return 1 fi ficheck_multipathd() local pod=$1 kubectl exec $pod -- nsenter --mount=/proc/1/ns/mnt -- bash -c systemctl status --no-pager multipathd.service /dev/null 21 if [ $? = 0 ]; then node=$(kubectl get $pod --no-headers -o=custom-columns=:.spec.nodeName) warn multipathd is running on $node known to have a breakage that affects Longhorn. See description and solution at https://longhorn.io/kb/troubleshooting-volume-with-multipath return 1 ficheck_packages() local pod=$1 OS=$(detect_node_os $pod) if [ x$OS = x ]; then error Failed to detect OS on node $node return 1 fi set_packages_and_check_cmd for ((i=0; i$#PACKAGES[@]; i++)); do check_package $PACKAGES[$i] if [ $? -ne 0 ]; then return 1 fi donecheck_package() local package=$1 kubectl exec $pod -- nsenter --mount=/proc/1/ns/mnt -- timeout 30 bash -c $CHECK_CMD $package /dev/null 21 if [ $? -ne 0 ]; then node=$(kubectl get $pod --no-headers -o=custom-columns=:.spec.nodeName) error $package is not found in $node. return 1 ficheck_nfs_client() local pod=$1 local node=$(kubectl get $pod --no-headers -o=custom-columns=:.spec.nodeName) local options=(CONFIG_NFS_V4_2 CONFIG_NFS_V4_1 CONFIG_NFS_V4) local kernel=$(detect_node_kernel_release $pod) if [ x$kernel = x ]; then warn Failed to check NFS client installation, because unable to detect kernel release on node $node return 1 fi for option in $options[@]; do kubectl exec $pod -- nsenter --mount=/proc/1/ns/mnt -- bash -c [ -f /boot/config-$kernel ] /dev/null 21 if [ $? -ne 0 ]; then warn Failed to check $option on node $node, because /boot/config-$kernel does not exist on node $node continue fi check_kernel_module $pod $option nfs if [ $? = 0 ]; then return 0 fi done error NFS clients $options[*] not found. At least one should be enabled return 1check_kernel_module() local pod=$1 local option=$2 local module=$3 local kernel=$(detect_node_kernel_release $pod) if [ x$kernel = x ]; then warn Failed to check kernel config option $option, because unable to detect kernel release on node $node return 1 fi kubectl exec $pod -- nsenter --mount=/proc/1/ns/mnt -- bash -c [ -e /boot/config-$kernel ] /dev/null 21 if [ $? -ne 0 ]; then warn Failed to check kernel config option $option, because /boot/config-$kernel does not exist on node $node return 1 fi value=$(kubectl exec $pod -- nsenter --mount=/proc/1/ns/mnt -- bash -c grep ^$option= /boot/config-$kernel | cut -d= -f2) if [ -z $value ]; then error Failed to find kernel config $option on node $node return 1 elif [ $value = m ]; then kubectl exec $pod -- nsenter --mount=/proc/1/ns/mnt -- bash -c lsmod | grep $module /dev/null 21 if [ $? -ne 0 ]; then node=$(kubectl get $pod --no-headers -o=custom-columns=:.spec.nodeName) error kernel module $module is not enabled on $node return 1 fi elif [ $value = y ]; then return 0 else warn Unknown value for $option: $value return 1 ficheck_hugepage() local pod=$1 local expected_nr_hugepages=$2 nr_hugepages=$(kubectl exec $pod -- nsenter --mount=/proc/1/ns/mnt -- bash -c cat /proc/sys/vm/nr_hugepages) if [ $? -ne 0 ]; then error Failed to check hugepage size on node $node return 1 fi if [ $nr_hugepages -lt $expected_nr_hugepages ]; then error Hugepage size is not enough on node $node. Expected: $expected_nr_hugepages, Actual: $nr_hugepages return 1 fifunction check_sse42_support() local pod=$1 node=$(kubectl get $pod --no-headers -o=custom-columns=:.spec.nodeName) machine=$(kubectl exec $pod -- nsenter --mount=/proc/1/ns/mnt -- bash -c uname -m 2/dev/null) if [ $? -ne 0 ]; then error Failed to check machine on node $node return 1 fi if [ $machine = x86_64 ]; then sse42_support=$(kubectl exec $pod -- nsenter --mount=/proc/1/ns/mnt -- bash -c grep -o sse4_2 /proc/cpuinfo | wc -l 2/dev/null) if [ $? -ne 0 ]; then error Failed to check SSE4.2 instruction set on node $node return 1 fi if [ $sse42_support -ge 1 ]; then return 0 fi error CPU does not support SSE4.2 return 1 else warn Skip SSE4.2 instruction set check on node $node because it is not x86_64 fifunction show_help() cat EOFUsage: $0 [OPTIONS]Options: -s, --enable-spdk Enable checking SPDK prerequisites -p, --expected-nr-hugepages Expected number of 2 MiB hugepages for SPDK. Default: 1024 -h, --help Show this help message and exitEOF exit 0enable_spdk=falseexpected_nr_hugepages=1024while [[ $# -gt 0 ]]; do opt=$1 case $opt in -s|--enable-spdk) enable_spdk=true ;; -p|--expected-nr-hugepages) expected_nr_hugepages=$2 shift ;; -h|--help) show_help ;; *) instance_manager_options+=($1) ;; esac shiftdone####################################################### Main logics######################################################DEPENDENCIES=(kubectl jq mktemp sort printf)check_local_dependencies $DEPENDENCIES[@]# Check the each host has a unique hostname (for RWX volume)check_hostname_uniqueness# Create a daemonset for checking the requirements in each nodeTEMP_DIR=$(mktemp -d)trap cleanup EXITcreate_dswait_ds_readycheck_mount_propagationcheck_nodes kernel release check_kernel_releasecheck_nodes iscsid check_iscsidcheck_nodes multipathd check_multipathdcheck_nodes packages check_packagescheck_nodes nfs client check_nfs_clientif [ $enable_spdk = true ]; then check_nodes x86-64 SSE4.2 instruction set check_sse42_support check_nodes kernel module nvme_tcp check_kernel_module CONFIG_NVME_TCP nvme_tcp check_nodes kernel module uio_pci_generic check_kernel_module CONFIG_UIO_PCI_GENERIC uio_pci_generic check_nodes hugepage check_hugepage $expected_nr_hugepagesfiexit 0 安装当前支持Racher 、Kubectl、Helm安装，本文档采用Helm安装 添加Helm仓库 helm repo add longhorn https://charts.longhorn.iohelm repo update 下载 # 下载包helm fetch longhorn/longhorn# 解压包tar xf longhorn-1.7.0.tgz# 或者去官网下载tgz包# https://github.com/longhorn/longhorn/releases/tag/v1.7.0 修改配置 values.yaml # 看需求设置是否为默认storageclass（defaultClass），修改pvc策略为Retainpersistence: defaultClass: true defaultFsType: ext4 defaultMkfsParams: defaultClassReplicaCount: 3 defaultDataLocality: disabled # best-effort otherwise reclaimPolicy: Retain migratable: false recurringJobSelector: enable: false jobList: [] backingImage: enable: false name: ~ dataSourceType: ~ dataSourceParameters: ~ expectedChecksum: ~ defaultNodeSelector: enable: false # disable by default selector: removeSnapshotsDuringFilesystemTrim: ignored # enabled or disabled otherwise# 设置longhorn挂载的本地存储路径，/data/longhorndefaultSettings: backupTarget: ~ backupTargetCredentialSecret: ~ allowRecurringJobWhileVolumeDetached: ~ createDefaultDiskLabeledNodes: ~ defaultDataPath: /data/longhorn # 修改ui界面的service类型为NodePortservice: ui: type: NodePort nodePort: null manager: type: ClusterIP nodePort: loadBalancerIP: loadBalancerSourceRanges: 部署# 在线部署命令helm install longhorn longhorn/longhorn --namespace longhorn-system --create-namespace -f ./values.yaml# 离线环境安装命令，进入到longhorn的helm目录下helm install longhorn --namespace longhorn-system --create-namespace -f ./ 查看kubectl get all -n longhorn-system###############PS:LOG################################(base) [root@node77 longhorn]# kubectl get all -n longhorn-systemNAME READY STATUS RESTARTS AGEpod/csi-attacher-54946dbcb8-88kmf 1/1 Running 2 (27h ago) 4d1hpod/csi-attacher-54946dbcb8-frz87 1/1 Running 1 4d1hpod/csi-attacher-54946dbcb8-pc2c2 1/1 Running 1 (4d1h ago) 4d1hpod/csi-provisioner-7b64855c94-2ns99 1/1 Running 2 (27h ago) 4d1hpod/csi-provisioner-7b64855c94-dssvx 1/1 Running 2 (4d1h ago) 4d1hpod/csi-provisioner-7b64855c94-vr5ms 1/1 Running 1 (4d1h ago) 4d1hpod/csi-resizer-8b4b94dcd-lg5t4 1/1 Running 1 (4d1h ago) 4d1hpod/csi-resizer-8b4b94dcd-sc6r9 1/1 Running 2 (27h ago) 4d1hpod/csi-resizer-8b4b94dcd-zsd7t 1/1 Running 1 (4d1h ago) 4d1hpod/csi-snapshotter-5847d4c879-mkmsp 1/1 Running 0 4d1hpod/csi-snapshotter-5847d4c879-qxhxc 1/1 Running 1 (4d1h ago) 4d1hpod/csi-snapshotter-5847d4c879-t5nh5 1/1 Running 1 (27h ago) 4d1hpod/engine-image-ei-b0369a5d-g6jnc 1/1 Running 1 (27h ago) 32hpod/engine-image-ei-b0369a5d-n6hjh 1/1 Running 3 (27h ago) 4d1hpod/engine-image-ei-b0369a5d-v5qw7 1/1 Running 1 (27h ago) 4d1hpod/instance-manager-01b254fdf9643d3bc7715d8ad0beeeef 1/1 Running 0 27hpod/instance-manager-0eab76db8dc7fc6ffb02464073dcd3b7 1/1 Running 0 27hpod/instance-manager-65b907de8eac2c4029e4abf9b53b17c4 1/1 Running 0 27hpod/longhorn-csi-plugin-6q74l 3/3 Running 1 (4d1h ago) 4d1hpod/longhorn-csi-plugin-dg825 3/3 Running 1 4d1hpod/longhorn-csi-plugin-qsnzn 3/3 Running 0 32hpod/longhorn-driver-deployer-68cb9bf546-x5ls7 1/1 Running 1 4d1hpod/longhorn-manager-6rtxm 1/1 Running 1 (4d1h ago) 4d1hpod/longhorn-manager-f9dll 1/1 Running 1 4d1hpod/longhorn-manager-trp7k 1/1 Running 0 32hpod/longhorn-ui-5db87b4db5-2h8gk 1/1 Running 0 4d1hpod/longhorn-ui-5db87b4db5-6p9vq 1/1 Running 0 4d1hNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/longhorn-admission-webhook ClusterIP 10.96.0.169 none 9502/TCP 4d1hservice/longhorn-backend ClusterIP 10.96.0.91 none 9500/TCP 4d1hservice/longhorn-conversion-webhook ClusterIP 10.96.0.89 none 9501/TCP 4d1hservice/longhorn-engine-manager ClusterIP None none none 4d1hservice/longhorn-frontend NodePort 10.96.0.249 none 80:32601/TCP 4d1hservice/longhorn-recovery-backend ClusterIP 10.96.1.207 none 9503/TCP 4d1hservice/longhorn-replica-manager ClusterIP None none none 4d1hNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEdaemonset.apps/engine-image-ei-b0369a5d 3 3 3 3 3 none 4d1hdaemonset.apps/longhorn-csi-plugin 3 3 3 3 3 none 4d1hdaemonset.apps/longhorn-manager 3 3 3 3 3 none 4d1hNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/csi-attacher 3/3 3 3 4d1hdeployment.apps/csi-provisioner 3/3 3 3 4d1hdeployment.apps/csi-resizer 3/3 3 3 4d1hdeployment.apps/csi-snapshotter 3/3 3 3 4d1hdeployment.apps/longhorn-driver-deployer 1/1 1 1 4d1hdeployment.apps/longhorn-ui 2/2 2 2 4d1hNAME DESIRED CURRENT READY AGEreplicaset.apps/csi-attacher-54946dbcb8 3 3 3 4d1hreplicaset.apps/csi-provisioner-7b64855c94 3 3 3 4d1hreplicaset.apps/csi-resizer-8b4b94dcd 3 3 3 4d1hreplicaset.apps/csi-snapshotter-5847d4c879 3 3 3 4d1hreplicaset.apps/longhorn-driver-deployer-68cb9bf546 1 1 1 4d1hreplicaset.apps/longhorn-ui-5db87b4db5 2 2 2 4d1hNAME COMPLETIONS DURATION AGEjob.batch/kbench 0/1 4d1h 4d1h 测试安装测试说明基于https://github.com/yasker/kbench项目进行性能测试对于官方基准测试： SIZE环境变量：大小应至少为读写带宽的 25 倍，以避免缓存影响结果。 如果要测试像 Longhorn 这样的分布式存储解决方案，请始终首先针对本地存储进行测试，以了解基线是什么。 如果您使用 Kubernetes 进行测试，则可以为本地存储安装存储提供程序，例如本地路径配置器，用于此测试。 CPU_IDLE_PROF环境变量：CPU 空闲度分析测量 CPU 空闲，但它会带来额外的开销并降低存储性能。默认情况下，该标志处于禁用状态。 参数解析 IOPS：每秒 IO 操作数。越高越好。 它是衡量设备在一秒钟内可以处理多少 IO 操作的度量，主要涉及较小的 IO 块，例如 4k。 带宽：也称为吞吐量。越高越好。 它是设备在一秒钟内可以读取写入多少数据的度量。它主要是处理较大的IO块，例如128k。 延迟：每个请求在 IO 路径中花费的总时间。越_低越好。_ 它是存储系统处理每个请求的效率的度量。 存储系统的数据路径开销可以表示为它在本机存储系统 （SSDNVMe） 之上增加的延迟。 CPU 空闲：运行测试的节点上的 CPU 空闲程度。越高越好。 它是存储设备生成的 CPU 负载开销的度量。 请注意，这是空闲，因此如果该值更高，则意味着该节点上的 CPU 具有更多的空闲周期。 不幸的是，此测量值目前无法反映分布式存储系统整个群集上的负载。但是，在基准测试时，它仍然是存储客户端的CPU负载的值得参考（取决于分布式存储的架构方式）。 对于_比较基准_，该列指示将第二个卷与第一个卷进行比较时的百分比差异。 对于 **IOPS、带宽、CPU 空闲，**正百分比更好。 对于延迟，负百分比更好。 对于 **CPU 空闲，**我们显示的不是变化的百分比，而是差异。 了解分布式存储系统的结果对于分布式存储系统，始终需要先测试本地存储作为基准。_在以下_情况下出现问题： 读取_延迟低于本地存储_。 您可能会获得比本地存储更高的读取 IOPS带宽，因为存储引擎可以聚合来自不同节点磁盘的性能。但是，与本地存储相比，您应该无法获得更低的读取延迟。 如果发生这种情况，很可能是由于存在缓存。增加以避免这种情况。SIZE 写入 IOPS带宽延迟比本地存储更好。 与分布式存储解决方案的本地存储相比，几乎不可能获得更好的写入性能，除非在本地存储前面有一个持久性缓存设备。 如果得到此结果，则存储解决方案可能不是崩溃一致的，因此它不会在响应之前将数据提交到磁盘中，这意味着在发生事件时，您可能会丢失数据。 您将获得延迟_基准的低 CPU 空闲率，例如 40%。 对于延迟，CPU 空闲应至少为 40%，以确保测试不会受到 CPU 不足的影响。 如果发生这种情况，请向节点添加更多 CPU，或移动到增强计算机。 部署 FIO 基准在 Kubernetes 集群中部署单卷基准测试默认情况下： 基准测试将使用默认存储类。 您可以在本地使用 YAML 指定存储类。 将使用文件系统模式。 您可以在本地使用 YAML 切换到阻止模式。 该测试暂时需要 33G PVC。 可以在本地使用 YAML 更改测试大小。 如上所述，对于正式基准测试，大小应至少为读写带宽的 25 倍，以避免缓存影响结果。 测试步骤Longhorn基准测试kubectl apply -f https://raw.githubusercontent.com/yasker/kbench/main/deploy/fio.yaml kind: PersistentVolumeClaimapiVersion: v1metadata: name: kbench-pvcspec: volumeMode: Filesystem accessModes: - ReadWriteOnce resources: requests: storage: 33Gi---apiVersion: batch/v1kind: Jobmetadata: name: kbenchspec: template: metadata: labels: kbench: fio spec: containers: - name: kbench image: yasker/kbench:latest imagePullPolicy: Always env: - name: FILE_NAME value: /volume/test - name: SIZE value: 30G # must be 10% smaller than the PVC size due to filesystem also took space - name: CPU_IDLE_PROF value: disabled # must be enabled or disabled volumeMounts: - name: vol mountPath: /volume/ restartPolicy: Never volumes: - name: vol persistentVolumeClaim: claimName: kbench-pvc backoffLimit: 0 结果展示(base) [root@node77 longhorn]# kubectl logs -f kbench-msz5kTEST_FILE: /volume/testTEST_OUTPUT_PREFIX: test_deviceTEST_SIZE: 30GBenchmarking iops.fio into test_device-iops.jsonfio: pid=0, err=30/file:filesetup.c:224, func=write, error=Read-only file systemfio: io_u error on file /volume/test: Read-only file system: read offset=0, buflen=4096 卸载步骤注意：卸载前最好将所有pvc及pv都逐个删除 删除标志设置：方式一，终端kubectl -n longhorn-system patch -p value: true --type=merge lhs deleting-confirmation-flag 删除标志设置：方式二，界面 勾选Deleting Confirmation Flag选项 删除操作helm uninstall longhorn -n longhorn-system FAQ 安装iscsi-initiator-utils报错 报错信息如下 http://mirrors.bfsu.edu.cn/centos/7.9.2009/updates/x86_64/Packages/iscsi-initiator-utils-iscsiuio-6.2.0.874-22.el7_9.x86_64.rpm: [Errno 14] HTTP Error 404 - Not Found--:-- ETA Trying other mirror.iscsi-initiator-utils-6.2.0.87 FAILED http://mirrors.huaweicloud.com/centos/7.9.2009/updates/x86_64/Packages/iscsi-initiator-utils-6.2.0.874-22.el7_9.x86_64.rpm: [Errno 14] HTTP Error 404 - Not Found --:--:-- ETA Trying other mirror.iscsi-initiator-utils-iscsiuio FAILED http://mirrors.163.com/centos/7.9.2009/updates/x86_64/Packages/iscsi-initiator-utils-iscsiuio-6.2.0.874-22.el7_9.x86_64.rpm: [Errno 14] HTTP Error 404 - Not Found --:--:-- ETA Trying other mirror.Error downloading packages: iscsi-initiator-utils-6.2.0.874-22.el7_9.x86_64: [Errno 256] No more mirrors to try. iscsi-initiator-utils-iscsiuio-6.2.0.874-22.el7_9.x86_64: [Errno 256] No more mirrors to try. 解决方案： 更换yum.repo.dCentos.Base.repo # CentOS-Base.repo## The mirror system uses the connecting IP address of the client and the# update status of each mirror to pick mirrors that are updated to and# geographically close to the client. You should use this for CentOS updates# unless you are manually picking other mirrors.## If the mirrorlist= does not work for you, as a fall back you can try the # remarked out baseurl= line instead.##[base]name=CentOS-$releasever - Base - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos/$releasever/os/$basearch/ http://mirrors.aliyuncs.com/centos/$releasever/os/$basearch/ http://mirrors.cloud.aliyuncs.com/centos/$releasever/os/$basearch/gpgcheck=1gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7#released updates [updates]name=CentOS-$releasever - Updates - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos/$releasever/updates/$basearch/ http://mirrors.aliyuncs.com/centos/$releasever/updates/$basearch/ http://mirrors.cloud.aliyuncs.com/centos/$releasever/updates/$basearch/gpgcheck=1gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7#additional packages that may be useful[extras]name=CentOS-$releasever - Extras - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos/$releasever/extras/$basearch/ http://mirrors.aliyuncs.com/centos/$releasever/extras/$basearch/ http://mirrors.cloud.aliyuncs.com/centos/$releasever/extras/$basearch/gpgcheck=1gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7#additional packages that extend functionality of existing packages[centosplus]name=CentOS-$releasever - Plus - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos/$releasever/centosplus/$basearch/ http://mirrors.aliyuncs.com/centos/$releasever/centosplus/$basearch/ http://mirrors.cloud.aliyuncs.com/centos/$releasever/centosplus/$basearch/gpgcheck=1enabled=0gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7#contrib - packages by Centos Users[contrib]name=CentOS-$releasever - Contrib - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos/$releasever/contrib/$basearch/ http://mirrors.aliyuncs.com/centos/$releasever/contrib/$basearch/ http://mirrors.cloud.aliyuncs.com/centos/$releasever/contrib/$basearch/gpgcheck=1enabled=0gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7 更换后再执行安装命令 sudo yum install iscsi-initiator-utils#################op log#############################Running transaction checkRunning transaction testTransaction test succeededRunning transaction Installing : iscsi-initiator-utils-iscsiuio-6.2.0.874-22.el7_9.x86_64 1/2 Installing : iscsi-initiator-utils-6.2.0.874-22.el7_9.x86_64 2/2 Verifying : iscsi-initiator-utils-6.2.0.874-22.el7_9.x86_64 1/2 Verifying : iscsi-initiator-utils-iscsiuio-6.2.0.874-22.el7_9.x86_64 2/2 Installed: iscsi-initiator-utils.x86_64 0:6.2.0.874-22.el7_9 Dependency Installed: iscsi-initiator-utils-iscsiuio.x86_64 0:6.2.0.874-22.el7_9","tags":["K8S","存储组件"],"categories":["云原生"]},{"title":"包管理工具Helm的使用","path":"/2025/03/11/tech/k8s/01-helm/","content":"Helm概述基本信息 Github地址：Helm 文档地址 当前最新版本v3.15.4，当前Star数26.6k. Chart包托管平台：ArtifactHub Helm是什么？Helm是Kubernetes（K8S）的包管理工具，Helm主要是用来管理Chart包，类似于Python的pip、CentOS的yum、MacOS的brew等。Helm主要有三大基本概念，分别为： Chart: 代表一个Helm包，它包含了在K8S集群中运行的应用程序、工具或服务所需要的所有YAML格式的资源定义文件以及这些资源的配置，可以通过Helm Chart包来整体维护这些资源。 Repository：它是用来存放和共享Helm Chart的地方，类似于存放源码的Github的Repository，以及存放镜像的Docker的Repository。 Release: 它是运行在K8S集群中的Chart的实例。一个Chart通常可以在同一个集群中安装多次。每一次安装都会创建一个新的Release。 Helm也提供了一个helm命令行工具，该工具可以基于Chartb包一键创建应用，在创建应用时，可以自定义Chart配置。应用发布者可以通过Helm打包应用、管理应用依赖管理，管理应用版本、并发布应用到软件仓库。 对于使用者来说，使用Helm后不需要写复杂的应用部署文件，可以非常方便的在K8S上查找、安装、升级、回滚以及卸载应用程序。 Helm架构 Helm Client（helm命令）和Helm Chart包时核心，helm命令可以从Chart Repository中下载Helm Chart包，读取kubeconfig文件，并构建kube-apiserver REST API 接口的HTTP请求，通过调用K8S提供的REST API 接口，将Chart包中包含的YAML格式定义的K8S资源，在K8S集群中创建， 这些资源以Release的形式存在与K8S集群中，每个Release 又包含多个K8S资源，如Deployment、Pod、Service等。 Helm组成 模板文件 【通常有多个】，基于text/template 模板文件，提供了强大的模板渲染能力。Helm可以将配置文件中的值渲染进模板文件中，最终生成一个可以部署的K8S YAML格式的资源定义文件。 配置文件 【通常有一个】 总结： 在Helm中，部署一个应用可以简化为Chart模板（多个服务）+ Chart配置 — 应用。 Chart模板一个应用只用编写一次，可以重复使用，再部署时，可以指定不同的配置，从而将应用部署到不同环境或同一环境部署不同配置的应用。 Helm安装先决条件 一个可用的K8S集群 确定安装版本和安全配置 安装和配置Helm 安装 官方安装指南 Helm 各个版本安装包：Helm Releases 安装步骤$ wget https://get.helm.sh/helm-v3.15.4-linux-amd64.tar.gz$ tar -xvzf helm-v3.15.4-linux-amd64.tar.gz$ mv linux-amd64/helm /usr/local/bin/helm$ chmod +x /usr/local/bin/helm$ helm version # 输出版本号即表示安装成功######log info ########version.BuildInfoVersion:v3.15.4, GitCommit:fa9efb07d9d8debbb4306d72af76a383895aa8c4, GitTreeState:clean, GoVersion:go1.22.6 安装helm命令自动补全脚本（非必须） 文档参考 helm completion bash /etc/bash_completion.d/helm 执行 helm comp，就会自动补全为helm completion。 Helm使用Helm常用命令汇总 命令 描述 completion 生成指定Shell的自动补全脚本，比如bash、zsh等 create 创建一个 Chart 并指定名字 dependency 管理 Chart 依赖 env 打印Helm客户端的环境变量信息，例如HELM_CACHE_HOME、HELM_NAMESPACE、HELM_REPOSITORY_CONFIG等 get 下载一个 Release。可用子命令：all、hooks、manifest、notes、values help 打印helm命令的帮助信息 history 获取 Release历史 install 安装一个 Chart lint 检查一个Chart包，并打印潜在的问题 list 列出 Release package 将 Chart 目录打包到 Chart 存储文件中 plugin Helm插件 pull 从远程仓库中下载 Chart 并解压到本地，例如helm pull stableredis –untar repo 添加（add）、列出（list）、移除（remove）、更新（update）和索引（index）Chart 仓库 rollback 从之前版本回滚 search 根据关键字搜索 Charts，可用子命令：hub、repo show 查看 Chart 详细信息。可用子命令：all、chart、readme、values status 显示已命名版本的状态 template 本地呈现模板 test 在 Release 中运行 Helm 中的预定义测试 uninstall 卸载一个 Release upgrade 更新一个 Release verify 验证给定路径的Chart是否已被签名且有效 version 查看 Helm 客户端版本","tags":["K8S","包管理工具"],"categories":["云原生"]},{"title":"Docker镜像无法拉取问题的解决方案","path":"/2025/03/10/tech/docker/01-docker-image-pull-failed/","content":"自2024年5月底6月初，由于某些原因，突然间在国内就无法正常访问和拉取Docker镜像源，鉴于笔者目前从事云原生ModelOps工作，几乎每天都要和K8S、Docker打交道，镜像无法拉取一直让人很苦恼和无奈，所以在日常工作和探索中，总结汇总了如下可以拉去的官方镜像源的解决方案，由于之前都是零零散散的记录在不同的笔记中，一直想找个时间系统的整理汇总一下，拖了很久，终于在这个周末挤出了一些时间进行了整理汇总，详见下文四个方案，在此，这些方案可能会随着时间的推移无法使用，若是如此请见谅! 方案一：在线下载 渡渡鸟的容器镜像小站：本站作者宣传100%同步官方镜像，目前支持同步镜像源：gcr.io ghcr.io quay.io k8s.gcr.io docker.io registry.k8s.io docker.elastic.co skywalking.docker.scarf.sh 单个镜像大小限制2G，截止目前，全站目前镜像索引数量7516，同步5213GB; 方案二：在拉取镜像加上下列前缀 推荐项目1：https://github.com/DaoCloud/public-image-mirror 推荐项目2：https://github.com/kubesre/docker-registry-mirrors 源站 替换为 cr.l5d.io l5d.kubesre.xyz docker.elastic.co elastic.kubesre.xyz docker.io dhub.kubesre.xyz gcr.io gcr.kubesre.xyz ghcr.io ghcr.kubesre.xyz k8s.gcr.io k8s-gcr.kubesre.xyz registry.k8s.io k8s.kubesre.xyz mcr.microsoft.com mcr.kubesre.xyz nvcr.io nvcr.kubesre.xyz quay.io quay.kubesre.xyz registry.jujucharms.com jujucharms.kubesre.xy # 示例docker pull k8s.kubesre.xyz/ingress-nginx/opentelemetry-1.25.3-v20240813-b93310d 方案三：通过配置Docker加速源在完成安装Docker后，创建或修改/etc/docker/daemon.json 添加加速源 关于加速源可参见文末 【拓展：最新稳定加速元列表】 sudo tee /etc/docker/daemon.json EOF registry-mirrors: [ https://docker.1ms.run, https://hub.rat.dev, https://docker.1panel.live ]EOF 重启Docker服务 sudo systemctl daemon-reload sudo systemctl restart docker 使用docker info 检查是否配置生效，如何输出下列内容则说明配置成功 Registry Mirrors: [...] https://docker.1panel.live 如果当前的docker服务不方便重启，可以使用如下方式进行镜像拉取 # docker pull 加速源/library/mysql:5.7docker pull docker.1panel.live/library/mysql:5.7 方案四： 通过开源项目的 issue 此方案适用于可以访问Docker镜像仓库，但无法访问其他镜像仓库（registry.k8s.io、gcr.io 等）。 https://github.com/myysophia/hub-mirror 使用说明： 在该项目中创建一个新issue，会出现一个模板，将模板内容修改为需要拉去的镜像，提交issue，就会触发github的CI，就会进行拉去拉取镜像，并推送到公开Docker仓库。 hub-mirror: [ 你需要转换的镜像, 你需要转换的镜像, 每次最多 11 个, 改这个 json 就可以了, 别乱改内容, 标题随意，保持阵型是最好的, hub-mirror 标签是必选的, ...... ]####################PS改成下面这个################hub-mirror: [ registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3 ]####################PS改成下面这个################ 然后从docker镜像仓库拉去所需要的镜像 拓展：最新稳定加速源列表 提供者 镜像加速地址 说明 加速类型 耗子面板 https://hub.rat.dev 无限制 Docker Hub rainbond https://docker.rainbond.cc 无限制 Docker Hub 1panel https://docker.1panel.live 无限制 Docker Hub 毫秒镜像 https://docker.1ms.run 大部分镜像都能拉 Docker Hub DaoCloud https://docker.m.daocloud.io 白名单和限流 Docker Hub 阿里云 https://xxx.mirror.aliyuncs.com 需登录分配 镜像太旧 南京大学 https://ghcr.nju.edu.cn 暂无限制 ghcr.io Docker Layer ICU https://docker.cloudlayer.icu 暂无限制 Docker Hub","tags":["Docker"],"categories":["云原生"]},{"title":"子进程管理工具之subprocess","path":"/2025/03/08/tech/python/08-pylib-subprocess/","content":"Reference：官方文档 模块概述介绍：subprocess是Python内置库中用于管理子进程的模块，支持在Windows、Linux、MacOS平台中使用； 该模块通过创建启动一个子进程来执行系统级命令、调用其他的可执行文件或脚本、与其他进程进行交互等等；其主要通过管道技术连接进程间的输入输出错误管道，从而获得返回值。 在subprocess模块发布后，官方建议使用该模块去替换标准库中的os.system()、os.spawn*()等方法去执行系统命令； 版本更新 subprocess是python2.4中新增的模块； 在python3.5之前，可以通过subprocess.call()，subprocess.getoutput()、subprocess.check_output()等等方法实现模块功能； 在python3.5之后，新增subprocess.run()方法，并且官方文档建议通过subprocess.run()或者subprocess.Popen()来替换原有的老版本的功能函数； 应用场景执行系统命令 在程序内部执行系统级命令，如ls、kill、mkdir、awk等，可以使用subprocess实现； 调用其他的可执行文件或脚本 在程序内部执行其他的执行文件或脚本，可以使用subprocess实现； 与其他的进程交互 在需要与其他进程进行交互，比如向某个进程发送数据、从某个进程读取数据等，可以使用subprocess实现； 实现异步任务 在程序内部中需要实现异步任务，可以使用subprocess实现； 替代os.system() 不想使用os.system()或os.system()无法实现需求时，可以subprocess实现； 常用接口重要接口 接口类别 接口方法名 功能描述 返回结果 高阶接口 subprocess.run() 执行命令并等待命令完成，返回一个对象，包含了命令的输出信息 阻塞等待；返回一个对象，包含状态码、输出信息（stdinstdoutstderr） 等层接口 subprocess.Popen() 执行命令不等待完成，返回一个对象 非阻塞等待；返回一个对象，包含状态码、输出信息（stdinstdoutstderr） 低版本接口（基本已被高阶的run()方法替代了） subprocess.call() 执行命令，返回命令的结果和执行状态 阻塞等待；0或者非0 subprocess.check_call() 执行命令，返回结果和状态 阻塞等待；正常为0 ，执行错误则抛出异常 subprocess.getstatusoutput() 接受字符串形式的命令，返回 一个元组形式的结果， 阻塞等待；第一个元素是命令执行状态，第二个为执行结果。 subprocess.getoutput() 接收字符串形式的命令，返回结果 阻塞等待；返回执行结果 subprocess.check_output() 执行命令，如何执行状态码为0则返回命令的结果，否则抛出异常 阻塞等待；0或抛出异常 核心接口分析之subprocess.run()函数特点： 输入：默认情况下，子进程会继承父进程的设置，会将输出显示在终端上 输出：阻塞等待，执行成功returncode为0， 非0，表示执行异常 subprocess.run( args, # 要执行的shell命令，默认应该是一个字符串序列，如[‘df’, ‘-Th’]或(‘df’, ‘-Th’)，也可以是一个字符串，如’df -Th’，但是此时需要把shell参数的值置为True； *, stdin=None, # 子进程的标准输入。有三个参数可选：subprocess.PIPE 创建一个管道，允许与子进程进行通信；subprocess.DEVNULL 特殊的文件对象，可以将其用于丢弃子进程的输出；一个打开的文件对象，将内容写入文件 input=None, # 将参数传递给 Popen.communicate() 以及子进程的 stdin,允许将字节或字符串传递给子进程的标准输入(stdin); stdout=None, # 子进程的标准输出(stdout); stderr=None, # 子进程的标准错误(stderr); capture_output=False, #参数控制是否捕获外部命令的标准输出(stdout)和标准错误(stderr)。如果将其设置为True，run()函数将返回一个CompletedProcess对象，该对象具有stdout和stderr属性，分别存储了命令的标准输出和标准错误输出。如果设置为False，标准输出和标准错误将被发送到控制台。默认为False shell=False, # 指定是否通过shell来执行命令。如果为True，命令将在shell中执行；如果为False，则直接调用可执行文件； cwd=None, # 设置子进程的工作目录。默认为None，表示使用当前工作目录； timeout=None, #设置子进程的超时时间（秒）。如果子进程在指定的时间内没有运行完成，则会引发TimeoutExpired异常； check=False, # 设置是否检查子进程的返回码。如果为True，并且子进程的返回码不为零，则会引发CalledProcessError异常； encoding=None, # 默认是字节数据，如果指定了编码格式，则输出为字符串； errors=None, # 该参数定义在解码输出时如何处理编码错误，常用的值包括strict (默认值，抛出异常)、ignore (忽略错误字符) 和 replace (用替代字符代替错误字符); text=None, # 指定是否将输出结果以文本形式返回。如果为True，则结果以字符串形式返回，同时input或者stdin参数也需要输入String；如果为False，则返回字节流。默认为False。 env=None, # 该参数允许您为子进程指定环境变量。它可以接受一个字典类型的对象，其中键是环境变量的名称，值是环境变量的值。通过设置env参数，可以在子进程中使用特定的环境变量。 universal_newlines=None, # 该参数影响的是输入与输出的数据格式，比如它的值默认为False，此时stdout和stderr的输出是字节序列；当该参数的值设置为True时，stdout和stderr的输出是字符串。 **other_popen_kwargs) 重要参数：args 可以接收两种方法：字符串或列表。 使用列表形式subprocess.run([“ls”, “-al”]) 使用字符串形式 subprocess.run(“ls -al”, shellTrue)。使用字符串形式必须设置参数shellTrue 重要参数：stdin、stdout、sterr 用来设置标准输入，标准输出，标准错误的。默认情况下，子进程会继承父进程的设置，会将输出显示在控制台； subprocess.PIPE 创建一个管道，允许与子进程进行通信； subprocess.DEVNULL 特殊的文件对象，可以将其用于丢弃子进程的输出 一个打开的文件对象，将内容写入文件 使用示例In [1]: import subprocessIn [3]: subprocess.run(ls -al, shell=True)total 16drwxr-xr-x 3 root root 4096 2024-01-12 16:18:55 .drwxr-xr-x. 14 root root 4096 2024-01-12 16:17:37 ..-rw-r--r-- 1 root root 0 2024-01-12 16:18:08 find_big_file.log-rwxr-xr-x 1 root root 1453 2024-01-12 16:18:00 find_big_file.shdrwxr-xr-x 2 root root 4096 2024-01-12 16:18:56 shell_demoOut[3]: CompletedProcess(args=ls -al, returncode=0)In [4]: subprocess.run([ls,-al])total 16drwxr-xr-x 3 root root 4096 2024-01-12 16:18:55 .drwxr-xr-x. 14 root root 4096 2024-01-12 16:17:37 ..-rw-r--r-- 1 root root 0 2024-01-12 16:18:08 find_big_file.log-rwxr-xr-x 1 root root 1453 2024-01-12 16:18:00 find_big_file.shdrwxr-xr-x 2 root root 4096 2024-01-12 16:18:56 shell_demoOut[4]: CompletedProcess(args=[ls, -al], returncode=0)In [5]: subprocess.run([ls, -l, /dev/null], capture_output=True)Out[5]: CompletedProcess(args=[ls, -l, /dev/null], returncode=0, stdout=bcrw-rw-rw- 1 experiment root 1, 3 2023-07-19 15:53:04 /dev/null , stderr=b)In [6]: res = subprocess.run([ls,-al])total 16drwxr-xr-x 3 root root 4096 2024-01-12 16:18:55 .drwxr-xr-x. 14 root root 4096 2024-01-12 16:17:37 ..-rw-r--r-- 1 root root 0 2024-01-12 16:18:08 find_big_file.log-rwxr-xr-x 1 root root 1453 2024-01-12 16:18:00 find_big_file.shdrwxr-xr-x 2 root root 4096 2024-01-12 16:18:56 shell_demoIn [7]: res.returncodeOut[7]: 0In [8]: res = subprocess.run([ls,-al,./cmd_output], stdout=subprocess.PIPE)ls: cannot access ./cmd_output: No such file or directoryIn [9]: res = subprocess.run([ls,-al,./], stdout=subprocess.PIPE)In [10]: res.stdoutOut[10]: btotal 16 drwxr-xr-x 3 root root 4096 2024-01-12 16:18:55 . drwxr-xr-x. 14 root root 4096 2024-01-12 16:17:37 .. -rw-r--r-- 1 root root 0 2024-01-12 16:18:08 find_big_file.log -rwxr-xr-x 1 root root 1453 2024-01-12 16:18:00 find_big_file.sh drwxr-xr-x 2 root root 4096 2024-01-12 16:18:56 shell_demo In [11]: res = subprocess.run([ls,-al,./], stdout=subprocess.PIPE, text=True)In [12]: res.stdoutOut[12]: total 16 drwxr-xr-x 3 root root 4096 2024-01-12 16:18:55 . drwxr-xr-x. 14 root root 4096 2024-01-12 16:17:37 .. -rw-r--r-- 1 root root 0 2024-01-12 16:18:08 find_big_file.log -rwxr-xr-x 1 root root 1453 2024-01-12 16:18:00 find_big_file.sh drwxr-xr-x 2 root root 4096 2024-01-12 16:18:56 shell_demo In [14]: with open(cmd_output.txt, a+) as f: ...: res = subprocess.run([ls,-al,./], stdout=f, text=True) ...: print(fcode:res.returncode) ...: print(fstdout: res.stdout) ...: code:0stdout: NoneIn [15]: lscmd_output.txt find_big_file.log find_big_file.sh* shell_demo/In [16]: res = subprocess.run([cat,cmd_output.txt])total 16drwxr-xr-x 3 root root 4096 2024-01-12 16:27:11 .drwxr-xr-x. 14 root root 4096 2024-01-12 16:17:37 ..-rw-r--r-- 1 root root 0 2024-01-12 16:27:11 cmd_output.txt-rw-r--r-- 1 root root 0 2024-01-12 16:18:08 find_big_file.log-rwxr-xr-x 1 root root 1453 2024-01-12 16:18:00 find_big_file.shdrwxr-xr-x 2 root root 4096 2024-01-12 16:18:56 shell_demototal 20drwxr-xr-x 3 root root 4096 2024-01-12 16:27:11 .drwxr-xr-x. 14 root root 4096 2024-01-12 16:17:37 ..-rw-r--r-- 1 root root 375 2024-01-12 16:27:11 cmd_output.txt-rw-r--r-- 1 root root 0 2024-01-12 16:18:08 find_big_file.log-rwxr-xr-x 1 root root 1453 2024-01-12 16:18:00 find_big_file.shdrwxr-xr-x 2 root root 4096 2024-01-12 16:18:56 shell_demo 核心接口分析之subprocess.Popen()函数特点： run()方法底层调用的是Popen()方法，当run方法无法满足需求是，使用Popen()方法去实现； 输入：执行的命令 输出：非阻塞输出，执行后立即返回，执行结果通过返回对象获取 subprocess.Popen( args, #同run()方法 bufsize=- 1, # 定义子进程的缓冲大小，默认值-1表示使用系统默认的缓冲大小； executable=None, # 指定要执行的程序路径，若未指定，则通过PATH环境变量来确定可执行文件位置； stdin=None, # 同run()方法 stdout=None, # 同run()方法 stderr=None, # 同run()方法 preexec_fn=None, # 指定在子进程启动之前要执行的函数，该函数在fork()调用成功，在exec()调用之前被调用； close_fds=True, # 指定是否关闭所有文件描述符， shell=False, # # 同run()方法 cwd=None, # 同run()方法 env=None, # 同run()方法 universal_newlines=None, # 同run()方法 startupinfo=None, # 一个可选的subprocess.STARTUPINFO对象，用于指定子进程的启动信息，如窗口大小、窗口标题等 creationflags=0, # 用于指定子进程的创建标志，控制子进程的各种行为。可以使用subprocess.CREATE_NEW_CONSOLE、subprocess.CREATE_NEW_PROCESS_GROUP等常量进行设置 restore_signals=True, # 用于确定是否在子进程中恢复信号处理程序的默认行为 start_new_session=False, # 用于代替使用 preexec_fn 的代码来在子进程中调用 os.setsid() 或 os.setpgid()；如果 start_new_session 为真值则 setsid() 系统调用将在执行子进程之前在子进程中执行 pass_fds=(), # 是一个可选的在父子进程间保持打开的文件描述符序列。提供任何 pass_fds 将强制 close_fds 为 True *, group=None, #（仅 POSIX）: 如果 group 不为 None，则 setregid() 系统调用将于子进程执行之前在下级进程中进行。 如果所提供的值为一个字符串，将通过 grp.getgrnam() 来查找它，并将使用 gr_gid 中的值。 如果该值为一个整数，它将被原样传递 extra_groups=None, # （仅 POSIX）: 如果 extra_groups 不为 None，则 setgroups() 系统调用将于子进程之前在下级进程中进行。 在 extra_groups 中提供的字符串将通过 grp.getgrnam() 来查找，并将使用 gr_gid 中的值。 整数值将被原样传递 user=None, # 仅 POSIX）: 如果 user 不为 None，则 setreuid() 系统调用将于子进程执行之前在下级进程中进行。 如果所提供的值为一个字符串，将通过 pwd.getpwnam() 来查找它，并将使用 pw_uid 中的值。 如果该值为一个整数，它将被原样传递 umask=- 1, # 如果 umask 不为负值，则 umask() 系统调用将在子进程执行之前在下级进程中进行 encoding=None, # 同run()方法 errors=None, # 同run()方法 text=None, # 同run()方法 pipesize=- 1, # 当 PIPE 被用作 stdin, stdout 或 stderr 时 pipesize 可被用于改变管道的大小。 管道的大小仅会在受支持的平台上被改变（当撰写本文档时只有 Linux 支持） process_group=None # 用于代替使用 preexec_fn 的代码来在子进程中调用 os.setsid() 或 os.setpgid()；如果 start_new_session 为真值则 setsid() 系统调用将在执行子进程之前在子进程中执行) 重要属性-Popen对象 Popen.communicate(inputNone, timeoutNone) 与进程交互：将数据发送到 stdin。 从 stdout 和 stderr 读取数据，直到抵达文件结尾。 等待进程终止并设置 returncode 属性，可选的 input 参数应为要发送到下级进程的数据，或者如果没有要发送到下级进程的数据则为 None。 如果流是以文本模式打开的，则 input 必须为字符串。 在其他情况下，它必须为字节串。 communicate() 返回一个 (stdout_data, stderr_data) 元组。如果文件以文本模式打开则为字符串；否则字节。 Popen.poll() 检查子进程是否已被终止。设置并返回 returncode 属性。否则返回 None。 Popen.wait(timeoutNone) 等待子进程被终止。设置并返回 returncode 属性。 Popen.send_signal(signal) 将信号 signal 发送给子进程，如SIGINT、SIGTERM等 使用示例 基本操作 In [1]: import subprocessIn [2]: res = subprocess.Popen(sleep 10 pwd, shell=True)In [3]: resOut[3]: Popen: returncode: None args: sleep 10 pwdIn [4]: res.stdoutIn [5]: res.stdoutIn [7]: /data/sswang/subprocess_demoIn [7]: 案例 使用subprocess的Popen来执行shell命令，这种方式会创建一个新的进程来执行shell使用要放在后台一直执行的shell进程，比如通过shell启动某种服务，如果通过上面的方式，就会遇到当python被关闭时，shell启动的程序也被关闭需要注意的是close_fds参数，默认为false，表示继承fb，即文件资源符，如果继承fb，那么当父进程关闭时shell启动的子进程会去继承父进程相应的资源，如Flask web服务中通过shell启动了某个服务，此时Flask关闭了但原本Flask监听的端口依旧会被shell子进程占用，这样就导致Flask无法再次启动。将close_fds设置为false可以避免这种情况def execshell2(self,shell): try: p = subprocess.Popen(shell, shell=True,stdout=subprocess.PIPE ,stderr=subprocess.STDOUT, close_fds=True) output = p.stdout.read() # p.terminate() code = 0 print(%s execute success! % shell) except subprocess.CalledProcessError as e: code = e.returncode output = e.output print(%s execute error: exit_status [%s] err [%s] % (shell, str(code), output)) exit() return code, output subprocess常用异常类SubprocessError 此模块的其他异常 TimeoutExpired SubprocessError 的子类，等待子进程的过程中发生超时时被抛出 CalledProcessError SubprocessError 的子类，当一个由 check_call(), check_output() 或 run() (附带 checkTrue) 运行的进程返回了非零退出状态码时将被引发 实践案例 需求：使用typer+subprocess实现了一个用于管理项目依赖和执行一些常见任务的脚本的工具。 import sysfrom pathlib import Pathimport subprocessimport clickclass UpgradeDependencies: @classmethod def _build_args(cls, dependencies): # 解析依赖项并构建参数 packs, specials = [], for dep in dependencies: if : in dep: k, v = dep.split(:) specials[k.strip()] = v.strip().split() else: packs.append(dep.strip()) return packs, specials @classmethod def _parse_dependencies(cls, text, title): # 从文本中解析依赖项 main, dev = text.split(title) devs = dev.split([tool.)[0].strip().splitlines() mains = main.strip().splitlines() prod_packs, specials = cls._build_args(mains) dev_packs, specials = cls._build_args(devs) return prod_packs, dev_packs, specials @classmethod def get_args(cls): # 获取依赖项参数 dev_title = [tool.poetry.group.dev.dependencies] dev_flag = --group dev main_args, dev_args, others = [], [], [] if dev_title in text: main_title = [tool.poetry.dependencies] else: dev_flag = --group dev dev_title = [tool.poetry.group.dev.dependencies] main_args, dev_args, others = cls._parse_dependencies(text, main_title, dev_title, dev_flag) return main_args, dev_args, others, dev_flag @classmethod def gen_cmd(cls): # 生成升级依赖项的命令 main_args, dev_args, others, dev_flag = cls.get_args() command = fpoetry add .join(main_args) poetry add dev_flag .join(dev_args) for packages in others: command += f poetry add .join(packages) return commanddef exit_if_run_failed(cmd): # 如果运行命令失败，则退出 try: subprocess.run(cmd, check=True, shell=True) except subprocess.CalledProcessError as e: print(fError: e) sys.exit(1)@click.command()def update(): 升级所有依赖包到最新版 exit_if_run_failed(UpgradeDependencies.gen_cmd())@click.command()def lint(): 格式化加静态检查 remove_imports = autoflake --in-place --remove-all-unused-imports cmd = paths = . if args := sys.argv[1:]: if -r in args: args.remove(-r) if all(Path(i).is_file() for i in args): cmd = fremove_imports paths + cmd paths = .join(args) tools = (isort, black, ruff, mypy) cmd += fpoetry run tools[0] paths poetry run tools[1] paths poetry run tools[2] paths poetry run tools[3] paths exit_if_run_failed(cmd)@click.command()def dev(): 启动服务：相当于django的runserver cmd = poetry run python main.py if args := sys.argv[1:]: cmd += + .join(args) exit_if_run_failed(cmd)@click.command()def makemigrations(): 生成数据库迁移文件，类似Django的./manage.py makemigrations exit_if_run_failed(aerich migrate)@click.command()def migrate(): 更新数据库表结构：相当于django的./manage.py migrate exit_if_run_failed(aerich upgrade)if __name__ == __main__: cli() # Assuming you have defined `cli` somewhere","tags":["python库","进程管理"],"categories":["Python"]},{"title":"进程管理工具之Supervisor","path":"/2025/03/07/tech/python/07-pylib-supervisor/","content":"概述： 官方文档：http://supervisord.org/ supervisor是用python开发的一款用于linux操作系统的进程管理工具，supervisor可以高效地实现对进程状态的监控及统一管理 supervisor分为supervisord（服务端）和supervisorctl（客户端）两部分 supervisord作为服务端负责所有的注册进程的启动及监控，出现崩溃的服务时会自动重启 supervisorctl是客户端，可以实现对子进程的管理，如启动、停止、重启等 注意： supervisord 要求管理的程序是非 daemon 程序， 因此用supervisord 来管理进程，进程需要以非daemon的方式启动。 例如：管理nginx，必须在 nginx 的配置文件里添加一行设置 daemon off 让 nginx 以非 daemon 方式启动。 每次修改配置文件后需进入 supervisorctl，执行 reload， 改动部分才能生效。 使用：安装pip install supervisor# 生成配置文件到指定路径（此处路径指定为了/etc/supervisord.conf）echo_supervisord_conf /etc/supervisord.conf 常用命令： supervisord: 服务端命令： # 启动服务supervisord -c /etc/supervisord.conf# 查看是否在运行ps aux | grep supervisord supervisorctl: 客户端命令： supervisorctl status # 查看进程状态supervisorctl start program_name # 启动program_name进程supervisorctl stop program_name # 终止program_name进程supervisorctl restart program_name # 重启program_name进程supervisorctl reread # 更新配置，根据最新的配置启动所有程序supervisorctl update # 更新配置，重启配置有变化的进程 supervisorctl: shell命令： supervisorctl # 进入shell命令行 status # 查看进程状态 start program_name # 启动program_name进程 stop program_name # 终止program_name进程 restart program_name # 重启program_name进程 reread # 更新配置，根据最新的配置启动所有程序 update # 更新配置，重启配置有变化的进程 start fastapi:* # 启动 fastapi组 程序 stop fastapi:* # 停止 fastapi组 程序 快速使用： step1：编辑supervisor配置文件 在配置文件的末尾加上[include]内容 ...[include]files=/etc/supervisor.d/*.conf #若你本地无/etc/supervisor.d目录，请自建... step2：新建etcsupervisor.d.demo.conf 配置文件 [program:自定义的服务名称]command=python3 /xxxx/main.py #服务的启动命令，在这里用的python环境为supervisor安装的环境，若想指定其他python环境则可指定python包的位置如command=xxxx/xxxx/bin/python3 /xxxx/main.py便可process_name=%(program_name)sstdout_logfile=/home/task.log #输出日志文件路径，可根据需要自定义stderr_logfile=/home/task.log #报错日志文件路径，可根据需要自定义 step3:启动 supervisord -c /etc/supervisor.conf 配置文件解读：常用配置解读 打开配置文件： etcsupervisord.conf [unix_http_server]file=/tmp/supervisor.sock ;UNIX socket 文件，supervisorctl 会使用;chmod=0700 ;socket文件的mode，默认是0700;chown=nobody:nogroup ;socket文件的owner，格式：uid:gid[inet_http_server] ;HTTP服务器，提供web管理界面port=0.0.0.0:9001 ;Web管理后台运行的IP和端口，如果开放到公网，需要注意安全性username=user ;登录管理后台的用户名password=123 ;登录管理后台的密码[supervisord]logfile=/tmp/supervisord.log ;日志文件，默认是 $CWD/supervisord.loglogfile_maxbytes=50MB ;日志文件大小，超出会rotate，默认 50MB，如果设成0，表示不限制大小logfile_backups=10 ;日志文件保留备份数量默认10，设为0表示不备份loglevel=info ;日志级别，默认info，其它: debug,warn,tracepidfile=/tmp/supervisord.pid ;pid 文件nodaemon=false ;是否在前台启动，默认是false，即以 daemon 的方式启动minfds=1024 ;可以打开的文件描述符的最小值，默认 1024minprocs=200 ;可以打开的进程数的最小值，默认 200[supervisorctl]serverurl=unix:///tmp/supervisor.sock ;通过UNIX socket连接supervisord，路径与unix_http_server部分的file一致;serverurl=http://127.0.0.1:9001 ; 通过HTTP的方式连接supervisord# 以上东西都不需要改，就加下面这行[include]files=/etc/supervisor.d/*.conf #若你本地无/etc/supervisor.d目录，请自建 详细配置解读：[unix_http_server] file=/tmp/supervisor.sock ; socket文件的路径，supervisorctl用XML_RPC和supervisord通信就是通过它进行 的。如果不设置的话，supervisorctl也就不能用了 不设置的话，默认为none。 非必须设置 ;chmod=0700 ; 这个简单，就是修改上面的那个socket文件的权限为0700 不设置的话，默认为0700。 非必须设置 ;chown=nobody:nogroup ; 这个一样，修改上面的那个socket文件的属组为user.group 不设置的话，默认为启动supervisord进程的用户及属组。非必须设置 ;username=user ; 使用supervisorctl连接的时候，认证的用户 不设置的话，默认为不需要用户。 非必须设置 ;password=123 ; 和上面的用户名对应的密码，可以直接使用明码，也可以使用SHA加密 如：SHA82ab876d1387bfafe46cc1c8a2ef074eae50cb1d 默认不设置。。。非必须设置 ;[inet_http_server] ; 侦听在TCP上的socket，Web Server和远程的supervisorctl都要用到他 不设置的话，默认为不开启。非必须设置 ;port=127.0.0.1:9001 ; 这个是侦听的IP和端口，侦听所有IP用 :9001或*:9001。 这个必须设置，只要上面的[inet_http_server]开启了，就必须设置它 ;username=user ; 这个和上面的uinx_http_server一个样。非必须设置 ;password=123 ; 这个也一个样。非必须设置 [supervisord] ;这个主要是定义supervisord这个服务端进程的一些参数的 这个必须设置，不设置，supervisor就不用干活了 logfile=/tmp/supervisord.log ; 这个是supervisord这个主进程的日志路径，注意和子进程的日志不搭嘎。 默认路径$CWD/supervisord.log，$CWD是当前目录。。非必须设置 logfile_maxbytes=50MB ; 这个是上面那个日志文件的最大的大小，当超过50M的时候，会生成一个新的日 志文件。当设置为0时，表示不限制文件大小 默认值是50M，非必须设置。 logfile_backups=10 ; 日志文件保持的数量，上面的日志文件大于50M时，就会生成一个新文件。文件 数量大于10时，最初的老文件被新文件覆盖，文件数量将保持为10 当设置为0时，表示不限制文件的数量。 默认情况下为10。。。非必须设置 loglevel=info ; 日志级别，有critical, error, warn, info, debug, trace, or blather等 默认为info。。。非必须设置项 pidfile=/tmp/supervisord.pid ; supervisord的pid文件路径。 默认为$CWD/supervisord.pid。。。非必须设置 nodaemon=false ; 如果是true，supervisord进程将在前台运行 默认为false，也就是后台以守护进程运行。。。非必须设置 minfds=1024 ; 这个是最少系统空闲的文件描述符，低于这个值supervisor将不会启动。 系统的文件描述符在这里设置cat /proc/sys/fs/file-max 默认情况下为1024。。。非必须设置 minprocs=200 ; 最小可用的进程描述符，低于这个值supervisor也将不会正常启动。 ulimit -u这个命令，可以查看linux下面用户的最大进程数 默认为200。。。非必须设置 ;umask=022 ; 进程创建文件的掩码 默认为022。。非必须设置项 ;user=chrism ; 这个参数可以设置一个非root用户，当我们以root用户启动supervisord之后。 我这里面设置的这个用户，也可以对supervisord进行管理 默认情况是不设置。。。非必须设置项 ;identifier=supervisor ; 这个参数是supervisord的标识符，主要是给XML_RPC用的。当你有多个 supervisor的时候，而且想调用XML_RPC统一管理，就需要为每个 supervisor设置不同的标识符了 默认是supervisord。。。非必需设置 ;directory=/tmp ; 这个参数是当supervisord作为守护进程运行的时候，设置这个参数的话，启动 supervisord进程之前，会先切换到这个目录 默认不设置。。。非必须设置 ;nocleanup=true ; 这个参数当为false的时候，会在supervisord进程启动的时候，把以前子进程 产生的日志文件(路径为AUTO的情况下)清除掉。有时候咱们想要看历史日志，当 然不想日志被清除了。所以可以设置为true 默认是false，有调试需求的同学可以设置为true。。。非必须设置 ;childlogdir=/tmp ; 当子进程日志路径为AUTO的时候，子进程日志文件的存放路径。 默认路径是这个东西，执行下面的这个命令看看就OK了，处理的东西就默认路径 python -c import tempfile;print tempfile.gettempdir() 非必须设置 ;environment=KEY=value ; 这个是用来设置环境变量的，supervisord在linux中启动默认继承了linux的 环境变量，在这里可以设置supervisord进程特有的其他环境变量。 supervisord启动子进程时，子进程会拷贝父进程的内存空间内容。 所以设置的 这些环境变量也会被子进程继承。 小例子：environment=name=haha,age=hehe 默认为不设置。。。非必须设置 ;strip_ansi=false ; 这个选项如果设置为true，会清除子进程日志中的所有ANSI 序列。什么是ANSI 序列呢？就是我们的 ,\\t这些东西。 默认为false。。。非必须设置 ; the below section must remain in the config file for RPC ; (supervisorctl/web interface) to work, additional interfaces may be ; added by defining them in separate rpcinterface: sections [rpcinterface:supervisor] ;这个选项是给XML_RPC用的，当然你如果想使用supervisord或者web server 这 个选项必须要开启的 supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface [supervisorctl] ;这个主要是针对supervisorctl的一些配置 serverurl=unix:///tmp/supervisor.sock ; 这个是supervisorctl本地连接supervisord的时候，本地UNIX socket 路径，注意这个是和前面的[unix_http_server]对应的 默认值就是unix:///tmp/supervisor.sock。。非必须设置 ;serverurl=http://127.0.0.1:9001 ; 这个是supervisorctl远程连接supervisord的时候，用到的TCP socket路径 注意这个和前面的[inet_http_server]对应 默认就是http://127.0.0.1:9001。。。非必须项 ;username=chris ; 用户名 默认空。。非必须设置 ;password=123 ; 密码 默认空。。非必须设置 ;prompt=mysupervisor ; 输入用户名密码时候的提示符 默认supervisor。。非必须设置 ;history_file=~/.sc_history ; 这个参数和shell中的history类似，我们可以用上下键来查找前面执行过的命令 默认是no file的。。所以我们想要有这种功能，必须指定一个文件。。。非 必须设置 ; The below sample program section shows all possible program subsection values, ; create one or more real program: sections to be able to control them under ; supervisor. ;[program:theprogramname] ;这个就是咱们要管理的子进程了，:后面的是名字，最好别乱写和实际进程 有点关联最好。这样的program我们可以设置一个或多个，一个program就是 要被管理的一个进程 ;command=/bin/cat ; 这个就是我们的要启动进程的命令路径了，可以带参数 例子：/home/test.py -a hehe 有一点需要注意的是，我们的command只能是那种在终端运行的进程，不能是 守护进程。这个想想也知道了，比如说command=service httpd start。 httpd这个进程被linux的service管理了，我们的supervisor再去启动这个命令 这已经不是严格意义的子进程了。 这个是个必须设置的项 ;process_name=%(program_name)s ; 这个是进程名，如果我们下面的numprocs参数为1的话，就不用管这个参数 了，它默认值%(program_name)s也就是上面的那个program冒号后面的名字， 但是如果numprocs为多个的话，那就不能这么干了。想想也知道，不可能每个 进程都用同一个进程名吧。 ;numprocs=1 ; 启动进程的数目。当不为1时，就是进程池的概念，注意process_name的设置 默认为1 。。非必须设置 ;directory=/tmp ; 进程运行前，会前切换到这个目录 默认不设置。。。非必须设置 ;umask=022 ; 进程掩码，默认none，非必须 ;priority=999 ; 子进程启动关闭优先级，优先级低的，最先启动，关闭的时候最后关闭 默认值为999 。。非必须设置 ;autostart=true ; 如果是true的话，子进程将在supervisord启动后被自动启动 默认就是true 。。非必须设置 ;autorestart=unexpected ; 这个是设置子进程挂掉后自动重启的情况，有三个选项，false,unexpected 和true。如果为false的时候，无论什么情况下，都不会被重新启动， 如果为unexpected，只有当进程的退出码不在下面的exitcodes里面定义的退 出码的时候，才会被自动重启。当为true的时候，只要子进程挂掉，将会被无 条件的重启 ;startsecs=1 ; 这个选项是子进程启动多少秒之后，此时状态如果是running，则我们认为启 动成功了 默认值为1 。。非必须设置 ;startretries=3 ; 当进程启动失败后，最大尝试启动的次数。。当超过3次后，supervisor将把 此进程的状态置为FAIL 默认值为3 。。非必须设置 ;exitcodes=0,2 ; 注意和上面的的autorestart=unexpected对应。。exitcodes里面的定义的 退出码是expected的。 ;stopsignal=QUIT ; 进程停止信号，可以为TERM, HUP, INT, QUIT, KILL, USR1, or USR2等信号 默认为TERM 。。当用设定的信号去干掉进程，退出码会被认为是expected 非必须设置 ;stopwaitsecs=10 ; 这个是当我们向子进程发送stopsignal信号后，到系统返回信息 给supervisord，所等待的最大时间。 超过这个时间，supervisord会向该 子进程发送一个强制kill的信号。 默认为10秒。。非必须设置 ;stopasgroup=false ; 这个东西主要用于，supervisord管理的子进程，这个子进程本身还有 子进程。那么我们如果仅仅干掉supervisord的子进程的话，子进程的子进程 有可能会变成孤儿进程。所以咱们可以设置可个选项，把整个该子进程的 整个进程组都干掉。 设置为true的话，一般killasgroup也会被设置为true。 需要注意的是，该选项发送的是stop信号 默认为false。。非必须设置。。 ;killasgroup=false ; 这个和上面的stopasgroup类似，不过发送的是kill信号 ;user=chrism ; 如果supervisord是root启动，我们在这里设置这个非root用户，可以用来 管理该program 默认不设置。。。非必须设置项 ;redirect_stderr=true ; 如果为true，则stderr的日志会被写入stdout日志文件中 默认为false，非必须设置 ;stdout_logfile=/a/path ; 子进程的stdout的日志路径，可以指定路径，AUTO，none等三个选项。 设置为none的话，将没有日志产生。设置为AUTO的话，将随机找一个地方 生成日志文件，而且当supervisord重新启动的时候，以前的日志文件会被 清空。当 redirect_stderr=true的时候，sterr也会写进这个日志文件 ;stdout_logfile_maxbytes=1MB ; 日志文件最大大小，和[supervisord]中定义的一样。默认为50 ;stdout_logfile_backups=10 ; 和[supervisord]定义的一样。默认10 ;stdout_capture_maxbytes=1MB ; 这个东西是设定capture管道的大小，当值不为0的时候，子进程可以从stdout 发送信息，而supervisor可以根据信息，发送相应的event。 默认为0，为0的时候表达关闭管道。。。非必须项 ;stdout_events_enabled=false ; 当设置为ture的时候，当子进程由stdout向文件描述符中写日志的时候，将 触发supervisord发送PROCESS_LOG_STDOUT类型的event 默认为false。。。非必须设置 ;stderr_logfile=/a/path ; 这个东西是设置stderr写的日志路径，当redirect_stderr=true。这个就不用 设置了，设置了也是白搭。因为它会被写入stdout_logfile的同一个文件中 默认为AUTO，也就是随便找个地存，supervisord重启被清空。。非必须设置 ;stderr_logfile_maxbytes=1MB ; 这个出现好几次了，就不重复了 ;stderr_logfile_backups=10 ; 这个也是 ;stderr_capture_maxbytes=1MB ; 这个一样，和stdout_capture一样。 默认为0，关闭状态 ;stderr_events_enabled=false ; 这个也是一样，默认为false ;environment=A=1,B=2 ; 这个是该子进程的环境变量，和别的子进程是不共享的 ;serverurl=AUTO ; ; The below sample eventlistener section shows all possible ; eventlistener subsection values, create one or more real ; eventlistener: sections to be able to handle event notifications ; sent by supervisor. ;[eventlistener:theeventlistenername] ;这个东西其实和program的地位是一样的，也是suopervisor启动的子进 程，不过它干的活是订阅supervisord发送的event。他的名字就叫 listener了。我们可以在listener里面做一系列处理，比如报警等等 楼主这两天干的活，就是弄的这玩意 ;command=/bin/eventlistener ; 这个和上面的program一样，表示listener的可执行文件的路径 ;process_name=%(program_name)s ; 这个也一样，进程名，当下面的numprocs为多个的时候，才需要。否则默认就 OK了 ;numprocs=1 ; 相同的listener启动的个数 ;events=EVENT ; event事件的类型，也就是说，只有写在这个地方的事件类型。才会被发送 ;buffer_size=10 ; 这个是event队列缓存大小，单位不太清楚，楼主猜测应该是个吧。当buffer 超过10的时候，最旧的event将会被清除，并把新的event放进去。 默认值为10。。非必须选项 ;directory=/tmp ; 进程执行前，会切换到这个目录下执行 默认为不切换。。。非必须 ;umask=022 ; 淹没，默认为none，不说了 ;priority=-1 ; 启动优先级，默认-1，也不扯了 ;autostart=true ; 是否随supervisord启动一起启动，默认true ;autorestart=unexpected ; 是否自动重启，和program一个样，分true,false,unexpected等，注意 unexpected和exitcodes的关系 ;startsecs=1 ; 也是一样，进程启动后跑了几秒钟，才被认定为成功启动，默认1 ;startretries=3 ; 失败最大尝试次数，默认3 ;exitcodes=0,2 ; 期望或者说预料中的进程退出码， ;stopsignal=QUIT ; 干掉进程的信号，默认为TERM，比如设置为QUIT，那么如果QUIT来干这个进程 那么会被认为是正常维护，退出码也被认为是expected中的 ;stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10) ;stopasgroup=false ; send stop signal to the UNIX process group (default false) ;killasgroup=false ; SIGKILL the UNIX process group (def false) ;user=chrism ;设置普通用户，可以用来管理该listener进程。 默认为空。。非必须设置 ;redirect_stderr=true ; 为true的话，stderr的log会并入stdout的log里面 默认为false。。。非必须设置 ;stdout_logfile=/a/path ; 这个不说了，好几遍了 ;stdout_logfile_maxbytes=1MB ; 这个也是 ;stdout_logfile_backups=10 ; 这个也是 ;stdout_events_enabled=false ; 这个其实是错的，listener是不能发送event ;stderr_logfile=/a/path ; 这个也是 ;stderr_logfile_maxbytes=1MB ; 这个也是 ;stderr_logfile_backups ; 这个不说了 ;stderr_events_enabled=false ; 这个也是错的，listener不能发送event ;environment=A=1,B=2 ; 这个是该子进程的环境变量 默认为空。。。非必须设置 ;serverurl=AUTO ; override serverurl computation (childutils) ; The below sample group section shows all possible group values, ; create one or more real group: sections to create heterogeneous ; process groups. ;[group:thegroupname] ;这个东西就是给programs分组，划分到组里面的program。我们就不用一个一个去操作了 我们可以对组名进行统一的操作。 注意：program被划分到组里面之后，就相当于原来 的配置从supervisor的配置文件里消失了。。。supervisor只会对组进行管理，而不再 会对组里面的单个program进行管理了 ;programs=progname1,progname2 ; 组成员，用逗号分开 这个是个必须的设置项 ;priority=999 ; 优先级，相对于组和组之间说的 默认999。。非必须选项 ; The [include] section can just contain the files setting. This ; setting can list multiple files (separated by whitespace or ; newlines). It can also contain wildcards. The filenames are ; interpreted as relative to this file. Included files *cannot* ; include files themselves. ;[include] ;这个东西挺有用的，当我们要管理的进程很多的时候，写在一个文件里面 就有点大了。我们可以把配置信息写到多个文件中，然后include过来 ;files = relative/directory/*.ini Supervisor控制台 在etcsupervisord.conf中修改[inet_http_server]的参数，具体如下： [inet_http_server] ; inet (TCP) server disabled by defaultport=*:9001 ; ip_address:port specifier, *:port for all ifaceusername=root ; default is no username (open server)password=xxxx ; default is no password (open server) 修改后重启supervisor进程，在浏览器访问 http://host-ip:9001 分组管理 Supervisor 同时还提供了另外一种进程组的管理方式，通过这种方式，可以使用 supervisorctl 命令来管理一组进程，通过在配置文件中添加如下的配置，将多个服务添加至分组中 [group:group_name]programs=progname1,progname2 注意: 当添加了上述配置后，progname1 和 progname2 的进程名就会变成 group_name:progname1 和 group_name:progname2 以后就要用这个名字来管理进程了，而不是之前的 progname1。 以后执行 supervisorctl stop group_name: 就能同时结束 progname1 和 progname2，执行 supervisorctl stop group_name:progname1 就能结束 progname1 Supervisor配置开机启动方式一： Linux环境中：以systemd的方式管理 编写启动脚本：vim etcrc.dinit.dsupervisord #!/bin/sh## /etc/rc.d/init.d/supervisord## Supervisor is a client/server system that# allows its users to monitor and control a# number of processes on UNIX-like operating# systems.## chkconfig: - 64 36# description: Supervisor Server# processname: supervisord# Source init functions. /etc/rc.d/init.d/functionsprog=supervisordprefix=/usrexec_prefix=$prefixprog_bin=$exec_prefix/bin/supervisordPIDFILE=/var/run/$prog.pidstart()echo -n $Starting $prog: daemon $prog_bin --pidfile $PIDFILE -c /etc/supervisord.conf[ -f $PIDFILE ] success $$prog startup || failure $$prog startupechostop()echo -n $Shutting down $prog: [ -f $PIDFILE ] killproc $prog || success $$prog shutdownechocase $1 instart)start;;stop)stop;;status)status $prog;;restart)stopstart;;*)echo Usage: $0 start|stop|restart|status;;esac 设置开机启动及systemd方式启动。 sudo chmod +x /etc/rc.d/init.d/supervisordsudo chkconfig --add supervisordsudo chkconfig supervisord onsudo service supervisord start 方式二： centos-7 进入 libsystemdsystem 目录，并创建 supervisord.service 文件。 [Unit]Description=supervisordAfter=network.target [Service]Type=forkingExecStart=/usr/bin/supervisord -c /etc/supervisor/supervisord.confExecStop=/usr/bin/supervisorctl $OPTIONS shutdownExecReload=/usr/bin/supervisorctl $OPTIONS reloadKillMode=processRestart=on-failureRestartSec=42s [Install]WantedBy=multi-user.target 加入开机启动 chmod 766 /lib/systemd/system/supervisord.servicesystemctl daemon-reloadsystemctl enable supervisord.servicesystemctl start supervisordsystemctl status supervisord 配置案例:Django的配置文件#项目名[program:django]#脚本目录directory=/root/social_engineering/#脚本执行命令command=/root/anaconda3/bin/python manage.py runserver 0.0.0.0:8000#supervisor启动的时候是否随着同时启动，默认Trueautostart=true#当程序exit的时候，这个program不会自动重启,默认unexpected，设置子进程挂掉后自动重启的情况，有三个选项，false,unexpected和true。如果为false的时候，无论什么情况下，都不会被重新启动，如果为unexpected，只有当进程的退出码不在下面的exitcodes里面定义的autorestart=true#这个选项是子进程启动多少秒之后，此时状态如果是running，则我们认为启动成功了。默认值为1startsecs=1#脚本运行的用户身份 #日志输出 stderr_logfile=/tmp/django_stderr.logstdout_logfile=/tmp/django_stdout.log#把stderr重定向到stdout，默认 falseredirect_stderr = true#stdout日志文件大小，默认 50MBstdout_logfile_maxbytes = 20#stdout日志文件备份数stdout_logfile_backups = 20; 可以通过 environment 来添加需要的环境变量，一种常见的用法是修改 PYTHONPATH; environment=PYTHONPATH=$PYTHONPATH:/path/to/somewhere Celery的配置文件#项目名[program:celery]#脚本目录directory=/root/social_engineering/#脚本执行命令command=/root/anaconda3/bin/celery -A tasks worker --loglevel=info#supervisor启动的时候是否随着同时启动，默认Trueautostart=true#程序崩溃时自动重启autorestart=true#这个选项是子进程启动多少秒之后，此时状态如果是running，则我们认为启动成功了。默认值为1startsecs=1#脚本运行的用户身份 #日志输出 stderr_logfile=/tmp/celery_stderr.logstdout_logfile=/tmp/celery_stdout.log#把stderr重定向到stdout，默认 falseredirect_stderr = true#stdout日志文件大小，默认 50MBstdout_logfile_maxbytes = 20#stdout日志文件备份数stdout_logfile_backups = 20 Redis的配置文件[program:zedis]command=/opt/zedis/redis-server /opt/zedis/redis.confuser=redisuserautostart=true ;启动supervisord的时候会将该配置项设置为true的所有进程自动启动stopasgroup=true ;使用supervisorctl停止zedis时，子进程也会一起停止killasgroup=true ;向进程组发送kill信号，包括子进程startsecs=10 ;进程从STARING状态转换到RUNNING状态所需要保持运行10s时间startretries=3 ;启动失败自动重试次数，默认是3autorestart=true ;进程停止后自动启动stdout_logfile=/var/log/zedis-outstdout_logfile_maxbytes=1MBstdout_logfile_backups=10stderr_logfile=/var/log/zedis-errstderr_logfile_maxbytes=1MBstderr_logfile_backups=10 Ø 因为无法监视后台进程，需要把Zedis前台运行（daemonize no），否则会识别不到Zedis启动，重试启动3次,日志提示6379已占用。 Ø 因为redis.conf指定了logfile，stdout_logfile并无内容输出；如果不指定redis.conf只是启动redis-server，日志会输出到stdout_logfile。 tomcat的配置文件[program:tomcat]command=/usr/local/tomcat/bin/catalina.sh runstdout_logfile=/usr/local/tomcat/logs/catalina.outstderr_logfile=/usr/local/tomcat/logs/catalina.outenvironment=JAVA_HOME=/usr/local/jdk1.8.0_144,JAVA_BIN=/usr/local/jdk1.8.0_144/binautorestart=falsestartsecs=60priority=1stopasgroup=truekillasgroup=true 分组管理的配置文件 新建文件，fastapi_app.conf [group:fastapi]programs=fastapi-app[program:fastapi-1]command=/home/python/scripts/fastapi1_app.shdirectory=/home/python/fastapi-1-backenduser=pythonautorestart=trueredirect_stderr=falseloglevel=infostopsignal=KILLstopasgroup=truekillasgroup=true[program:fastapi-2]command=/home/python/scripts/fastapi-2.shdirectory=/home/python/fastapi-2-backenduser=pythonautorestart=trueredirect_stderr=falseloglevel=infostopsignal=KILLstopasgroup=truekillasgroup=true 常见问题：问题1：unix:tmpsupervisor.sock no such file的问题 在supervisor默认配置中，其启动的sock等都会放到tmp目录，而tmp目录会自动清理导致无法使用supervisorctl vim /etc/supervisord.conf#step1: 将配置文件中的tmp替换掉，否则容易被linux自动清除/tmp/supervisor.sock 改成 /var/run/supervisor.sock，/tmp/supervisord.log 改成 /var/log/supervisor.log，/tmp/supervisord.pid 改成 /var/run/supervisor.pid# step2:修改权限，如果不修改，启动时会报错无权限sudo chmod 777 /runsudo chmod 777 /var/log# step3: 创建supervisor.socksudo touch /var/run/supervisor.socksudo chmod 777 /var/run/supervisor.sock# step4: 杀掉原来的进程，重新启动supervisord -c /etc/supervisord.conf 问题2：command中指定的进程已经起来，但supervisor还不断重启 command中启动方式为后台启动，导致识别不到pid，然后不断重启，这里使用的是elasticsearch，command指定的是$pathbinelasticsearch -d 解决办法：supervisor无法检测后台启动进程的pid，而supervisor本身就是后台启动守护进程，因此不用担心这个 问题3：启动了多个supervisord服务，导致无法正常关闭服务 在运行supervisord -c etcsupervisord.conf之前，直接运行过supervisord -c etcsupervisord.dxx.conf导致有些进程被多个superviord管理，无法正常关闭进程 解决办法：使用ps -fe | grep supervisord查看所有启动过的supervisord服务，kill相关的进程。","tags":["python库","进程管理"],"categories":["Python"]},{"title":"任务管理工具之APScheduler","path":"/2025/03/06/tech/python/06-pylib-apscheduler/","content":"APScheduler之定时任务工具APScheduler概述 APScheduler （advanceded python scheduler）是一款Python开发的定时任务工具。 文档地址 https://apscheduler.readthedocs.io/en/latest/userguide.html#starting-the-scheduler 特点： 不依赖于Linux系统的crontab系统定时，独立运行 可以动态添加新的定时任务，如 下单后30分钟内必须支付，否则取消订单，就可以借助此工具（每下一单就要添加此订单的定时任务） 对添加的定时任务可以做持久保存 1.安装pip install apscheduler 2.使用方式from apscheduler.schedulers.background import BackgroundScheduler# 创建定时任务的调度器对象scheduler = BackgroundScheduler()# 定义定时任务def my_job(param1, param2): pass# 向调度器中添加定时任务scheduler.add_job(my_job, date, args=[100, python])# 启动定时任务调度器工作scheduler.start() 3.调度器Scheduler负责管理定时任务 BlockingScheduler: 作为独立进程时使用 from apscheduler.schedulers.blocking import BlockingSchedulerscheduler = BlockingScheduler()scheduler.start() # 此处程序会发生阻塞 BackgroundScheduler: 在框架程序（如Django、Flask）中使用 from apscheduler.schedulers.background import BackgroundSchedulerscheduler = BackgroundScheduler()scheduler.start() # 此处程序不会发生阻塞 4.执行器 executors在定时任务该执行时，以进程或线程方式执行任务 ThreadPoolExecutor from apscheduler.executors.pool import ThreadPoolExecutorThreadPoolExecutor(max_workers) ThreadPoolExecutor(20) # 最多20个线程同时执行 使用方法 executors = default: ThreadPoolExecutor(20)scheduler = BackgroundScheduler(executors=executors) ProcessPoolExecutor from apscheduler.executors.pool import ProcessPoolExecutorProcessPoolExecutor(max_workers)ProcessPoolExecutor(5) # 最多5个进程同时执行 使用方法 executors = default: ProcessPoolExecutor(3)scheduler = BackgroundScheduler(executors=executors) 5.触发器 Trigger指定定时任务执行的时机 1） date 在特定的时间日期执行 from datetime import date# 在2019年11月6日00:00:00执行sched.add_job(my_job, date, run_date=date(2009, 11, 6))# 在2019年11月6日16:30:05sched.add_job(my_job, date, run_date=datetime(2009, 11, 6, 16, 30, 5))sched.add_job(my_job, date, run_date=2009-11-06 16:30:05)# 立即执行sched.add_job(my_job, date) sched.start() 2） interval 经过指定的时间间隔执行 weeks (int) – number of weeks to wait days (int) – number of days to wait hours (int) – number of hours to wait minutes (int) – number of minutes to wait seconds (int) – number of seconds to wait start_date (datetime|str) – starting point for the interval calculation end_date (datetime|str) – latest possible datetime to trigger on timezone (datetime.tzinfo|str) – time zone to use for the datetime calculations from datetime import datetime# 每两小时执行一次sched.add_job(job_function, interval, hours=2)# 在2010年10月10日09:30:00 到2014年6月15日的时间内，每两小时执行一次sched.add_job(job_function, interval, hours=2, start_date=2010-10-10 09:30:00, end_date=2014-06-15 11:00:00) 3） cron 按指定的周期执行 year (int|str) – 4-digit year month (int|str) – month (1-12) day (int|str) – day of the (1-31) week (int|str) – ISO week (1-53) day_of_week (int|str) – number or name of weekday (0-6 or mon,tue,wed,thu,fri,sat,sun) hour (int|str) – hour (0-23) minute (int|str) – minute (0-59) second (int|str) – second (0-59) start_date (datetime|str) – earliest possible datetime to trigger on (inclusive) end_date (datetime|str) – latest possible datetime to trigger on (inclusive) timezone (datetime.tzinfo|str) – time zone to use for the datetime calculations (defaults to scheduler timezone) # 在6、7、8、11、12月的第三个周五的00:00, 01:00, 02:00和03:00 执行sched.add_job(job_function, cron, month=6-8,11-12, day=3rd fri, hour=0-3)# 在2014年5月30日前的周一到周五的5:30执行sched.add_job(job_function, cron, day_of_week=mon-fri, hour=5, minute=30, end_date=2014-05-30) 6. 配置方法方法1from apscheduler.schedulers.background import BackgroundSchedulerfrom apscheduler.executors.pool import ThreadPoolExecutorexecutors = default: ThreadPoolExecutor(20),scheduler = BackgroundScheduler(executors=executors) 方法2from pytz import utcfrom apscheduler.schedulers.background import BackgroundSchedulerfrom apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStorefrom apscheduler.executors.pool import ProcessPoolExecutorexecutors = default: type: threadpool, max_workers: 20, processpool: ProcessPoolExecutor(max_workers=5)scheduler = BackgroundScheduler()# .. 此处可以编写其他代码# 使用configure方法进行配置scheduler.configure(executors=executors) 7. 启动scheduler.start() 对于BlockingScheduler ，程序会阻塞在这，防止退出 对于BackgroundScheduler，程序会立即返回，后台运行 8.扩展任务管理方式一job = scheduler.add_job(myfunc, interval, minutes=2) # 添加任务job.remove() # 删除任务job.pause() # 暂定任务job.resume() # 恢复任务 方式二scheduler.add_job(myfunc, interval, minutes=2, id=my_job_id) # 添加任务 scheduler.remove_job(my_job_id) # 删除任务scheduler.pause_job(my_job_id) # 暂定任务scheduler.resume_job(my_job_id) # 恢复任务 调整任务调度周期job.modify(max_instances=6, name=Alternate name)scheduler.reschedule_job(my_job_id, trigger=cron, minute=*/5) 停止APScheduler运行scheduler.shutdown()","tags":["python库","任务管理"],"categories":["Python"]},{"title":"优秀的Python库汇总","path":"/2025/03/06/tech/python/05-pylib-awesome/","content":"Web开发库 Django: 一款全面且重量（大而全）的Web框架 FastAPI: 一款高性能、异步的轻量级Web框架， Flask: 一款轻量级用于构建Web应用程序的Python微框架 streamlit: 一款快速搭建数据共享与可视化的Web应用框架 Litestar：一个功能强大、灵活且具有鲜明设计理念的ASGI框架、专注于构建API。它提供高性能数据验证、依赖注入、一流的ORM集成、授权原语、丰富的插件API、中间件以及应用程序启动和运行所需的诸多功能。 django-rest-framework: 一款基于Django，用于快速开发Web API程序的扩展框架 gradio: 一款用于快速构建AI算法可视化部署的框架 Tornado: 一款异步的网络库，可用于快速开发Web应用的框架 sanic: 一款快速构建、快速开发的异步Web框架， Falcon：面向python开发人员的简单Web数据平面API和微服务框架 nicegui: 一款使用python开发的Web用户界面 nameko:一款用于构建微服务的Python框架 cherrypy: 一个轻量级、Pythonic、面向对象的HTTP 框架 Microdot: 适用于Python和Microdot的超小型Web框架 Reflex：一个用于纯Python构建全栈Web应用程序库，使用Python创建高效且可自定义的网页应用程序，几秒钟内即可部署 HTTP相关 Niquests：一个简洁而优雅的HTTP库。它可以完全替代目前功能已经被冻结的Requests库。同时，它也是“最安全、最简单、最先进”的Python HTTP客户端，已经准备好投入生产环境。自动支持 HTTP1.1、HTTP2 和 HTTP3。包含 WebSocket 和 SSE。 Python 机器学习库 lightautoml：一款全自动机器学习框架 Python 数据处理库 voluptuous: 一个 Python 数据验证库 ORM 库 sqlalchemy: Python 数据库工具包 databases: Python的异步数据库支持 django-orm: peewee tortoise-orm alembic: 数据库迁移工具 环境包管理库 uv: 一个用Rust编写的、速度极快的Python包和项目管理器。 conda: 包管理工具 poetry: 依赖管理工具 pdm: 依赖管理工具 pipx: 管理Python包的工具,在隔离环境中安装和运行 Python 应用程序 pipenv: 管理Python包的工具 virtualenv: 虚拟环境管理工具 venv: 虚拟环境管理工具 virtualenvwrapper: 虚拟环境管理工具 pyenv: Python版本管理工具 任务库 celery: 分布式任务调度框架 apscheduler: 定时任务调度框架 funboost: 异步任务调度框架 rq: 轻量级任务调度框架 sched: 轻量级任务调度框架 schedule: 轻量级任务调度框架 huey：一个用于Python的小型任务队列，支持Redis、SQLite、文件系统或内存存储。 性能分析相关 Pympler：一款开发工具，用于测量、监控和分析正在运行的 Python 应用程序中 Python 对象的内存行为。 cProfile: 标准库自带的性能分析 memory_profiler: 内存分析工具，监控Python代码的内存使用情况 line_profiler: 行级性能分析工具 代码编译加密相关 Nuitka: 代码加密工具,一个用 Python 编写的 Python 编译器 pyarmor: 代码加密工具 py2sec: 一个跨平台、快速且灵活的工具，可将 .py 更改为 .so（Linux 和 Mac）或 .pyd（Win） pyinstaller: 代码打包工具,将 Python 程序冻结（打包）为独立的可执行文件 py2dist: 讲Python项目编译成二进制文件，以便分发。 测试库 pytest: 测试框架 unittest: 单元测试框架 pytest-html: 测试报告生成工具 mock: 模拟对象库 依赖注入框架 dependency-injector: 依赖注入框架 bevy: 是一个 Python 依赖注入框架！它的主要目标是帮助您更轻松地编写出色的代码。 重试库： retry: 重试库 进程管理库 supervisor: 进程管理工具 gunicorn: 进程管理工具 uwsgi: 进程管理工具 操作日期时间库 time: 官方内置库 datetime: 官方内置库 arrow: 日期时间库 dateutil arrow delorean Freezegun moment maya 文件处理相关库操作Json文件 Json:官方内置库 【☆】Jmespath: 是一个json查询库，可以使用声明的方式从Json中提取元素 操作YAML文件 PyYAML 操作 PDF 文件 pdf-craft: 可以将 PDF 文件转化为各种其他格式(Markdown、EPUB等)。该项目将专注于扫描书籍的 PDF 文件的处理 未分类 Faker: 一个用于生成虚拟数据的python库 数据可视化-BI工具 DataEase：一款开源的BI工具。帮助用户快速分析数据并洞察业务趋势，从而实现业务的改进和优化。支持丰富的数据源连接，能够通过拖拉拽方式快速制作图表，并可以方便的与他人分享。 爬虫相关 Scrapling：一个难以察觉、功能强大、灵活且高性能的Python库，让网页抓取变得轻松自如，旨在应对反爬虫保护和网页结构变化的挑战。","tags":["python库"],"categories":["Python"]},{"title":"常用的Pypi源汇总","path":"/2025/03/04/tech/python/03-py-pypi-source/","content":"常用国外源 官方源 常用国内源 清华源 # 清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/ # 阿里云 http://mirrors.aliyun.com/pypi/simple/ # 中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/ # 豆瓣(douban) http://pypi.douban.com/simple/ pip 国内源配置 https://mirrors.tuna.tsinghua.edu.cn/help/pypi/ 临时指定国内源安装 在python3.6后，官方推荐使用 python -m pip的方式进行安装相关包x python -m pip install -i https://mirrors.aliyun.com/pypi/simple pkgname# pip install -i https://mirrors.aliyun.com/pypi/simple pkgnamepython -m pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pkgnamepython -m pip install -i https://mirrors.ustc.edu.cn/pypi/web/simple pkgname 永久设置国内源安装 下列以清华源设置为例，其他源设置类似 mkdir -p ~/.pip/tee ~/.pip/pip.conf EOF源名称下面的配置EOF----# 清华源[global]timeout = 6000 index-url = https://pypi.tuna.tsinghua.edu.cn/simple[install] trusted-host = pypi.tuna.tsinghua.edu.cn 命令行设置默认源pip install pip -U -i https://pypi.tuna.tsinghua.edu.cn/simplepip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple conda 国内源配置# 中科大源conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/# 阿里源conda config --add channels https://mirrors.aliyun.com/pypi/simple/# 豆瓣源conda config --add channels http://pypi.douban.com/simple/# 其他配置## 显示检索路径，每次安装包时会将包源路径显示出来conda config --set show_channel_urls yesconda config --set always_yes True## 显示所有镜像通道路径命令conda config --show channels# 恢复默认源conda config --remove-key channels PDM 国内源配置通过如下命令设置默认镜像： pdm config pypi.url https://pypi.tuna.tsinghua.edu.cn/simple Poetry 国内源配置通过以下命令设置首选镜像： poetry source add --priority=primary mirrors https://pypi.tuna.tsinghua.edu.cn/simple/ 通过以下命令设置补充镜像： poetry source add --priority=supplemental mirrors https://pypi.tuna.tsinghua.edu.cn/simple/","tags":["python工程实践","pypi"],"categories":["Python"]},{"title":"Pythoner的优雅编码之道","path":"/2025/03/03/tech/python/250903-py-coding-statndards/","content":"在编程的世界里，优雅的代码，从来都不是为了取悦计算机，而是为了善待未来的自己、同事，甚至那个素未谋面却要维护你代码的陌生人。 我们写的代码，终将被阅读，所以我认为好的代码自己会说话，而我们要做的就是让它说的更清楚，更有逻辑。 命名艺术：让代码自解释在主流的编程语言中，变量命名风格主要分为驼峰命名派（CameCase）和蛇形命名派（snake_case），其中驼峰又分为大驼峰（CameCase）和小驼峰（cameCase）。在Python中，大驼峰和蛇形命名两种风格在不同场景中应用的都比较频繁，大驼峰命名主要应用在类名的定义上，其他场景绝大部分都是使用蛇形命名。另一方面 Python 社区普遍遵循的是 PEP 8 规范，强调一致性与可读性。 命名规范速查 类型 推荐命名方式 示例 说明 模块 lower_with_under utils.py, _private.py 简短小写，避免下划线开头 包 lowercase mypackage 不建议使用下划线 类 CapWords（大驼峰） DatabaseConnection, User 异常类应以 Error 结尾 函数方法 lower_with_under() get_user_by_id() 动词开头，描述行为 常量 CAPS_WITH_UNDER MAX_RETRIES, API_TIMEOUT 全大写，表示不可变值 布尔变量 is_xxx, has_xxx 等 is_connected, has_data 明确表达真假状态 私有成员 __private_attr __password, __init_db() 双下划线触发名称重整 命名编写原则 遵循PEP 8原则（Python官方制定的编码风格指南和建议）； 命名需要做到名如其意，不要吝啬名字的长度，但除了一些特殊场景，变量名一般不建议超过4个单词; 变量的描述性要强，比如：冬天的梅花比花的描述性更强； 当命名与关键字冲突时，在变量末尾加下划线，比如：class_; 优秀命名实践 增强描述性 # (bad) 描述性弱的名字，过于抽象，看不懂在做什么value = process(s.strip())data = get_info()# (good) 描述性强的名字，尝试从用户输入里解析一个用户名username = extract_username(input_string.strip()) 避免歧义 # 容易混淆l = [] # 看起来像数字1O = 0 # 看起来像数字0# 清晰明确users_list = []default_value = 0 类型暗示 # 集合类型user_ids_set = 1, 2, 3configuration_dict = timeout: 30# 布尔类型should_retry = Trueis_connected = Falsehas_permission = True# 数值类型port、age # 表示数字的单词user_id, host_id # 使用_id结尾max_length、users_count # 使用length/count相关词retry_count = 3total_amount = 100.50 注释之道：解释「为什么」而非「是什么」 好的注释应该解释设计意图，而不是重复代码内容。 Python中的注释主要分为以#开头的单行注释、和以三连字符串（...）表示的多行注释或文档注释。 注释编写原则 代码块注释，在代码块上一行的相同缩进处以 # 开始书写注释；代码块注释最需要写注释的地方是代码中那些技巧性的部分；对于复杂的操作，应该在其操作开始之前写上若干行注释，对于不是一目了然的代码，应该在其行尾添加注释； 代码行注释，在代码行的尾部跟2个空格，然后以 # 开始书写注释，行注释尽量少写； TODO注释应该在所有开头处包含“TODO”字符串，紧跟着用括号括起来你的名字，email地址或其他标识符，然后是一个可选的冒号； 对于TODO注释的目的是用来表示“将来做某件事”，建议添加指定的日期； 英文注释开头要大写，结尾要写标点符号，避免语法错误和逻辑错误，中文注释也是相同要求； 改动代码逻辑时，一定要及时更新相关的注释； 注释实践1. 文档注释（Docstrings）： class DatabaseConnection: 数据库连接管理类 提供连接池管理、自动重连和查询重试功能 支持MySQL和PostgreSQL数据库 Attributes: pool_size: 连接池大小 timeout: 查询超时时间（秒） max_retries: 最大重试次数 def __init__(self, connection_string: str, pool_size: int = 10): 初始化数据库连接 Args: connection_string: 数据库连接字符串 pool_size: 连接池大小，默认为10 Raises: ConnectionError: 当连接数据库失败时 self.connection_string = connection_string self.pool_size = pool_size self._initialize_pool() def execute_query(self, query: str, parameters: dict = None) - list: 执行SQL查询 Args: query: SQL查询语句 parameters: 查询参数字典 Returns: 查询结果列表 Raises: QueryTimeoutError: 查询超时时 DatabaseError: 数据库错误时 # 方法实现 pass 2. 为什么注释（Why Comments）： # 使用快速排序而不是内置sort，因为需要稳定排序# 并且在处理大量数据时性能更好quicksort(data) 3. 警示注释： # 注意：这个函数修改传入的列表，而不是返回新列表# 使用前请确保不需要原始数据def process_in_place(items): # ... 实现细节 4. 代码逻辑注释 def merge_sorted_arrays(arr1: list, arr2: list) - list: 合并两个已排序数组 使用双指针法实现O(n)时间复杂度合并 适用于大规模数据合并场景 result = [] i = j = 0 # 双指针遍历，选择较小元素加入结果 while i len(arr1) and j len(arr2): if arr1[i] arr2[j]: result.append(arr1[i]) i += 1 else: result.append(arr2[j]) j += 1 # 将剩余元素添加到结果中 result.extend(arr1[i:]) result.extend(arr2[j:]) return result TODO注释 # TODO(ssw@gmail.com): Use a * here for string repetition.# TODO(Luke) Change this to use relations. 改良一些不好的注释习惯 # 1. 过度注释# 不好：注释重复代码内容x = x + 1 # 给x加1# 好：注释解释为什么这样做x = x + 1 # 补偿数组索引从0开始的问题# 2.魔法数字# 不好：直接使用魔法数字if len(users) 100: # ...# 好：使用有意义的常量MAX_USERS_THRESHOLD = 100if len(users) MAX_USERS_THRESHOLD: # ... # 3. 过长的函数名# 不好：过于冗长def get_user_information_from_database_by_user_id(): # ...# 好：简洁明确def get_user_by_id(user_id): # ... 导包规范：整洁的代码门面导入语句应该按照标准库 → 第三方库 → 本地应用的顺序分组排列： 导包编写原则from __future__ import annotations# 1. 标准库导入import osimport sysfrom pathlib import Pathfrom typing import List, Dict, Optional# 2. 第三方库导入import requestsfrom django.conf import settingsimport pandas as pd# 3. 本地应用导入from .models import User, Profilefrom .utils.helpers import logger, configfrom .exceptions import CustomError# 模块级常量DEFAULT_TIMEOUT = 30MAX_RETRIES = 3 优雅导包实践# 每行一个导入import jsonimport time# 使用括号的多行导入from collections import ( defaultdict, OrderedDict, namedtuple)# 使用别名简化长模块名import matplotlib.pyplot as pltimport numpy as np 格式之美：缩进、空行与空格Python的强制缩进本身就是一个约束美，但我们仍需注意细节，让代码看着更舒服。 缩进与换行 使用 4 个空格 缩进（不混用 Tab） 单行不超过 120 字符 超长语句可用括号隐式续行： result = ( process_data( input_data, transform_fn, validate=True )) 空行规范 文件级函数类之间：两个空行 类内方法之间：一个空行 函数内部逻辑块之间：一个空行 文件末尾：保留一个空行 空格规范 场景 示例 操作符两边 a + b, x == y 逗号后 [1, 2, 3], func(a, b) 冒号后（字典） name: Alice 注释符号后 # 这是一个注释 括号内不加空格 (1, 2), [x for x in data] 自动化: 让工具守护代码规范手动遵循规范太累？那就让工具替我们守规。 常用工具链 Ruff: 极速的Python linter和格式化工具 Black: 无妥协的代码格式化器 isort: 自动规范import语句顺序 mypy: 静态类型检查 3. 自动化工具配置pyproject.toml 配置示例[tool.ruff]line-length = 120select = [E, F, W, I, B, C, N, A, S, T, Q]ignore = [E501] # 忽略行长警告（由 black 处理）target-version = py312[tool.black]line-length = 120target-version = [py312]skip-string-normalization = true[tool.isort]profile = blackline_length = 120known_first_party = [myapp] 预提交钩子配置 安装后运行 pre-commit install，每次提交自动格式化 + 检查。 # .pre-commit-config.yamlrepos: - repo: https://github.com/astral-sh/ruff-pre-commit rev: v0.1.0 hooks: - id: ruff args: [--fix, --show-fixes] - repo: https://github.com/psf/black rev: 23.10.0 hooks: - id: black - repo: https://github.com/pre-commit/mirrors-mypy rev: v1.6.0 hooks: - id: mypy 写在最后优秀的代码风格，不是限制，而是解放，它解放了我们的注意力，让我们不再纠结“这变量是啥”，不再困惑“这段代码想干啥”，让我们能够专注于解决真正重要的事：解决问题，创造价值。","tags":["python工程实践"],"categories":["Python"]},{"title":"一切都是新的开始","path":"/2025/03/02/thinking/01-letsgo/","content":"过去日子走走停停，零零散散，往后的岁月，从此刻开始，重新出发… 关于过往我也曾站在人潮拥挤的大街愣神，任由来往行人推推嚷嚷，扪心自问我想要过的生活究竟会是怎样，我又该以何种努力去换来更好的人生… 当一天过去、一个月过去、一年过去，当我和所有的人一样，在时光的催促下，又老去了一岁，我看到了人生教会我的那些安静却真实的道理… 关于生活一个人想要幸福，就必须先放下对幸福的执念… 没有一个人是完全幸福的，所谓幸福，是在于认清一个人的限度而安于这个限度… 关于代码我有一个梦想，我写的代码，可以像诗歌一样优美。我有一个梦想，我做的设计，能恰到好处，既不过度，也无不足。这种带有一点洁癖的完美主义就像一把达摩克利斯之剑，时刻提醒我，不能将就、不能妥协。 —摘自《代码精进之路：从码农到工匠》","categories":["时笺"]},{"title":"pyflink之分布式计算","path":"/2024/03/28/tech/python/240328-pyflink-usage/","content":"此文内容基于apache-flink-1.17.1版本，如有偏差详见官方文档 文档说明官方文档： Flink官方文档 Python API 文档 开发文档： pyflink官方文档 部署文档： 命令行部署 安装pyflink 环境准备： Java环境：Java8+ python环境： python3.7+， conda(miniconda)或其他, 基础环境安装： java环境安装步骤略 conda环境安装步骤略 pyflink安装 # 创建虚拟环境 conda create -n pyflink-py38env python=3.8 -y ​ conda activate pyflink-py38env ​ pip install apache-flink==1.17.1 -i https://pypi.tuna.tsinghua.edu.cn/simple 环境验证 conda activate pyflink-py38env ​ cd -/pyflink-py38env/lib/python3.8/site-packages/pyflink/ ​ python examples/table/word_count.py -----------log---------------- (pyflink-py38env) [root@node80 pyflink]# python examples/table/word_count.py Using Any for unsupported type: typing.Sequence[~T] No module named google.cloud.bigquery_storage_v1. As a result, the ReadFromBigQuery transform *CANNOT* be used with `method=DIRECT_READ`. Executing word_count example with default input data set. Use --input to specify file input. Printing result to stdout. Use --output to specify output path. +I[To, 1] +I[be,, 1] +I[or, 1] +I[not, 1] +I[to, 1] +I[be,--that, 1] +I[is, 1] +I[the, 1] +I[question:--, 1] +I[Whether, 1] ... # pyflink安装成功 重要提示： 如果需要将pyflink程序提交到远程的YARN集群，需要从集群拷贝如下配置文件，并到到该虚拟环境的pyflinkconf目录下 core-site.xml hdfs-site.xml flink-conf.yaml 开发pyflink程序Kafka源的使用 官方使用说明 第三方依赖的使用在使用pyflink进行程序开发时，用户在开发UDF算子时常常会引入第三方依赖库,此时对依赖包引入和运行的管理就会成为刚需；当在本地执行pyflink时，用户可以将第三方python库下载安装到本地后在进行执行，但当pyflink程序需要提交到远程执行时，此方法就行不通。于是pyflink官方提供了对依赖包的管理方法，其中在DataStream API 和 Table API实现方式是不同的。 目前官方提供了对Jar依赖和Python依赖管理的说明和示例，详见官方链接， 需要注意是，在pyflink代码中添加依赖时，目前只支持本地文件的引入（即：file:)，不支持远程（如 hdfs:）方式的引入。 提交pyflink程序（部署）提交模式概述当通过flink run 提交pyflink程序时，flink将执行python命令，解析编译pyflink程序，生成一个jar程序，此过程也被称为JobGraph对象生成，其本质是python vm 与java vm 通过RPC的方式进行通信，当然，不同的提交模式，其实现逻辑会不同；目前，提交模式分为以下几种： Session模式 Standalone模式 Application模式（此模式目前支持YARN 或 K8S，也是官方推荐的模式） Per-job 模式（在1.17版本中已被弃用） 提交参数说明 官方参数使用说明 选项 描述 -py、–python 带有程序入口点的 Python 脚本。可以用 –pyFiles 选项配置依赖资源。 -pym、–pyModule 带有程序入口点的 Python 模块。该选项必须与 –pyFiles 一起使用。 -pyfs、–pyFiles 为作业附加自定义文件。支持 .py.egg.zip.whl 之类的标准资源文件后缀或目录。这些文件将被添加到本地客户端和远程 Python UDF Worker 的 PYTHONPATH 中。以 .zip 为后缀的文件将被解压缩，并且被添加到 PYTHONPATH。逗号（“,”）可以用作指定多个文件的分隔符（比如，–pyFiles file:tmpmyresource.zip,hdfs:$namenode_addressmyresource2.zip）。 -pyarch、–pyArchives 为作业添加 Python 归档文件。归档文件将被解压缩到 Python UDF Worker 的工作目录。可以为每个归档文件指定目标目录。如果指定目标目录名称，那么归档文件将被解压缩到具有指定名称的目录。否则，归档文件将被解压缩到与归档文件同名的目录中。通过该选项上传的文件可以通过相对路径访问。可以使用 “#” 作为归档文件路径和目标目录名称的分隔符。可以使用 “,” 作为指定多个归档文件的分隔符。可以使用该选项上传 Python UDF 中使用的虚拟环境和数据文件（比如，–pyArchives file:tmppy37.zip,file:tmpdata.zip#data –pyExecutable py37.zippy37binpython）。在 Python UDF 中可以访问数据文件，比如：f open(‘datadata.txt’, ‘r’)。 -pyclientexec、–pyClientExecutable 当通过 “flink run” 提交 Python 作业或编译包含 Python UDF 的 JavaScala 作业时，用于发起 Python 进程的 Python 解释器的路径（比如，–pyArchives file:tmppy37.zip –pyClientExecutable py37.zippy37python）。 -pyexec、–pyExecutable 指定用于执行 Python UDF Worker 的 Python 解释器的路径（比如，–pyExecutable usrlocalbinpython3）。Python UDF Worker 依赖 Python 3.7+、Apache Beam（版本 2.43.0）、Pip（版本 20.3）和 SetupTools（版本 37.0.0）。请确保指定的环境满足上述要求。 -pyreq、–pyRequirements 指定定义第三方依赖的 requirements.txt 文件。这些依赖将被安装，并且被添加到 Python UDF Worker 的 PYTHONPATH。可选地指定包含这些依赖的安装包的目录。如果可选参数存在，那么使用“#”作为分隔符（–pyRequirements file:tmprequirements.txt#file:tmpcached_dir）。 Note: 通过 -pyarch 指定的归档文件将通过 Blob 服务被分发到 TaskManager，文件大小限制是 2GB。如果归档文件的大小超过 2GB，那么可以将它上传到分布式文件系统，然后在命令行选项 -pyarch 中使用路径 打包pyflink运行环境 # 找到 minconda(安装路径 envs目录下) 或者对应虚拟环境安装目录# 打包 pyflink-py38env 虚拟环境cd -/conda/envs/pyflink-py38envzip -r pyflink-py38env.zip pyflink-py38env Note： 需要注意打包后的zip是单层（pyflink-py38env.zipbinpython）的还是双层的(pyflink-py38env.zippyflink-py38envbinpython) 本地运行 无依赖的运行 ./bin/flink run --python examples/python/table/word_count.py # pyflink/bin/flink ``` - 引入外部资源时运行```bash./bin/flink run \\ --python examples/python/table/word_count.py \\ --pyFiles file:///user.txt,hdfs:///$namenode_address/username.txt 运行引用 Java UDF 或外部连接器的 PyFlink 作业。在 –jarfile 指定的文件将被上传到集群。 ./bin/flink run \\ --python examples/python/table/word_count.py \\ --jarfile jarFile 提交到JobManager Tips： 如果提交的pyflink程序及其依赖在本地：路径参数采用：file: 如果提交的pyflink程序在文件系统（如hdfs）,路径参数采用：hdfs: 单文件提交 ./flink run \\ --jobmanager localhost:8081 \\ -pyarch file:///path/pyflink-py38env.zip \\ -pyexec pyflink-py38env.zip/bin/python3 \\ -pyclientexec pyflink-py38env.zip/bin/python3 \\ -py /pyflink-py38env/lib/python3.8/site-packages/pyflink/examples/table/word_count.py 以工程（文件夹）的方式提交 ./flink run \\ --jobmanager localhost:8081 \\ -pyarch file:///path/pyflink-py38env.zip \\ -pyexec pyflink-py38env.zip/bin/python3 \\ -pyfs /pyflink-py38env/lib/python3.8/site-packages/pyflink/examples/table \\ -pym word_count # 入口程序 提交到YARN（Flink On YARN） 此处使用的run-application模式，其他模式类似 提交程序在本地 ./bin/flink run-application \\ -t yarn-application \\ -Dyarn.ship-files=/data/bluewhale/modelhub/model-pyflink-stream \\ -Dyarn.application.name=pyflink-battery-calculate01 \\ -pyarch model-pyflink-stream/pyflink-py38env.zip \\ -pyclientexec pyflink-py38env/bin/python \\ -pyexec pyflink-py38env/bin/python \\ -pyfs model-pyflink-stream \\ -pym evcrrc_calculate_code_by_kafka \\ --jarfile /data/workspace/flink-1.17.1/lib/flink-sql-connector-kafka-1.17.1.jar 提交程序在hdfs 在提交程序前，先将程序和相关依赖上传到hdfs ./bin/flink run-application -t yarn-application \\ -pyarch hdfs:///data/mmodelhub/model-pyflink-stream/pyflink-py38env.zip \\ -Dyarn.application.name=pyflink-stream-model-1 \\ -pyclientexec pyflink-py38env.zip/bin/python \\ -pyexec pyflink-py38env.zip/bin/python \\ -py hdfs:///data/mmodelhub/model-pyflink-stream/pyflink-stream-kafka.py \\ --jarfile /data/workspace/flink-1.17.1/lib/flink-sql-connector-kafka-1.17.1.jar 提交的是pyflink-datastream模型，需要注意一下参数 Dyarn.ship-files： 该参数需要指定了pyflink流模型的绝对路径 ，路径指到模型文件夹 (本地提交时才会指定) -pyfs和-pym需要一起使用 实践总结 使用Application模式提交pyflink程序时，建议优先将pyflink程序使用到的依赖包，执行环境提前上传到HDFS文件系统（或其他文件系统）上，再在Shell 命令中以hdfs:的方式指定需要用到的资源路径，以提高作业提交的效率。 在开发pyflink程序时，若需要引入第三方Jar，建议在提交任务时通过–jarfile 指定Jar包，不建议在代码中通过add_jar方式，因为此方式目前仅支持指定本地Jar路径，不支持指定hdfs路径。 在Flink On Yarn高可用架构下，目前遇到提交多个任务，在集群中只有一个任务正常执行，其他任务提交成功但未执行的问题，目前解决方案是：在flink-conf.yaml配置中，将cluster.id配置内容注释掉，此配置主要用于zookeeper管理多个flink集群，至于高可用部署场景该配置是否为必配置项，暂未深入分析；参考 拓展pyflink相关开源项目： Provide docker environment and examples for PyFlink 快速入门pyflink Gathers Python deployment, infrastructure and practices. 参考博文： https://devpress.csdn.net/big-data/647a9c5d762a09416a07f59b.html https://developer.aliyun.com/article/743088 https://www.modb.pro/db/128620：PyFlink核心技术揭秘","tags":["python库"],"categories":["Python"]},{"title":"关于","path":"/about/index.html","content":"我认清时间的时候，只剩下一寸光阴，这是我的个人主页，到处是岁月痕迹… About Me 👨‍💻 日出未必意味着光明，太阳也无非是一颗晨星，只有在我们醒着时，才是真正的破晓. 你好~, 我是BluesSen: 一位热爱Coding的九零后🐂🐴，主要使用Python和Go, 专注于后端开发和ModelOps方向，积极拥抱AI，喜欢在Github闲逛， 立志做一个全沾艺人… 一名重度咖啡爱好者，现居NanJing… 信奉”大道至简”、”长期主义”… About Site 🖥️ 记录是对抗遗忘和自我欺骗的唯一武器… 这个博客名为AinLife，既是AI in Life，也是All in Life的组合，寓意着用心生活，拥抱AI。 一直以来，写博客对我而言总是断断续续，几经平台迁移。直到遇见 hexo-theme-stellar 这个主题，才决定在此驻足，开启新的记录篇章。 目前，这里主要分享技术文章、学习心得以及生活感悟等内容。 About Msg 📧 主站点：https://sswfive.xyz/ 备用站点：https://sswfive.github.io/ 邮箱: sswss5005@163.com 公众号：码上有咖啡 扫一扫👇"},{"title":"此时此刻","path":"/now/index.html","content":"2025-10-26 23:21:56 周日 🌙 12°今天学到两个词： Nice Fold：理性带着遗憾的放弃 Hero Call：明知有风险却依然坚定选择的跟注 2025-10-10 00:06:45 周五 🌙 22°今天学到了值得深思与耐人寻味的两个词：默则威、献丑不如藏拙 2025-09-21 17:11:44 周日 ☁️ 26°生存法则很简单，就是忍人所不忍，能人所不能。忍是一条线，能是一条线，两者的间距就是生存机会。 今天看到一句话，感触很深。 2025-08-28 22:43:52 周四 ☀️ 36°当AI编程助手飞速发展的当下，关于Coding我想说：1.当把思考的环节外包给LLM，就在为短期的轻松而牺牲长期的精通。2.新技术降低了完成任务所需的精力，却提高了保持竞争力所需的努力。 2025-08-15 23:41:17 周五 ☀️ 🔥当一天过去、一个月过去、一年过去，当我和所有的人一样，在时光的催促下，又老去了一岁，我看到了人生教会我的那些安静却真实的道理。 2025-08-08 21:30:17 周五 🌥️所有的决定，到头来并非真正选择了哪一种幸福，而更像是选择宁愿受哪一种苦… 2025-08-01 01:23:53 周五 ☀️夜疲惫的人强撑着睡眼用夜偷偷赎回那些白天里被生活抢走的时光。"},{"title":"常用的Chrome和VSCode插件","path":"/notes/collections/250318-plugins-usage.html","content":"Chrome常用插件 官方插件商店 谷歌扩展商店 第三方插件商店 CrxDL.COM：免费下载 Chrome 插件扩展程序，支持 Manifest V2 和 V3 版本，提供丰富的插件分类和搜索功能。 收藏猫插件 装机必备OneTab 简介：一款优化内存的插件，将活跃的多个页签一键变成链接聚合到一个页面中。 类别：内存优化工具 说明：装机必备，目前一直在用，高频；点击 OneTab 可将标签页转换成一个列表以释放系统资源，当您需要再次访问这些标签页时，可以单独或全部恢复它们。 iTab新标签页 简介：一款新型的小组件式浏览器插件，用于美化和管理浏览器主页，优化浏览器使用体验。它集成了一系列实用的小组件。 类别：标签页工具 说明：装机必备，多种浏览器都支持，目前一直在用，高频。 沉浸式翻译 简介：一款免费的（原文译文）双语对照网页翻译插件，支持多端，支持全文翻译，段落翻译。 类别：翻译工具 说明：装机必备，个人日常阅读外文资料的助手，目前一直在用，高频。 Surfingkeys 简介：一款让用户可以通过Vim风格的键盘操作来控制浏览器。 类别：效率工具(Vim) 说明：装机必备，使用 Vim 的方式操作浏览器，既能强化 Vim的常用指令，又能提高浏览器的操作效率，目前一直在用，高频。 GreasyFork 简介：油猴脚本商店，一个专注于用户脚本的网站。 类别：效率工具、JS 脚本工具 说明：装机必备，一般在此站点寻找并安装自己需要的脚本工具，用以提高使用浏览器的效率，低频。 redirect-skipper 一款让你无感跳过各种提示外链中转页的浏览器插件，无感跳过 掘金、 知乎、 少数派、 CSDN 等站点的外链提示页，让你的网页浏览体验更加顺畅。 甄选收藏NopeCHA: CAPTCHA Solver 一款能够自动识别图片验证码的浏览器扩展，可以自动识别验证码，无需再手动点击，从此解放双手。 时间：2025-10-18 MarkSnip 简介：一个 Chrome 插件，用于将网页内容（特别是文章）抓取并保存为 纯净的Markdown格式文件。 类别：下载工具 说明：Markdown处理工具，平时关闭，用时开启，低频。 时间：2025-05-12 Cloud Document Converter 简介：一款支持将飞书文档下载或复制为Markdown的Chrome扩展， Github链接。 类别：下载工具， 说明：飞书文档下载，平时关闭，用时开启，低频； GoFullPage 简介：一款浏览器长截图 插件，社交媒体展示产品完整截图绝佳利器，免费又好用，官网链接。 类别：截图工具 说明：浏览器长截图工具，平时关闭，用时开启，低频。 Infinity新标签页Pro 简介：重新定义你的标签页，不仅能美化你的主页，还能提高你的生产力，官网地址。 类别：标签页工具 说明：以前一直用的，后来遇到了更好 替代品 iTab新标签页，之后就不用了。 FeHelper 简介：本插件支持Chrome、Firefox、MS-Edge浏览器，内部工具集持续增加，目前包括 JSON自动手动格式化、JSON内容比对、代码美化与压缩、信息编解码转换、二维码生成与解码、图片Base64编解码转换、Markdown、 网页油猴、网页取色器、脑图(Xmind)等贴心工具，官网链接； 类别：前端开发助手 说明：未使用，仅收藏。 VSCode常用插件装机必备Catppuccin 简介：超看好的主题配色方案。该项目是由社区驱动的配色方案，内含以暖色调为主、色彩丰富的主题，可用于 VSCode、JetBrains、Vim 等编辑器和 IDE，同样适用于各种编程语言的开发库、终端、操作系统、浏览器等应用；链接直达：Catppuccin for VSCode，官网地址。 类别：主题配色插件 说明：推荐！！！目前已将 VSCode、Cursor、PyCharm 、Jupyterlab都换成了该主题配色。 vscode-markdown-github VSCode 的 Markdown 渲染插件，特点是完全按照 GitHub 的 Markdown 效果进行渲染 甄选收藏PPZ 简介：一个 VSCode 插件，提供操作数据库的图形界面，支持多种数据库 类别：数据库可视化工具 说明：数据库（MySQLPostreSQL)连接插件，某些场景可以替代Navicat工具。 code-server 简介：一个 VS Code 远程服务的封装，可以安装在任意机器上面，然后就能在浏览器使用 VS Code 类别：远程工具 说明：在任意浏览器上运行 VSCode。 编程助手Kilo Code 简介：一个开源的AI Agent的VSCode插件，它通过生成代码，自动执行任务和提供建议来帮助你更高效的编写代码。（官方自称：Kilo 是 Cline、Roo 和我们自己添加的功能的合并） 说明：它定位为Cursor的开源替代品，并在此基础上进行了功能扩展和优化 时间：2025-05-12 Cline 简介：一个可以使用你的终端和编辑器的AI助手，同时还支持使用MCP来创建新工具，并扩展自身的能力。 说明：使用体验还是挺不错的。VSCode: 安装链接 Obsidian插件Composer 简介：一款治愈视觉的主题插件，布局设计遵循“简单”原则，去除多余的框线和元素，给人干净的感觉。配色方面采用了低饱和偏冷的设计，并提供了素、青、霞三套配色方案供用户切换（需要通过style settings插件切换）。 时间：2025-09-22 17:40:30 Maple 简介：一款适用于桌面和移动设备的简洁现代 Obsidian 主题，拥有出色的组件、优雅的动画效果，以及通过样式设置插件实现的自定义选项。它内置了自行设计的等宽字体 Maple Mono。 时间：2025-11-09 21:55:52","tags":[null]},{"title":"机场（科学上网）合集","path":"/notes/collections/250407-vpn-selection.html","content":"使用中wmsxwd 简介：一家提供網絡加速服務的網絡供應商，可以給您在原有基礎上提供更高效、快速、穩定的網絡加速服務，基于ShadowsocksShadowsocksRV2ray的科学上网方式。 数字站点 我们所向往的 简介：同上 字母站点 主站地址 这两个属于同一家的不同站点，一个是数字站点(new)，一个是字母站点（old），账号不互通，目测字母站点目前不给注册新用户了，但老用户依然可以用，这是它的主站地址。 如果可以欢迎使用我的邀请码进行注册: 邀请链接 使用过星辰加速 简介：从这一刻，连接您与星辰大海 一元机场 简介：主打一个便宜，稳定性很一般。 未使用极速Max 站点简介：一个为TrojanV2ray全中转机场 收录理由：一个关注很久的博主推荐，注册了解以后，发现价格不便宜，秉着“贵东西除了贵没有其他缺点”的想法，收录起来，以备不时之需。 收录时间：2025-09-10 17:36:36 AmyTelecom 站点简介：快速、简单、好用的SSR服务 收录理由：同事推荐 收录时间：2025-09-04 17:27:31 YTOO 站点简介：不详，需要下单后才能注册用户 添加时间：2025-09-04 17:26:41 添加理由：同事推荐 Just My Socks 简介：一个国外的服务商，费用挺贵 Clashxpro 简介：一款MacOS系统即苹果电脑系统下的代理软件客户端，是代理工具Clash在MacOS系统的图形客户端 Github链接 WgetCloud 简介：不仅仅是机场，更是你连接世界的桥梁。现在加入，即刻体验稳定、高速、安全的科学上网新境界！ Github链接 白月光 官网地址 Hiddify 简介：一款基于 Sing-box 通用代理工具的跨平台代理客户端 鳄鱼 简介：暂无 客户端Clash官网导航 Clash官方各版本下载地址与备份下载地址。","tags":[null]},{"title":"常用Proxy加速站","path":"/notes/collections/250413-cdn-site.html","content":"Docker加速渡渡鸟的容器镜像小站本站镜像100%同步自官方镜像！目前支持同步镜像源: gcr.io ghcr.io quay.io k8s.gcr.io docker.io registry.k8s.io docker.elastic.co skywalking.docker.scarf.sh mcr.microsoft.com 单个镜像大小限制2G; 备注：站点稳定，资源丰富，力荐！！！ KSpeeder多镜像并发下载，动态负载均衡，断点续传支持，Docker镜像代理服务 Docker Layer ICUDocker Layer ICU 是一个免费的 Docker 镜像加速服务，它提供了全球范围内的 Docker 镜像加速服务，包括中国地区的镜像加速服务。 Github加速Github520解决访问时图裂、加载慢的问题 Go 加速Goproxy.cngoproxy.ioHuggingFace 加速HF-Mirror魔搭社区Web静态加速Web缓存网CND Hub快速获取 cdnjs、npm 和 github 上资源 cdn 链接的网站，并支持耗时比较，便于选取最优 cdn 油猴加速https://scriptcat.org/zh-CN/ https://www.userscript.zone 综合站点chsrc全平台通用的一个命令行换源工具，目标支持 Linux, Windows (MSYS2, Cygwin), macOS, BSD 等尽可能多的操作系统环境，龙芯、飞腾、RISC-V 等尽可能多的 CPU。 快点软件镜像站提供开源软件国内镜像信息服务，内容包括：常用操作系统镜像、国内大学镜像站列表、前端开源项目CDN、PIP、Docker 等","tags":[null]},{"title":"甄选实用&效率工具","path":"/notes/collections/250909-practical-efficiency-tools.html","content":"在日常工作和学习中，好的工具能让效率提升不止一个档次。这是我收集整理的一份实用工具清单。本甄选合集将持续更新……🚀 Markdown编辑器FlyMD 一款轻量级、高性能的本地 Markdown 编辑器,支持 PDF 高精度解析、AI 辅助写作、智能待办提醒等功能。本地优先,数据安全可控,开箱即用。 UI相关storyset 免费的插画网站 菜鸟图标 免费商用矢量图标库，共有 20,0000+ 个高品质矢量图标，是设计师与开发者的灵感宝藏。一站式解决你的图标需求，提升效率的小助手！ Icons8 工具简介：Icons8 团队、设计软件和人工智能工具精心绘制的原创图片库。提供免费的图标、剪贴画插图、照片和音乐，为创作人员和开发人员准备的终极设计套件。 收录时间：2025-10-18 00:16:56 Emoji copy 工具简介：一个emoji图集网站，一键copy 官网地址：点击前往 收录时间：2025-09-10 07:52:50 Emoji 大全 该网站提供了最新、完整的 Emoji 搜索和相关信息，包括表情符号含义、使用示例、Unicode 代码点、高分辨率图片、复制和粘贴，以及 Emoji 大数据排名、矢量图形和动态图表、智能算法情感分析和表情符号语言学研究。 favicon.io 工具简介：网站图标 Favicon 的在线生成工具 官网地址：点击前往 收录时间：2025-09-10 07:48:45 Axure地图元件库 工具简介：可一键轻松复制省市区SVG地图元件 官网地址：点击前往 收录时间：2025-09-10 07:49:01 Vue Color Avatar 工具简介：一个纯前端的矢量风格头像生成网站，可以搭配不同的素材组件，生成自己的个性化头像 官网地址：点击前往 收录时间：2025-09-10 07:49:30 画图相关mind-map 工具简介：一个还算强大的 Web 思维导图 官网地址：点击前往 收录时间：2025-09-10 00:19:09 Frame0 工具简介：一个画草图的工具 官网地址：点击前往 收录时间：2025-09-10 00:15:35 Pddon 工具简介：一款免费多功能专业在线画图(Low Code)工具，提供低代码和 AI 智能辅助工具 官网地址：点击前往 收录时间：2025-09-10 00:13:03 Mermaid 工具简介：一种基于 JavaScript 的图表绘制工具，它使用 Markdown 启发的文本定义和渲染器来创建和修改复杂的图表。 Mermaid 的主要目的是帮助文档跟上开发的步伐。 官网地址：点击前往 收录时间：2025-09-10 00:11:25 Pointless 工具简介：一个开源的网页画板，可以当作白板使用，效果不错。 官网地址：点击前往 收录时间：2025-09-10 00:04:17 Drawio 工具简介：一个在线画流程图的网站，在线使用。 官网地址：点击前往 收录时间：2025-09-10 00:02:19 Excalidraw 工具简介：一个非常简单易用的白板绘图开源工具(手绘风格的白板 Web 应用。这是一款完全免费、开源的基于无限画布的白板 Web 应用，用户可以在上面创建手绘风格的作品 立即体验：点击前往 收录时间：2025-09-09 23:59:38 Drawnix 工具简介：一款开源的开箱即用的白板工具（SaaS），源于绘画( Draw )与凤凰( Phoenix )的灵感交织。一体化白板，包含思维导图、流程图、自由画等。 官网地址：点击前往 立即体验：点击前往 收录时间：2025-09-09 22:45:36 PDF相关章快图 免费在线 PDF 盖章工具，支持添加普通印章和骑缝章，快速处理 PDF 文档盖章需求，无需下载软件。 收录时间：2025-10-18 00:23:13 Stirling-PDF 工具简介：是一个强大的、本地托管的基于 Web 的 PDF 操作工具，使用 Docker。它使您能够对 PDF 文件执行各种作，包括拆分、合并、转换、重新组织、添加图像、旋转、压缩等。 官网地址：点击前往 收录时间：2025-09-09 22:36:58 图片相关PicView 工具简介：比系统自带更好用的看图工具。这是一款快速、免费的图片查看工具，适用于 Windows 和 macOS 平台。它采用 .NET NativeAOT 编译技术，体积小、启动速度快，支持浏览长图、编辑图片、格式转换、批量处理等功能。 官网地址：点击前往 收录时间：2025-12-04 16:09:36 upscayl 工具简介：AI 提升图片清晰度的软件，支持 Linux，Mac，Windows 多平台 官网地址：点击前往 收录时间：2025-09-10 00:26:10 carbon 工具简介：免费代码贴图工具 官网地址：点击前往 收录时间：2025-09-10 00:26:34 Drawl.ink 工具简介：一个在线的将链接转化成好看的图片的工具 立即体验：点击前往 收录时间：2025-09-10 00:17:35 照片转手绘 工具简介：这是一款先进的、免费的AI工具，它使用人工智能将照片转换成精美的艺术画作，提供多种艺术风格，包括铅笔素描、炭笔画、水彩和卡通效果等。 立即体验：点击前往 收录时间：2025-09-09 22:54:47 文字相关calligrapher.ai 工具简介：一个在线工具，可以将英文输入变为手写体，并具有动画效果，提供SVG格式下载。 立即体验：点击前往 收录时间：2025-09-10 00:20:52 文字转手写 工具简介：这是一款免费的在线工具，在各种纸张背景上将数组文字转换为漂亮的手写笔记，支持多种纸张背景。只需输入您的文本，选择纸张样式，预览结果，并下载图像。通过文字转手写，您可以轻松将数字文字转化为真实的手写效果，适用于请柬、个人笔记或专业文档。 立即体验：点击前往 收录时间：2025-09-09 23:06:16 翻译相关沉浸式翻译 工具简介：一款全网口碑炸裂的双语对照网页翻译插件。可以完全免费地使用它来实时翻译外语网页，PDF翻译，EPUB电子书翻译，视频双语字幕翻译等。还可以自由选择调用DeepL、Gemini等人工智能引擎来翻译上述内容。 官网地址：点击前往 收录时间：2025-09-09 23:19:35 电子书阅读器Koodo-reader 工具简介：一款跨平台的现在电子书管理器和阅读器。 官网地址：点击前往 收录时间：2025-09-09 23:23:34 Ebook-reader 工具简介：一个在线电子书阅读器，支持像 Yomitan 这样的词典扩展，托管在https://reader.ttsu.app上 立即体验： 点击前往 收录时间：2025-09-09 23:26:28 Flow 工具简介： 一款开源的基于浏览器端的 ePub 阅读器。 立即体验： 点击前往 收录时间：2025-09-09 23:28:25 任务管理Super Productivity 工具简介： Super Productivity 是一款先进的跨平台的待办事项列表应用程序，具有集成的 Timeboxing 和时间跟踪功能(一款先进的待办事项列表应用。超级生产力是一款用 TypeScript 开发的高级 TODO 应用，旨在帮助用户规划任务和管理待办事项，培养健康高效的习惯。它开源、免费、无需注册，支持与 Jira、GitHub、GitLab 等第三方平台集成，可即时收到任务变动的通知。除了可在线使用的 Web 网页版，还提供了 Windows、Linux、macOS、iOS 和 Android 客户端) 官网地址：点击前往 收录时间：2025-09-09 23:33:05 DooTask 工具简介：一款轻量级的开源在线项目任务管理工具，提供各类文档协作工具，在线思维导图、在线流程图、项目管理、任务分发、即时IM，文件管理等工具。 官网地址：点击前往 收录时间：2025-09-09 23:34:36 focalboard 工具简介：一款开源、多语言、自托管的项目管理工具，兼容了 Trello 和 Notion 的特点。它支持看板、表格和日历等视图管理任务，并提供评论同步、文件共享、用户权限等功能。该工具还提供了适用于 Windows、macOS、Linux 系统的客户端。 官网地址：点击前往 收录时间：2025-09-09 23:35:39 简历相关LapisCV 工具简介：一款开箱即用的简历模板（提供 Obsidian 和 Typora 的简历模板），它基于 Markdown 格式、编辑方便、所见即所得，设计简洁且正式，借助编辑器可直接导出 PDF 格式的简历。 官网地址：点击前往 收录时间：2025-09-10 00:23:21 热速美斯 工具简介：一个免费的简历制作网站，可以自由拖拽，帮助你制作一份精美简历 立即体验：点击前往 收录时间：2025-09-10 00:24:27 打字练习站点Code Typing Script代码测试打字速度，可以选择不同的编程语言。 10 Fast Fingers做一些输入练习 是一个在线的输入速度测试网站。定向训练，比如使用下面的提到的 keybr 这个网站上的单词不是真实的单词，但是可以通过输入的错误来定向看看哪些字母输入错误比较多 Qwerty暂无介绍 蓝豆打字 蓝豆打字是一个在线打字练习的网站，渐进式、智能辅助、强化记忆。 keybrtyping.io是一个编程打字练习网站。在线通过编程练习来训练输入的效率。 codeflow是一个开源的在线指法练习网站。提供了常用的关键字、单词、词组、句子、标点输入的指法练习。 typelit.io通过打小说来练习打字 typingclub系统学习打字 检测工具Setup Check 无广告的在线检测工具，可以用来测试网速、耳机、麦克风、鼠标键盘、摄像头等是否正常。 All in SSL 一个集证书申请、管理、部署和监控于一体的 SSL 证书全生命周期管理工具。 公众号排版工具bm.md 更好用的Markdown 排版助手，一键适配微信公众号、网页和图片。 WeMD 更优雅的Markdown公众号排版工具，告别复杂工具。Markdown写作，一键复制到公众号，专为公众号创作者设计的本地优先编辑器。 花生公众号排版器 一个专为微信公众号设计的Markdown编辑器。官网地址：点击前往 文颜 文颜- Markdown文章排版美化工具，支持微信公众号、今日头条、知乎等平台。 RSS阅读器MrRSS 一个现代化、跨平台的RSS阅读器 Folo 一个AI RSS阅读器","tags":[null]},{"title":"匠心甄选：一份技术学习的藏宝图","path":"/notes/collections/250919-learning-materials.html","content":"Agentagentic-design-patterns 谷歌新书Agent设计模式(agentic design patterns)中文版，持续更新。提供在线阅读、pdf和epub电子书下载。链接直达 Rust学习Rust Tutorial 一个针对新手的 Rust 快速教程，教你从零实现一个简单用于记录 Todo 事项的 CLI (Command Line Interface, 命令行接口) 程序。 Rust 语言实战 通过有挑战性的示例、练习题、实践项目来提升 Rust 水平，建立从入门学习到上手实战的直通桥梁。 数据结构与算法labuladong 的算法笔记 一个帮你练成框架化、模块化的思维方法解决算法题，该项目仓库目前约有 60 多篇基于 Leetcode 题目的原创文章，涵盖所有题型和技巧；前往：Github地址、官网地址 Hello 算法 动态图解、一键运行的数据结构与算法的教程， 支持 Python, Java, C++, C, C#, JS, Go, Swift, Rust, Ruby, Kotlin, TS, Dart 代码；前往：Github地址、官网地址 代码随想录 LeetCode 刷题攻略：200道经典题目刷题顺序，共60w字的详细图解，视频难点剖析，50余张思维导图，支持C++，Java，Python，Go，JavaScript等多语言版本，从此算法学习不再迷茫！🔥🔥 来看看，你会发现相见恨晚！🚀， 前往：Github地址、官网地址 数据结构和算法必知必会的50个代码实现 王争：数据结构和算法必知必会的50个代码实现 Data Structure Visualizations 本站点以可视化的方式讲解数据结构和算法 python_data_structures_and_algorithms Python 中文数据结构和算法教程 Play-with-Algorithms 慕课网上的实战课程《算法与数据结构精解》的官方代码仓 算法刷题站点labuladong 的题目列表CodeTop机器学习Pytorch实用教程 《Pytorch实用教程》（第二版）无论是零基础入门，还是CV、NLP、LLM项目应用，或是进阶工程化部署落地，在这里都有。相信在本书的帮助下，读者将能够轻松掌握 PyTorch 的使用，成为一名优秀的深度学习工程师。 Datawhale 一个专注于 AI 领域的开源组织，致力于构建一个纯粹的学习圈子，帮助学习者更好地成长。我们专注于机器学习，深度学习，编程和数学等AI领域内容的产出与学习。 ml-system-design-pattern 机器学习的系统设计模式：此库包含了实践生产环境中机器学习系统的训练、服务化和操作的系统设计模式。 大模型大语言模型 人民大学AI Box小组编写的开放电子书，本书适用于具有深度学习基础的高年级本科生以及低年级研究生使用，可以作为一本入门级的技术书籍。 Happy-LLM 从零开始的大语言模型原理与实践教程，一个系统性的 LLM 学习教程，将从 NLP 的基本研究方法出发，根据 LLM 的思路及原理逐层深入，依次为读者剖析 LLM 的架构基础和训练过程。 软件架构深入高可用架构原理与实践 这是一本关于架构设计的开源书籍,整个系列的内容主要集中在 网络、集群以及服务治理、FinOps 这三个主题，这也代表着基础架构的几个核心：稳定、效率、成本。 云原生Kubernetes 基础教程 全面介绍容器编排技术的实战手册，涵盖核心架构、关键组件和实际应用，涵盖容器、Kubernetes、服务网格、Serverless 等云原生的多个领域。 英语学习TypeWords 可在网页上使用的背单词软件，内置了常用的 CET-4 、CET-6 、GMAT 、GRE 、IELTS 、SAT 、TOEFL 、考研英语、专业四级英语、专业八级英语，也有程序员常见英语单词以及多种编程语言 API 等词库。点击前往 综合杂项ChinaTextbook 所有小初高、大学 PDF 教材开源合集，仓库维护着的初心是促进义务教育的普及和消除地区间的教育贫困。","tags":[null]},{"title":"SSH Terminal 工具合集","path":"/notes/collections/250921-ssh-selection.html","content":"跨平台Alacritty 一款跨平台的现代终端仿真器，具有合理的默认值、但允许进行广泛的配置。主题地址 Termora 一款跨平台终端模拟器和 SSH 客户端，支持 Windows、macOS、Linux。使用 KotlinJVM 开发，支持（正在实现中） XTerm 控制序列协议。 Xterminal 一款强大跨平台（MacOS、Linux、Win）的 SSH 工具，更提供本地控制台，尤其界面配色挺不错，在线编辑文件的功能很 nice。【有免费版和收费版，通常免费版本就满足日常工作需求】 Warp 是一款Rust语言编写的现代化终端工具，内置AI功能，号称是 21 世纪的终端。 EternalTerminal 基于SSH的远程登录工具，自动重连上一次的会话，即不会发生会话中断。 WindTerm 一款专业的跨平台（MacOS、Linux、Win）的 SSHSftpShellTelnet串口终端【项目貌似已停止维护了，但已有功能也基本满足需求】 PortX 一款跨平台的SSH工具，通过简单但全面的UI, PortX为您提供了纯粹的终端模拟体验。 Tabby 以前称为 Terminus， 是一款高度可配置的终端模拟器，用于本地 shell、串行、SSH 和 Telnet 连接，适用于 Windows 10、macOS 和 Linux。 Termius SSH client and terminal how it should be in 2024【有免费版和收费版，通常免费版本就满足日常工作需求】 tssh trzsz-ssh ( tssh )是一个 用Go开发的ssh 客户端，旨在替代 openssh 客户端。它旨在提供与 openssh 的完全兼容性，镜像其所有功能，同时还提供其他有用的功能。如登录提示、批量登录、记住密码、自动交互、trzsz、zmodem(rzsz)、mosh等udp模式等。 NxShell 一款用JS开发的跨平台的SSH新终端工具,就像当下流行的PuTTY终端一样.但NxShell随着后期功能的迭代完善,会生长出自己独有的终端工具超能力. MacOSiTerm2 一款专为 Mac 平台打造的 SSH 工具【MacOS用户受众很高】 Ghostty Ghostty 是一款快速、功能丰富的跨平台终端模拟器，它使用平台原生 UI 和 GPU 加速。 号称零配置（Zero Configuration Philosophy） WinOSMobaXterm 一款带有 X11 服务器、选项卡式 SSH 客户端、网络工具等的增强型 Windows 终端【有免费版和收费版，通常免费版本就满足日常工作需求】 其他终端SSHM SSHM 是一个漂亮的命令行工具，它改变了您管理和连接到 SSH 主机的方式。它采用 Go 构建并具有直观的 TUI 界面，使 SSH 连接管理变得轻松愉快。 ContainerSSH 一款在在 Kubernetes 和 Docker 中启动容器的 SSH 服务器","tags":[null]},{"title":"编程开发工具与解决方案","path":"/notes/collections/250924-dev-tools.html","content":"Agent相关Agent Skills Marketplace Everything you need to know about discovering and using agent skills Skills 此仓库包含 Anthropic 为 Claude 实现的技能。有关 Agent Skills 标准的信息，请访问 agentskills.io 。 「编程AI工具」JetBrains IDE全家桶 Visual Studio Code 微软出品的开源编辑器 Cursor AI辅助编程工具 Zed Zed 是一款面向高性能人机及 AI 协作的新一代代码编辑器。 TRAE 字节开发的AI编程IDE 「AI相关技术」Awesome-MCP-ZH 一个专为中文用户打造的 MCP（模型上下文协议）资源合集！ 这里有 MCP 的基础介绍、玩法、客户端、服务器和社区资源，帮你快速上手这个 AI 界的 “万能插头”。 Ai 迷思录 项目旨在帮助您从零基础成长为具备实战能力的人工智能安全专家。本指南涵盖了基础知识、法律法规、经典 AI 模型、漏洞与攻击、防御方法、安全开发与运维、相关框架、会议讲座以及实践实验室等多个方面，并为每个层级提供了的学习建议和资源。 开源解决方案可观测性相关OpenObserveOpenObserve（简称 O2）是一个云原生可观察性平台，专为日志、指标、跟踪、分析、RUM（真实用户监控 - 性能、错误、会话重放）而构建，设计用于 PB 级工作。【ELK的替代品】 开源书签作者分门别类的整理的常见热门的开源组件，包括但不限于接入层组件、数据库组件、消息中间件、低代码、大数据、K8S、监控等等（PS: 目前不清楚作者相关信息） 在线代码编辑器code-server 一款VS Code in the browser的代码编辑器，支持多种插件，使用 Coder 让开发人员使用完全配置的云开发环境，Coder 是您可以自行托管和管理以实现完全安全性和控制的唯一开源平台。 monaco-editor一款基于浏览器的在线代码编辑器。 Redis客户端工具Tiny RDM一款优秀的 Redis 可视化管理工具（⭐️）,它提供了多种连接方式、分段加载、慢日志、转码显示等功能，可以在 Windows、Linux 和 macOS 系统上使用 RedisShake:是一个 Redis 数据处理和迁移工具。 redis-manager Redis 一站式管理平台，支持集群的监控、安装、管理、告警以及基本的数据操作 正则测试工具regex101API管理工具Yaak 一个快速、注重隐私的 API 客户端，支持 REST、GraphQL、SSE、WebSocket 和 gRPC——使用 Tauri、Rust 和 React 构建。 postmanapifoxRestfox在线的api测试工具 ，postman替代品 Posting终端中的API测试工具 格式转换工具json-server仅需 30 秒、不用写代码就能模拟接口数据。这是一款小巧的接口模拟工具，它使用起来十分简单，只需创建 JSON 文件，然后一条命令就能快速启动接口服务。 Json Editor在线的 JSON 编辑器。该项目是一个基于 Web 的 JSON 编辑器，可用于查看、编辑、格式化和验证 JSON。它支持树形编辑器、代码编辑器和纯文本等模式，不仅可以直接在线使用，还可作为组件集成到项目中。 ConvertSimple一款 JsonYAMLTOML 之间的在线转换工具 transform一款多语言网络转换器 JsonFormatter一款 Json 格式化神器 docker-compose 工具集docker-compose通过docker-compose编排一系列环境进行一键快速部署运行，支持安装常见的数据库、消息队列、日志系统、CICD、存储系统等。 Kafka kafka-stack-docker-compose: 使用 docker-compose 一键创建 Kafka 服务，支持单节点创建和集群创建。 kafka-ui UI for Apache Kafka : 用于管理 Apache Kafka 集群的多功能、快速且轻量级的 Web UI. SSL证书管理AllinSSL AllinSSL 是一站式 SSL 证书全生命周期管理工具，集申请、签发、部署、续期与监控于一体，全面支持 Let’s Encrypt、ZeroSSL、Google Trust Services、SSL.COM、BuyPass 等主流 CA，提供多平台部署与自动化运维能力，助力企业高效、安全地管理数字证书。 Certd 开源SSL证书管理工具；全自动证书申请、更新、续期；通配符证书，泛域名证书申请；证书自动化部署到阿里云、腾讯云、主机、群晖、宝塔；https证书，pfx证书，der证书，TLS证书，nginx证书自动续签自动部署。 Nginx相关nginx-flow 一款功能强大的Nginx配置文件可视化编辑工具 nginx-proxy-manager 用于管理 Nginx 代理主机的 Docker 容器，提供简单而强大的界面 nginxconfig Nginx配置生成器，支持多种编程语言 向量数据库seekdb 开箱即用的轻量级向量数据库。该项目是 OceanBase 团队开源的一款轻量级 AI 原生搜索数据库，支持关系型、向量和文本数据的统一存储与检索。它提供嵌入式和服务器两种模式，最低仅需 1C1G 即可运行，并兼容 MySQL 协议。 K8S相关Kite 开源的轻量级 K8s 管理面板。这是一款轻量级、现代化的 Kubernetes 可视化管理平台，适用于管理和监控 K8s 集群。它拥有直观易用的界面，支持查看 Pod 日志、执行容器命令、编辑 YAML 配置、管理用户权限等功能。 其他LazyTyper 告别手动输入的繁琐！这是一款基于 Whisper 的免费语音输入应用，准确率高达 90%，速度超快且体积小巧。它完美支持中英日韩等多语言无缝混合输入，让沟通更高效。立即体验未来输入方式！ 代码托管工具Gogs Gogs（ /gɑgz/ ）项目旨在构建一个简单、稳定且可扩展的自托管 Git 服务，并尽可能简化其部署过程。借助 Go 语言，Gogs 可以以独立的二进制分发包的形式部署在所有 Go 支持的平台上，包括 Linux、macOS、Windows 和基于 ARM 的系统。 Codeberg Codeberg 是一个基于Git 的开源代码托管平台，致力于为开发者提供免费、开放且隐私友好的协作环境。 该平台主要服务于个人开发者、开源项目团队以及注重数据隐私的组织，提供类似于GitHub 或GitLab 的版本控制与项目管理功能，同时强调非营利性和社区驱动原则。 Forgejo 是一个自托管的轻量级软件仓库,安装简便，维护成本低，就能胜任工作。","tags":[null]},{"title":"Mac常用快捷键","path":"/notes/pc/250402-mac-quickkey.html","content":"macOS 上有几个常用的修饰键： Command（或 Cmd）⌘ Option（或 Alt）⌥ Caps Lock ⇪ Shift ⇧ Control（或 Ctrl）⌃ Fn 要使用键盘快捷键，需要先按住一个或多个修饰键，然后按快捷键的最后一个键。 例如复制的操作是 「⌘」+「C」，则需要先按住 「⌘」 键，然后再去按 「C」 键，最后同时松开这两个键。 个人常用 不同应用之间切换：command + tab 同一个应用的不同窗口之间切换：command + ~ 程序坞的显示与影藏： control + F3 或者 command + option + D 最小化窗口： command + M 恢复：command + tab 到应用后，松开 tab，然后按 option 上，再松开 command 隐藏当前窗口： command + H 全局类快捷键 快捷键 功 能 Command ⌘ + Tab 在已经打开的 App 之间进行切换。 Command ⌘ + Shift ⇧ + Tab 反向在已经打开的 App 之间进行切换。 Control ⌃ + Left ⬅︎ Right ➡︎ 左右切换「空间」。 Control ⌃ + Up ⬆︎ 显示「调度中心」。 Control ⌃ + Down ⬇︎ 显示当前应用程序窗口。 Command ⌘ + Option ⌥ + D 显示 隐藏「程序坞」。 剪切、拷贝、粘贴 快捷键 功 能 Command ⌘ + X 剪切所选项并拷贝到剪贴板。不适应于「访达」中的文件。 Command ⌘ + C 将所选项拷贝到剪贴板。适用于「访达」中的文件拷贝。 Command ⌘ + V 将剪贴板的内容粘贴到当前文稿或 App 中。适用于「访达」中文件粘贴。 Command ⌘ + Option ⌥ + C 拷贝样式：将所选项的格式设置拷贝到剪贴板。 Command ⌘ + Option ⌥ + V 粘贴样式：将拷贝的样式应用到所选项。适用于「访达」中文件移动。 Command ⌘ + Z 撤销上一个命令。 Command ⌘ + Shift ⇧ + Z 重做，反向执行撤销命令。在某些 App 中，可以撤销和重做多个命令。 Command ⌘ + A 全选各项。同样适应于「访达」中的文件。 关闭、退出程序 快捷键 功 能 Command ⌘ + W 关闭当前应用最前面的窗口，软件并未退出进程。 Command ⌘ + Option ⌥ + W 关闭当前应用的所有窗口，软件并未退出进程。 Command ⌘ + Q 真正退出程序，软件退出进程。 Command ⌘ + Option ⌥ + Esc 打开强制退出软件的窗口，通常在软件无响应的时候使用。 Command ⌘ + Option ⌥ + Shift ⇧ + Esc 直接强制退出当前软件。 窗口类 快捷键 功 能 Command ⌘ + 在当前软件的多个窗口直接切换（只适用于苹果系统软件）。 Command ⌘ + M 最小化当前窗口。 Command ⌘ + H 隐藏当前软件。 Command ⌘ + Option ⌥ + H 隐藏除了当前软件以外的所有软件。 Command ⌘ + Ctrl ⌃ + F 进入 退出窗口最大化。 Command ⌘ + W 关闭当前应用最前面的窗口，软件并未退出进程。 Command ⌘ + Option ⌥ + W 关闭当前应用的所有窗口，软件并未退出进程。 2.5 访达 快捷键 功 能 Command ⌘ + Shift ⇧ + G 打开「前往文件夹」窗口。 Command ⌘ + Shift ⇧ + . 显示 隐藏「文件」、「文件夹」。 Command ⌘ + Up ⬆︎ 打开包含当前文件夹的文件夹，即返回上一层。 Command ⌘ + Down ⬇︎ 打开当前所选「文件夹」或「文件」。 Command ⌘ + N 打开新的「访达」窗口。 Command ⌘ + T 打开新的「访达」标签页（在当前窗口）。 Command ⌘ + Shift ⇧ + N 新建「文件夹」。 Command ⌘ + Delete 将所选项移动废纸篓。 Command ⌘ + Shift ⇧ + Delete 清倒废纸篓。 Command ⌘ + C 拷贝所选文件到剪切板。 Command ⌘ + V 将剪切板中的文件粘贴到当前位置。 Command ⌘ + Option ⌥ + V 移动文件：将剪贴板中的文件从原始位置移动到当前位置。 截图 快捷键 功 能 Command ⌘ + Shift ⇧ + 3 截取当前全屏，并保存全屏文件到桌面。 Command ⌘ + Ctrl ⌃ + Shift ⇧ + 3 截取当前全屏，并保存全屏截图到剪切板。 Command ⌘ + Shift ⇧ + 4 按下后拖拽鼠标选择截图区域，并保存部分屏幕截图文件到桌面。 Command ⌘ + Ctrl ⌃ + Shift ⇧ + 4 按下后拖拽鼠标选择截图区域，并保存部分屏幕截图到剪切板。 浏览器 快捷键 功 能 Command ⌘ + T 在当前窗口，打开新的「标签页」。 Command ⌘ + N 打开新的「浏览器窗口」。 Command ⌘ + 鼠标点按链接 在新的「标签页」打开链接。 Command ⌘ + 鼠标点按书签 在新的「标签页」打开书签。 Command ⌘ + W 关闭当前「标签页」。 Command ⌘ + L 定位到「地址栏」。 2.8 文本编辑操作光标移动 快捷键 功 能 Ctrl ⌃ + F 光标前进一个字符（F Forward）。 Ctrl ⌃ + B 光标后退一个字符（B Backward）。 Ctrl ⌃ + P 光标上移一行（P Previous）。 Ctrl ⌃ + N 光标下移一行（N Next）。 Ctrl ⌃ + A 光标移动到当前行或段落开头（A Ahead）。 Ctrl ⌃ + E 光标移动到当前行或段落结尾（E End）。 Command ⌘ + Up ⬆︎ Down ⬇︎ 光标移动到全部文本开头 结尾。 Command ⌘ + Left ⬅︎ Right ➡︎ 光标移动到当前行的行首 行尾。 Option ⌥ + Left ⬅︎ Right ➡︎ 光标移动到当前单词的开头 结尾 文本选中在「光标移动」中带有方向键 Up ⬆︎ Down ⬇︎ Left ⬅︎ Right ➡︎ 相关快捷键的基础上，加入 Shift ⇧，则可以扩展为选中文本效果的快捷键。 快捷键 功能 Command ⌘ + Shift ⇧ + Up ⬆︎ Down ⬇︎ 选中光标到全部文本开头 结尾的文本。 Command ⌘ + Shift ⇧ + Left ⬅︎ Right ➡︎ 选中光标到当前行的行首 行尾的文本。 Option ⌥ + Shift ⇧ + Left ⬅︎ Right ➡︎ 选中光标到当前单词的开头 结尾的文本。 文本删除 快捷键 功 能 Ctrl ⌃ + H 或 Delete 删除光标前面的一个字符。 Ctrl ⌃ + D 或 Fn + Delete 删除光标后面的一个字符。 Option ⌥ + Delete 删除光标前面的一个单词。 Command ⌘ + Delete 删除光标之前的整行内容。 Ctrl ⌃ + K 删除光标之后的整行内容。","tags":[null],"categories":[null]},{"title":"Mac实用软件","path":"/notes/pc/250403-mac-tools.html","content":"「软件下载站点」xclient(🌟) 日常下载首选站点 xmac 类似一个资源搜索引擎，下载简单，国内如无法下载，需自由飞翔后可下载。 Mac os 必备软件 一些不错的 Mac 应用推荐 其他 苹果软件盒子 麦禾软件 马可波罗 appstorrent 「实用工具」uTools: 效率工具 一款一个多功能、现代化的工具平台, 俗称：效率神器，跨平台，支持 WinMacOSLinux，是 Alfred的替代品，拥有丰富的插件库。 Alfred：效率工具 一款效率神器，专注 MacOS, 是 uTools的同类工具。之前一直使用，后来被 utools 取代。 MessAuto：提取验证码 自动提取Mac平台的短信和邮箱验证码工具 Dropover：文件中转站 Dropover 是一款 macOS 效率工具，它通过一个可暂存文件的“篮子”来优化拖拽操作。你能轻松地批量管理从各处收集的文件、图片或链接，简化整理过程。 pear-rec：截图工具 是一个跨平台的截图、录屏、录音、录像软件; 官网地址：点击前往 ScrollSnap：滚动截图工具 Mac 滚动截图工具。这是一款用于解决 macOS 原生截图功能，不支持滚动截图的工具。只需要框选指定区域，然后滚动页面，即可轻松得到完整的长截图。 EcoPaste：剪切板工具 一款跨平台的剪贴板管理工具;官网地址：点击前往 MacCalendar: 日历 完全免费开源的离线小而美 macOS 菜单栏日历app，支持中国农历、节假日、放假安排、系统日程等. 「系统工具」WailBrew：Homebrew可视化工具 一种在Macos上管理Homebrew的现代且直观的方式，它提供了一个简洁的图形界面，让每个人都能轻松管理软件包。 Mole：系统清理工具 像鼹鼠一样深入挖掘来清理你的 Mac; PS:一个关注很久的大佬开发自用的工具。 Tidy Up：重复文件清理 Tidy Up 是Mac上最优秀的一款重复文件搜索和清理工具，这款软件可以让你指定搜索的目标目录，然后搜索指定类型的重复文件，如音乐、图片、应用等；下载链接 AirBattery：电池管理 在Mac上获取你所有设备的电量信息并显示在Dock 状态栏 小组件上! Tuxera：NTFS磁盘工具 在 Windows 和 Mac 之间无缝共享数据！Tuxera 推出的 Microsoft NTFS for Mac 让您能够读写使用 NTFS 格式的存储设备，例如 USB 记忆棒或外部硬盘——相比其他方案，其可靠性更佳！ 下载链接 Nigate：NTFS磁盘工具 一款为 Mac 完全兼容的 NTFS 读写解决方案，专为 Apple Silicon 优化。这是一款为 Mac 提供的开源 NTFS 工具，可实现对 NTFS 驱动器的读写访问、挂载和管理。 「视听工具」IINA：视频播放器 一款强大的视频播放器，IINA | Github, 软件官网，界面简洁、美观，契合 macOS 设计风格，功能强大，设置以播放体验为中心，在线字幕、缩略图预览、画中画等；brew 安装","tags":[null]},{"title":"Windows实用软件","path":"/notes/pc/250909-windows-tools.html","content":"本甄选合集将持续更新……🚀 激活工具 适用于 winOSoffice Microsoft-Activation-Scripts 工具简介：使用HWIDOhookKMS38在线KMS激活方法的 Windows 和 Office 激活工具，注重开源代码和较少的杀毒软件检测 官网地址：点击前往 收录时间：2025-09-09 23:43:19 KMS.CX 工具简介：一款一键激活 windowsoffice的工具。 立即体验：点击前往 收录时间：2025-09-09 23:43:58 装个机 专业为普通用户打造的电脑重装系统指南教程网站，提 供Windows 重装系统教程、macOS 重装系统指南、一键重装系统工具、U 盘启动盘制作方法，让系统重装变得简单快捷。 WindowsCleaner 专治 C 盘爆红及各种不服！专业清理系统垃圾，让系统始终保持最佳运行状态。无需注册登录即可使用，并且无任何广告。","tags":[null]},{"title":"我的个人工具清单","path":"/notes/pc/250927-my-usage-pc-tools.html","content":"跨平台清单语雀：知识库管理 语雀是蚂蚁集团旗下的文档与知识库工具。因它多端同步的功能，使它成为我日常工作中最为常用的工具之一，我主要使用小记来记录零散看到的一些链接或临时的一些想法，使用文档记录一些碎片化的知识点，便于后续消化整理。【高频使用】 小旺截图：截图工具 免费截图录屏全能神器，集截图、录屏于一体！专为 Windows、macOS 打造的电脑截图软件，轻巧、好用，功能强大！ Lepton:代码片段管理器 Lepton是一个由 GitHub Gist 提供支持的精益代码片段管理器。我目前使用谷歌邮箱登录，用于管理自己封装过的的代码片段及通用的工具类与函数。【按需使用】 MacOS清单Dropover：文件中转站 Dropover 是一款 macOS 效率工具，它通过一个可暂存文件的“篮子”来优化拖拽操作。你能轻松地批量管理从各处收集的文件、图片或链接，简化整理过程。 Windows软件清单 TODO 电子设备电脑MacbookPro 2024款于2025.3.14在京东官方旗舰店购买，3.18到货后正式服役,详细配置为：「Apple苹果AIMacBook Pro14英寸M4 Pro(14+20核)48G 512G深空黑色笔记本电脑Z1FE0008J」。 MacbookPro 2019款 - (已退役)于2019年9月19日在Apple商城购买，详细位置为 备注：于2025年4月10号送到南京江北虹悦城Apple官方旗舰店以2100元进行回收，至此正式退役。 手机备注Apple产品参数中心 Apple 苹果产品参数中心（hubweb.cn）是一个便捷的苹果产品参数查询平台，这里集中了丰富的信息，你可以轻松获取各类 Apple 产品及芯片的参数与规格信息，还能直观地进行同系列产品间的比较。 iPhone 参数大全 涵盖了 iPhone、iPad、AirPods、Apple Watch、MA 系产品芯片的详细参数。","tags":[null]}]